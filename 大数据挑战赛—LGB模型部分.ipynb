{"cells":[{"outputs":[],"execution_count":null,"source":"# 查看当前挂载的数据集目录\n!ls /home/kesci/input/","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"52580D361EAE4179AB021596E94AF4C1"}},{"outputs":[],"execution_count":null,"source":"# 查看个人持久化工作区文件\n!ls /home/kesci/work/","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"5F00D221CE3247479A366349CAEE391E"}},{"outputs":[],"execution_count":null,"source":"# 查看当前kernerl下的package\n!pip list --format=columns","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"11418C7E60854EC78746DBB4A35D148C"}},{"outputs":[],"execution_count":null,"source":"# 显示cell运行时长\n%load_ext klab-autotime","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"A6463E8752D343198C966AC65F8C4AC2"}},{"metadata":{"id":"283425EF870B4D2F8EDA564BE55DE8FC"},"cell_type":"code","outputs":[],"source":"# lgb模型","execution_count":null},{"metadata":{"id":"8A1DDA69F53B4A68833EB90798B833B3"},"cell_type":"code","outputs":[],"source":"# 存放读取特征并将所读取的所有特征拼接成一个dataframe的函数,\n# 注意在featurecol_h5中控制训练所用特征列\n# 注意在featurecol_map中控制读取特征特征列的位置\n\n# 数据转换函数工具\n\n# 计算qauc函数工具","execution_count":null},{"metadata":{"id":"0930EB8679FF4B0E8E3F4E3540822610","hide_input":true},"cell_type":"code","outputs":[],"source":"# -*- coding: utf-8 -*- \r\n'''\r\nCreated on Jul 9, 2019\r\n\r\n@author: Greatpan\r\n'''\r\nimport pandas as pd\r\nimport numpy as np\r\nimport gc\r\n\r\n# 显示cell运行时长\r\n%load_ext klab-autotime\r\n\r\nfeaturecol_h5={\r\n    'cross_feat':['query_in_title','query_title_pos'],\r\n    'query_pos_feat':['query_pos_1', 'query_pos_2', 'query_pos_3',\r\n                      'query_pos_4', 'query_pos_5', 'query_pos_6', \r\n                      'query_pos_7', 'query_pos_8', 'query_pos_9', \r\n                      'query_pos_10'\r\n                      ],\r\n    'sim_feat':['jaccard_q3_t3',  \r\n                'jaccard_q3_t5',  \r\n                'jaccard_q5_t5',   'levenshtein_q5_t5', \r\n                'jaccard_q5_t10',  'levenshtein_q5_t10', \r\n                'jaccard_q10_t10', 'levenshtein_q10_t10', \r\n                'jaccard_q15_t25', 'levenshtein_q15_t25',\r\n                'jaccard',         'levenshtein'],\r\n\r\n    'fuzz' :['token_sort_ratio','token_set_ratio',\r\n             'partial_ratio','partial_token_sort_ratio',\r\n             'partial_token_set_ratio', 'QRatio','WRatio'],\r\n    \r\n    'textpair':['total_unique_words','wc_ratio_unique',\r\n                'wc_diff_unique','token_set_diff','same_start'],\r\n    \r\n    'len_feat':[\"titlekw_num\", \"titlekw_querykw_diff\",\r\n                \"titlekw_querykw_sum\", \"titlekw_querykw_rate\"],\r\n    \r\n    \"nunique_feat\":[\"title_nunique_query\"],\r\n    \r\n    'title_score_count_feat':[\"title_score_count\",\"title_score_click_num\",\"title_click_rate\"],\r\n    'title_code_score_feat':[\"title_code_score\"],\r\n    'title_convert_feat':[\"title_code_convert\",\"title_code_label_count\"],\r\n    'count_feat':[\"title_count\"],\r\n    \r\n    'tag' :['tag'],\r\n    'tag_score_feat':[\"tag_score\"],\r\n    'tag_convert_feat':[\"tag_convert\",\"tag_label_count\"],\r\n    \r\n    \"match_feat\":['count_match', 'blockcount_match', 'proximity', 'maxMatchBlockLen',\r\n                  'q1_match_start', 'q1_match_end' ],\r\n                #   \"title_match_rate\",\"query_match_rate\",\"maxMatchBlockLen_rate\",\r\n                #   \"title_proximity_rate\", \"query_proximity_rate\"],\r\n    \r\n    'editdistance_relativepos':['editdistance','relative_pos'],\r\n    \r\n    \"BM25\":[\"BM25\"],\r\n    'sif_feat':[\"sif_cos\"],\r\n    'NN_SIM':['NN_SIM'],\r\n    \r\n    'sen_dis':['sent_cosine', 'sent_cityblock', \r\n                'sent_canberra', 'sent_euclidean', 'sent_minkowski',\r\n                'sent_braycurtis'],\r\n    \r\n    'sen_dis2':['skew_q','skew_t','kurtosis_q','kurtosis_t'],\r\n    \r\n}\r\n\r\n# 该函数功能是通过改变类型的方式降低pd中内存\r\ndef reduce_mem_usage(D,verbose=True):\r\n    start_mem = D.memory_usage().sum() / 1024**2\r\n    for c, d in zip(D.columns, D.dtypes):\r\n        if d.kind == 'f':\r\n            D[c] = pd.to_numeric(D[c], downcast='float')\r\n        elif d.kind == 'i':\r\n            D[c] = pd.to_numeric(D[c], downcast='signed')\r\n    end_mem = D.memory_usage().sum() / 1024**2\r\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\r\n    return D\r\n\r\n# def ReadData(datatype='train',nrows=40000000):\r\ndef ReadData(datatype='train',start=0,nrows=100000000):\r\n    if datatype=='train':\r\n        id_feature='/home/kesci/input/bytedance/train_final.csv'\r\n        usecols=[0,4]\r\n        names=['query_id','label']\r\n        \r\n        print(\"正在读取：\",id_feature)\r\n        DataSet = pd.read_csv(id_feature,\r\n                              header=None,\r\n                              skiprows=start,\r\n                              nrows=nrows,\r\n                              usecols=usecols,\r\n                              names=names\r\n                              )\r\n        path_h5=\"/home/kesci/work/pre_3billion_data/train/\"\r\n    elif datatype=='test1':\r\n        id_feature='/home/kesci/input/bytedance/test_final_part1.csv'\r\n        usecols=[0,2]\r\n        names=['query_id','query_title_id']\r\n        path_h5=\"/home/kesci/work/post_4kw_data/test1/\"\r\n        print(\"正在读取：\",id_feature)\r\n        DataSet = pd.read_csv(id_feature,\r\n                              header=None,\r\n                              skiprows=start,\r\n                              nrows=nrows,\r\n                              usecols=usecols,\r\n                              names=names\r\n                              )\r\n    elif datatype=='test2':\r\n        id_feature='/home/kesci/input/bytedance/bytedance_contest.final_2.csv'\r\n        usecols=[0,2]\r\n        names=['query_id','query_title_id']\r\n        path_h5=\"/home/kesci/work/post_4kw_data/test2/\"\r\n        print(\"正在读取：\",id_feature)\r\n        DataSet = pd.read_csv(id_feature,\r\n                              header=None,\r\n                              skiprows=start,\r\n                              nrows=nrows,\r\n                              usecols=usecols,\r\n                              names=names\r\n                              )\r\n\r\n    print(\"length:\",DataSet.__len__())\r\n    DataSet = reduce_mem_usage(DataSet, verbose=True)\r\n    \r\n    featuremap_h5={\r\n        'cross_feat':path_h5+f'cross_{datatype}_feat.h5',\r\n    \r\n        'query_pos_feat':path_h5+f'query_pos_{datatype}_feat.h5',\r\n        'title_pos_feat':path_h5+f'title_pos_{datatype}_feat.h5',\r\n    \r\n        'match_feat':path_h5+f'query_match_{datatype}_feat.h5',\r\n        'editDistance_feat':path_h5+f'editDistance_{datatype}_feat.h5',\r\n    \r\n        'sim_feat':path_h5+f'sim_{datatype}_feat.h5',\r\n        'tag_score_feat':path_h5+f'tag_score_10foldtime_{datatype}_feat.h5',\r\n        'title_score_count_feat':path_h5+f'title_score_count_{datatype}_feat.h5',\r\n        'title_code_score_feat':path_h5+f'title_code_score_10foldtime_{datatype}_feat.h5',\r\n        'title_convert_feat':path_h5+f'title_convert_{datatype}.h5',\r\n        'sif_feat':path_h5+f'sif_{datatype}_post_4kw.h5',\r\n        'len_feat':path_h5+f'len_{datatype}_feat.h5',\r\n        'count_feat':path_h5+f'count_feature_{datatype}.h5',\r\n        \"nunique_feat\":path_h5+f'nunique_feature_{datatype}.h5',\r\n    \r\n        'tag' :path_h5+f'tag_{datatype}.h5',\r\n        \"tag_convert_feat\":path_h5+f\"/tag_convert_{datatype}.h5\",\r\n        \"query_convert\":path_h5+f\"query_convert_{datatype}.h5\",\r\n    \r\n        \"M_cosine\":path_h5+f\"M_sim_{datatype}_feat.h5\",\r\n        \"M_tfidf_cosine\":path_h5+f\"M_tfidf_sim_{datatype}_feat.h5\",\r\n        \"BM25\":path_h5+f'BM25_{datatype}_feat.h5',\r\n        'NN_SIM':path_h5+f'nn_sim_feature.h5',\r\n    \r\n        'editdistance_relativepos':path_h5+f'editdistance_relativepos_{datatype}_feat.h5',\r\n        'fuzz' :path_h5+f\"fuzz_{datatype}_feat.h5\",\r\n        'textpair':path_h5+f\"textpair_{datatype}_feat.h5\",\r\n        \r\n        'sen_dis':path_h5+f\"sen_dis_{datatype}_200.h5\",\r\n        'sen_dis2':path_h5+f\"sen_dis2_{datatype}_200.h5\",\r\n    }\r\n\r\n    for featurefile in featurecol_h5:\r\n        print(\"正在读取：\",featuremap_h5[featurefile])\r\n        feature_set=pd.read_hdf(featuremap_h5[featurefile], key='data',start=start,stop=start+nrows).reset_index(drop=True)\r\n        print(\"length:\",feature_set.__len__())\r\n        # print(feature_set.head(1))\r\n        # feature_set=reduce_mem_usage(feature_set, verbose=True)\r\n        DataSet=pd.concat([DataSet,feature_set], axis=1)\r\n    \r\n    # DataSet = reduce_mem_usage(DataSet, verbose=True)\r\n    print(\"Data Read Finish!\")\r\n    return DataSet\r\n\r\n# *************************************** 数据转换 ************************************\r\ndef getCalcfeat(FeatureData):\r\n    features=[c for c in FeatureData.columns]\r\n\r\n    if \"querykw_num\" in features and \"titlekw_num\" in features:\r\n        print(\"计算长度比率特征..\")\r\n        FeatureData[\"titlekw_querykw_rate\"]=FeatureData[\"titlekw_num\"]/FeatureData[\"querykw_num\"]\r\n        FeatureData[\"titlekw_querykw_diff\"]=FeatureData[\"titlekw_num\"]-FeatureData[\"querykw_num\"]\r\n        FeatureData[\"titlekw_querykw_sum\"]=FeatureData[\"titlekw_num\"]+FeatureData[\"querykw_num\"]\r\n    \r\n    if \"title_score_count\" in features and \"title_score_click_num\" in features:\r\n        print(\"计算title点击率特征..\")\r\n        FeatureData[\"title_click_rate\"]=0\r\n        FeatureData.loc[FeatureData.title_score_count!=0,\"title_click_rate\"]= \\\r\n            FeatureData.loc[FeatureData.title_score_count!=0,\"title_score_click_num\"]/ \\\r\n            FeatureData.loc[FeatureData.title_score_count!=0,\"title_score_count\"]\r\n    \r\n    # if \"count_match\" in features and 'titlekw_num' in features:\r\n    #     print(\"计算匹配度相关比率特征\")\r\n    #     FeatureData[\"title_match_rate\"]=FeatureData[\"count_match\"]/FeatureData['titlekw_num']\r\n    #     FeatureData[\"query_match_rate\"]=FeatureData[\"count_match\"]/FeatureData['querykw_num']\r\n    #     FeatureData[\"maxMatchBlockLen_rate\"]=FeatureData[\"maxMatchBlockLen\"]/FeatureData['querykw_num']\r\n    #     FeatureData[\"title_proximity_rate\"]=FeatureData[\"proximity\"]/FeatureData['titlekw_num']\r\n    #     FeatureData[\"query_proximity_rate\"]=FeatureData[\"count_match\"]/(FeatureData[\"proximity\"]+0.0000001)\r\n    \r\n    FeatureData = reduce_mem_usage(FeatureData, verbose=True)\r\n    print(\"calc feat done\")\r\n    return FeatureData\r\n\r\ndef preDataXGBoost(FeatureData,hasLabel=True,datatype=None):\r\n    # 该函数的功能是进行是将pandas数据转换为numpy.array\r\n    FeatureData=getCalcfeat(FeatureData)\r\n    if hasLabel:\r\n        if datatype is not None:\r\n            group_path = f'/home/kesci/work/pre_3billion_data/groups_{datatype}.npy'\r\n            group=np.load(group_path)\r\n            target=FeatureData.label.get_values()\r\n        else:\r\n            print(\"error! Please set datatype...\")\r\n    cols=[]\r\n    for featurefile in featurecol_h5:\r\n        cols=cols+featurecol_h5[featurefile]\r\n    \r\n    featuredata = FeatureData[cols].get_values()\r\n\r\n    if hasLabel:\r\n        return group,featuredata,target\r\n    else:\r\n        return featuredata\r\n\r\n# *************************************** 计算auc源码 ************************************\r\ndef calAUC(labels,prob):\r\n    f = list(zip(prob,labels))\r\n    rank = [values2 for values1,values2 in sorted(f,key=lambda x:x[0])]\r\n    rankList = [i+1 for i in range(len(rank)) if rank[i]==1]\r\n    posNum = 0\r\n    negNum = 0\r\n    for i in range(len(labels)):\r\n        if(labels[i]==1):\r\n            posNum+=1\r\n        else:\r\n            negNum+=1\r\n    auc = (sum(rankList)- (posNum*(posNum+1))/2)/(posNum*negNum)\r\n    return auc\r\n\r\n# 计算各个组的qauc值\r\ndef sumAUC(mycombinedata):\r\n    grouplist=mycombinedata[0]\r\n    y_true=mycombinedata[1]\r\n    y_pred=mycombinedata[2]\r\n\r\n    if len(y_true)!=sum(grouplist):\r\n        print(\"评分函数中len(y_true)!=sum(group)\")\r\n        return\r\n    start=0\r\n    sum_AUC=0\r\n    for group in grouplist:\r\n        if 0 in y_true[start:start+group] and 1 in y_true[start:start+group]:\r\n            roc_auc = calAUC(y_true[start:start+group],y_pred[start:start+group])\r\n            # roc_auc=auc(fpr,tpr) ###计算auc的值\r\n        else:\r\n            roc_auc=0.5\r\n        start=start+group\r\n        sum_AUC=sum_AUC+roc_auc\r\n    return sum_AUC\r\n\r\nfrom joblib import Parallel, delayed\r\nfrom sklearn.metrics import roc_curve, auc\r\ndef qAUC(y_true,y_pred,group):\r\n    groupnum=16\r\n    import math\r\n    group_len=math.ceil(len(group)/groupnum)\r\n    groups=[group[i*group_len:(i+1)*group_len] for i in range(groupnum)]\r\n    mycombines=[]\r\n\r\n    start=0\r\n    for agroup in groups:\r\n        mycombinedata=[]\r\n        mycombinedata.append(agroup)\r\n        mycombinedata.append(y_true[start:start+sum(agroup)])\r\n        mycombinedata.append(y_pred[start:start+sum(agroup)])\r\n        start=start+sum(agroup)\r\n        mycombines.append(mycombinedata)\r\n\r\n    sum_AUC=Parallel(n_jobs=groupnum)(delayed(sumAUC)(mycombinedata) for mycombinedata in mycombines)\r\n\r\n    return \"qAUC\",sum(sum_AUC)/len(group)\r\n","execution_count":null},{"metadata":{"id":"2E3F25BBB7A94EBF87133A876EB1A9E8"},"cell_type":"code","outputs":[],"source":"# 读取数据,并将数据划分数据集和验证集","execution_count":null},{"metadata":{"id":"06A6E39A4D3D4DDE9303A742EF87A15C","hide_input":true},"cell_type":"code","outputs":[],"source":"#  读取处理成特征的数据集（只使用了后 1500000 query_id 的数据）\r\ntrain_query_title = ReadData(datatype='train',nrows=100000000)\r\nprint(\"数据读取完毕\")\r\n\r\ntrain_query_title.title_code_score=train_query_title.title_code_score.fillna(0)\r\ntrain_query_title.tag_score=train_query_title.tag_score.fillna(0)\r\n\r\n# 将前500w个query_id 作为验证集，后350w数据集作为训练集\r\n\r\nqueryids=list(range(1,train_query_title.query_id.max()))\r\nval_queryids = queryids[:1000000]\r\ntrain_queryids = queryids[1000000:]\r\n\r\nval_query_title=train_query_title.loc[train_query_title.query_id.isin(val_queryids)]\r\nprint(\"val len:\",val_query_title.__len__())\r\ntrain_query_title=train_query_title.loc[train_query_title.query_id.isin(train_queryids)]\r\nprint(\"train len:\",train_query_title.__len__())\r\n\r\ndel queryids,train_queryids,val_queryids\r\ngc.collect()\r\n\r\nprint(\"测试集验证集划分完毕\")","execution_count":null},{"metadata":{"id":"B15E19F4FB4D4A3BBD52C0A3331420FA"},"cell_type":"code","outputs":[],"source":"# 对训练集按照queryid分组放入lgb.Dataset()中","execution_count":null},{"metadata":{"id":"5E2D4C35C1FB46E28C1A9D6718EF0495","hide_input":true},"cell_type":"code","outputs":[],"source":"import gc\r\nTrain_flag=True\r\nValdation_Flag=True\r\nimport lightgbm as lgb\r\n\r\nif Train_flag:\r\n    # dgroup,dtrain,dtarget = preDataXGBoost(train_query_title,hasLabel=True,datatype=\"train\")\r\n    datatype=\"train\"\r\n    group_path = f'/home/kesci/work/pre_3billion_data/groups_{datatype}.npy'\r\n    train_data = lgb.Dataset(data=preDataXGBoost(train_query_title,hasLabel=False), \r\n                             label=train_query_title.label.get_values(),\r\n                             group=np.load(group_path))\r\n    del train_query_title\r\n    gc.collect()\r\n    print(\"train OK\")\r\n\r\nif Valdation_Flag:\r\n    # dvalgroup,dval,dvaltarget = preDataXGBoost(val_query_title,hasLabel=True,datatype=\"validation\")\r\n    datatype=\"validation\"\r\n    group_path = f'/home/kesci/work/pre_3billion_data/groups_{datatype}.npy'\r\n    \r\n    val_data = lgb.Dataset(data=preDataXGBoost(val_query_title,hasLabel=False), \r\n                           label=val_query_title.label.get_values(), \r\n                           group=np.load(group_path),\r\n                           reference=train_data)\r\n    \r\n    del val_query_title\r\n    gc.collect()\r\n    print(\"test OK\")\r\nprint(\"数据转换完毕\")\r\n\r\ngc.collect()","execution_count":null},{"metadata":{"id":"96E72DC380C948A3973AE27BEF07D241"},"cell_type":"code","outputs":[],"source":"# lgb模型训练","execution_count":null},{"metadata":{"id":"B27A6A544B96463F87F86F7025FE9997","hide_input":false},"cell_type":"code","outputs":[],"source":"import lightgbm as lgb\r\nimport time\r\n\r\nlgb_rank_params = {    \r\n    'boosting_type' : 'gbdt', \r\n    'objective' : 'lambdarank',\r\n    # 'objective' : 'binary',\r\n    'metric': 'map',\r\n    # 'metric':'auc',\r\n    'random_state' : 2019,\r\n    'n_jobs' : 13,\r\n    'silent' : False,\r\n    \r\n    'num_leaves' : 195,\r\n    'max_depth' : 12,\r\n    'learning_rate':0.05,\r\n    # 'n_estimators':10,\r\n    'max_bin':200,\r\n    'subsample_for_bin':200000,\r\n    'min_split_gain':0.0,\r\n    'min_child_weight':0.001,\r\n    'min_child_samples':20,\r\n    'subsample':0.8,\r\n    'subsample_freq':1,\r\n    'colsample_bytree':0.8,\r\n    'reg_alpha':3.0,\r\n    'reg_lambda':2.0,\r\n    \r\n    # 'device':'gpu',\r\n    # 'gpu_platform_id':0,\r\n    # 'gpu_device_id':0\r\n}\r\n\r\n# train_data = lgb.Dataset(data=dtrain, label=dtarget, group=dgroup)\r\n# train_data = lgb.Dataset(data=dtrain, label=dtarget)\r\nvaldation_flag = True\r\n\r\nif valdation_flag:\r\n    # val_data = lgb.Dataset(data=dval, label=dvaltarget, group=dvalgroup)\r\n    # val_data = lgb.Dataset(data=dval, label=dvaltarget)\r\n    print(\"Train 开始\")\r\n    start=time.time()\r\n    rankModel = lgb.train(lgb_rank_params, train_data, num_boost_round=3000,\\\r\n        valid_sets=[train_data,val_data],\r\n        verbose_eval=1,\r\n        early_stopping_rounds=300,\r\n        # init_model=rankModel,\r\n        # keep_training_booster=True,\r\n        # free_raw_data=False,\r\n        # init_model=myModelpath+\"trainSet\"+str(0)+\"/LGBRank2.model\",\r\n        # feval=qAUC)\r\n        )\r\n    print(\"训练完成，训练时间为：\",time.time()-start)\r\n    \r\n    rankModel.save_model(\"/home/kesci/work/LGBModel/LGBRank_pre1e_807_final_baseline_3000_1.model\")\r\n    print(\"save model finish.\")\r\n    \r\n    start=time.time()\r\n    test_pred=rankModel.predict(dval)\r\n    print(\"预测完成，预测时间为：\",time.time()-start)\r\n    \r\n    start=time.time()\r\n    score_cur=qAUC(dvaltarget,test_pred,dvalgroup)\r\n    print(\"计算qAUC完成，计算时间为：\",time.time()-start)\r\n    print(\"qAUC值为：\",score_cur)\r\n","execution_count":null},{"metadata":{"id":"EE848A045D8F463ABFD077CA4AAEE25F"},"cell_type":"code","outputs":[],"source":"# 模型test1集预测","execution_count":null},{"metadata":{"id":"0FDC31B2C1BE4AF08FF0173CA4AC3B6D","hide_input":true},"cell_type":"code","outputs":[],"source":"print(\"begin\")\n\ntest_query_title = ReadData(datatype='test1')\ntest_query_title.title_code_score=test_query_title.title_code_score.fillna(0)\ntest_query_title.tag_score=test_query_title.tag_score.fillna(0)\nprint(\"数据读取完毕\")\n\ndtest = preDataXGBoost(test_query_title,hasLabel=False)\nprint(\"dtest OK\")\n\nfrom lightgbm import Dataset,Booster\nrankModel=Booster(model_file=\"/home/kesci/work/LGBModel/LGBRank_pre1e_807_final_baseline_3000_1.model\")\n\ntest_pred=rankModel.predict(dtest)\nprint(\"test_pred OK\")\n\nsubmission=test_query_title[[\"query_id\",\"query_title_id\"]]\nsubmission[\"prediction\"]=pd.DataFrame(test_pred)\n\nSaveFile=\"/home/kesci/work/Submission/LGBRank_pre1e_807_final_baseline_3000_1.csv\"\nsubmission.to_csv(path_or_buf=SaveFile,header=False,index=0,encoding=\"utf-8\")\nprint(\"submission OK\")\n","execution_count":null}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":0}