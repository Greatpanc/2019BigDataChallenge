{"cells":[{"outputs":[],"execution_count":null,"source":"# 查看当前挂载的数据集目录\n!ls /home/kesci/input/","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"9065D50168744BD48020FE09FA382A74"}},{"outputs":[],"execution_count":null,"source":"# 查看个人持久化工作区文件\n!ls /home/kesci/work/","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"99A49ACED0FE4059860E79077EEE1623"}},{"outputs":[],"execution_count":null,"source":"# 查看当前kernerl下的package\n!pip list --format=columns","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"22B6EACD2792437C896D5F2AA5F597B1"}},{"outputs":[],"execution_count":null,"source":"# 显示cell运行时长\n%load_ext klab-autotime","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"CCB839C6D38E4A228AA42D9857AD803D"}},{"metadata":{"id":"A77A247BD6A5427E94347C124F165DE3"},"cell_type":"code","outputs":[],"source":"# 下一个cell实现以下功能：\n# 对query进行id编码\n#   为了确保程序能够顺利运行：\n#   1、在work目录下创建featureMap目录\n#   2、在featureMap目录下创建train、test1、test2三个子目录","execution_count":null},{"metadata":{"id":"A930418DB8634CECB4DD07A610EC7795","hide_input":true},"cell_type":"code","outputs":[],"source":"from collections import defaultdict\r\n\r\n# 1.1 读取test1文件 对test1中query文本进行id编码\r\nf = open(\"/home/kesci/input/bytedance/test_final_part1.csv\")\r\nchunk_size = 256 * 1024 * 1024\r\n\r\nlines = f.readlines(chunk_size)\r\ncount = 0\r\n\r\nquerys = defaultdict(int)\r\nquerys_code = 1\r\n\r\nwhile lines:\r\n    for line in lines:\r\n        count += 1\r\n        line = line.strip().split(\",\")\r\n        query = line[1].strip()\r\n\r\n        if query not in querys.keys():\r\n            querys[query] = querys_code\r\n            querys_code += 1\r\n\r\n        if count % 1000000 == 0:\r\n            print(\"count \", count)\r\n    lines = f.readlines(chunk_size)\r\nf.close()\r\n\r\n# 1.2 读取train文件 对train中query文本进行id编码\r\nf_train = open(\"/home/kesci/input/bytedance/train_final.csv\")\r\nlines = f_train.readlines(chunk_size)\r\n\r\nwhile lines:\r\n    for line in lines:\r\n        count += 1\r\n        line = line.strip().split(\",\")\r\n        query = line[1].strip()\r\n\r\n        if query not in querys.keys():\r\n            querys[query] = querys_code\r\n            querys_code += 1\r\n\r\n        if count % 50000000 == 0:\r\n            print(\"count \", count)\r\n    lines = f_train.readlines(chunk_size)\r\nf_train.close()\r\n\r\n# 1.3 读取test2文件 对test2中query文本进行id编码\r\nf_test2 = open(\"/home/kesci/input/bytedance/bytedance_contest.final_2.csv\")\r\nlines = f_test2.readlines(chunk_size)\r\n\r\nwhile lines:\r\n    for line in lines:\r\n        count += 1\r\n        line = line.strip().split(\",\")\r\n        query = line[1].strip()\r\n\r\n        if query not in querys.keys():\r\n            querys[query] = querys_code\r\n            querys_code += 1\r\n\r\n        if count % 50000000 == 0:\r\n            print(\"count \", count)\r\n    lines = f_test2.readlines(chunk_size)\r\nf_test2.close()\r\n\r\n# # 2.1 将query文本的字典存储在硬盘上 方便下次使用\r\n# import pickle\r\n# save_path = f'/home/kesci/work/dict/query_encode_map.pkl'  # query字典存储位置\r\n# with open(save_path, 'wb') as f:\r\n#     pickle.dump(querys, f)\r\n\r\n# # 2.2 从硬盘中读取query 字典代码\r\n# import pickle\r\n# query_encode_save_path = f'/home/kesci/work/dict/query_encode_map.pkl'  \r\n# with open(query_encode_save_path, 'rb') as f:\r\n#     querys=pickle.load(f)\r\n# print(\"done\")\r\n\r\n\r\n# 3.1 对train集文本进行id编码\r\nfrom collections import defaultdict\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\nchunk_size = 256 * 1024 * 1024\r\n\r\nf_train = open(\"/home/kesci/input/bytedance/train_final.csv\")\r\nlines = f_train.readlines(chunk_size)\r\ncount = 0\r\n\r\ntrain_querys_code = 0\r\ntrain_code=np.empty(1000000000,dtype = int)\r\nwhile lines:\r\n    for line in lines:\r\n        query = line.strip().split(\",\")[1].strip()\r\n        train_code[train_querys_code]=querys[query]\r\n        train_querys_code+=1\r\n        \r\n        count += 1\r\n        if count % 10000000 == 0:\r\n            print(\"count \", count)\r\n    lines = f_train.readlines(chunk_size)\r\nf_train.close()\r\n\r\ntrain_code_feat=pd.DataFrame(train_code,columns=[\"query_code\"])\r\ndataSetType=\"train\"\r\npath=f\"/home/kesci/work/featureMap/{dataSetType}/query_code_{dataSetType}_feat.h5\"\r\ntrain_code_feat.to_hdf(path, index=None, key='data', complevel=9)\r\nprint(\"done\")\r\n\r\n# 3.2 对test1集文本进行id编码\r\nfrom collections import defaultdict\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\nchunk_size = 256 * 1024 * 1024\r\n\r\nf_test = open(\"/home/kesci/input/bytedance/test_final_part1.csv\")\r\nlines = f_test.readlines(chunk_size)\r\ncount = 0\r\n\r\ntest_querys_code = 0\r\ntest_code=np.empty(20000000,dtype = int)\r\nwhile lines:\r\n    for line in lines:\r\n        query = line.strip().split(\",\")[1].strip()\r\n        test_code[test_querys_code]=querys[query]\r\n        test_querys_code+=1\r\n        \r\n        count += 1\r\n        if count % 10000000 == 0:\r\n            print(\"count \", count)\r\n    lines = f_test.readlines(chunk_size)\r\nf_test.close()\r\n\r\ntest_code_feat=pd.DataFrame(test_code,columns=[\"query_code\"])\r\ndataSetType=\"test1\"\r\npath=f\"/home/kesci/work/featureMap/{dataSetType}/query_code_{dataSetType}_feat.h5\"\r\ntest_code_feat.to_hdf(path, index=None, key='data', complevel=9)\r\nprint(\"done\")\r\n\r\n\r\n# 3.3 对test2集文本进行id编码\r\nfrom collections import defaultdict\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\nchunk_size = 256 * 1024 * 1024\r\n\r\nf_test2 = open(\"/home/kesci/input/bytedance/bytedance_contest.final_2.csv\")\r\nlines = f_test2.readlines(chunk_size)\r\ncount = 0\r\n\r\ntest_querys_code = 0\r\ntest_code=np.empty(20000000,dtype = int)\r\nwhile lines:\r\n    for line in lines:\r\n        query = line.strip().split(\",\")[1].strip()\r\n        test_code[test_querys_code]=querys[query]\r\n        test_querys_code+=1\r\n        \r\n        count += 1\r\n        if count % 10000000 == 0:\r\n            print(\"count \", count)\r\n    lines = f_test2.readlines(chunk_size)\r\nf_test2.close()\r\n\r\ntest_code_feat=pd.DataFrame(test_code,columns=[\"query_code\"])\r\ndataSetType=\"test2\"\r\npath=f\"/home/kesci/work/featureMap/{dataSetType}/query_code_{dataSetType}_feat.h5\"\r\ntest_code_feat.to_hdf(path, index=None, key='data', complevel=9)\r\nprint(\"done\")\r\n","execution_count":null},{"metadata":{"id":"B38D51F43A4E4E86B3A4C9587F784A89"},"cell_type":"code","outputs":[],"source":"","execution_count":null},{"metadata":{"id":"03FC79ED149E45AC84B88B7CD5BD7C2E"},"cell_type":"code","outputs":[],"source":"# 下一个cell实现以下功能：\n# 对title进行id编码\n#   为了确保程序能够顺利运行： \n#   1、在work目录下创建featureMap目录\n#   2、在featureMap目录下创建train、test1、test2三个子目录","execution_count":null},{"metadata":{"id":"02F98888E06748C888597639804EE9A7","hide_input":true},"cell_type":"code","outputs":[],"source":"from collections import defaultdict\r\n\r\n# 1.1 读取test1文件 对test1中title文本进行id编码\r\nf = open(\"/home/kesci/input/bytedance/test_final_part1.csv\")\r\nchunk_size = 256 * 1024 * 1024\r\n\r\nlines = f.readlines(chunk_size)\r\ncount = 0\r\n\r\ntitles = defaultdict(int)\r\ntitles_code = 1\r\n\r\nwhile lines:\r\n    for line in lines:\r\n        count += 1\r\n        line = line.strip().split(\",\")\r\n        title = line[3].strip()\r\n\r\n        if title not in titles.keys():\r\n            titles[title] = titles_code\r\n            titles_code += 1\r\n\r\n        if count % 1000000 == 0:\r\n            print(\"count \", count)\r\n    lines = f.readlines(chunk_size)\r\nf.close()\r\n\r\n# 1.2 读取train文件 对train中title文本进行id编码\r\nf_train = open(\"/home/kesci/input/bytedance/train_final.csv\")\r\nlines = f_train.readlines(chunk_size)\r\n\r\nwhile lines:\r\n    for line in lines:\r\n        count += 1\r\n        line = line.strip().split(\",\")\r\n        title = line[3].strip()\r\n\r\n        if title not in titles.keys():\r\n            titles[title] = titles_code\r\n            titles_code += 1\r\n\r\n        if count % 50000000 == 0:\r\n            print(\"count \", count)\r\n    lines = f_train.readlines(chunk_size)\r\nf_train.close()\r\n\r\n# 1.3 读取test2文件 对test2中title文本进行id编码\r\nf_test2 = open(\"/home/kesci/input/bytedance/train_final.csv\")\r\nlines = f_test2.readlines(chunk_size)\r\n\r\nwhile lines:\r\n    for line in lines:\r\n        count += 1\r\n        line = line.strip().split(\",\")\r\n        title = line[3].strip()\r\n\r\n        if title not in titles.keys():\r\n            titles[title] = titles_code\r\n            titles_code += 1\r\n\r\n        if count % 50000000 == 0:\r\n            print(\"count \", count)\r\n    lines = f_test2.readlines(chunk_size)\r\nf_test2.close()\r\n\r\n# # 2.1 将title文本的字典存储在硬盘上 方便下次使用\r\n# import pickle\r\n# save_path = f'/home/kesci/work/dict/title_encode_map.pkl'  \r\n# with open(save_path, 'wb') as f:\r\n#     pickle.dump(titles, f)\r\n# print(\" save done\")\r\n\r\n# # 2.2 从硬盘中读取title 字典代码\r\n# import pickle\r\n# title_encode_save_path = f'/home/kesci/work/dict/title_encode_map.pkl'  \r\n# with open(title_encode_save_path, 'rb') as f:\r\n#     titles=pickle.load(f)\r\n# print(\"done\")\r\n\r\n# 3.1 对train集title文本进行id编码\r\nfrom collections import defaultdict\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\nchunk_size = 256 * 1024 * 1024\r\n\r\nf_train = open(\"/home/kesci/input/bytedance/train_final.csv\")\r\nlines = f_train.readlines(chunk_size)\r\ncount = 0\r\n\r\ntrain_titles_code = 0\r\ntrain_code=np.empty(1000000000,dtype = int)\r\nwhile lines:\r\n    for line in lines:\r\n        title = line.strip().split(\",\")[3].strip()\r\n        train_code[train_titles_code]=titles[title]\r\n        train_titles_code+=1\r\n        \r\n        count += 1\r\n        if count % 1000000 == 0:\r\n            print(\"count \", count)\r\n    lines = f_train.readlines(chunk_size)\r\nf_train.close()\r\n\r\ntrain_code_feat=pd.DataFrame(train_code,columns=[\"title_code\"])\r\n\r\ndataSetType=\"train\"\r\npath=f\"/home/kesci/work/featureMap/{dataSetType}/title_code_{dataSetType}_feat.h5\"\r\ntrain_code_feat.to_hdf(path, index=None, key='data', complevel=9)\r\nprint(\"train done\")\r\n\r\n# 3.2 对test1集title文本进行id编码\r\nfrom collections import defaultdict\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\nchunk_size = 256 * 1024 * 1024\r\n\r\nf_test = open(\"/home/kesci/input/bytedance/test_final_part1.csv\")\r\nlines = f_test.readlines(chunk_size)\r\ncount=0\r\n\r\ntest_titles_code=0\r\ntest_code=np.empty(20000000,dtype = int)\r\nwhile lines:\r\n    for line in lines:\r\n        title = line.strip().split(\",\")[3].strip()\r\n        test_code[test_titles_code]=titles[title]\r\n        test_titles_code+=1\r\n        \r\n        count += 1\r\n        if count % 2000000 == 0:\r\n            print(\"count \", count)\r\n    lines = f_test.readlines(chunk_size)\r\nf_test.close()\r\n\r\ntest_code_feat=pd.DataFrame(test_code,columns=[\"title_code\"])\r\ncols=[\"title_code\"]\r\n\r\ndataSetType=\"test1\"\r\npath=f\"/home/kesci/work/featureMap/{dataSetType}/title_code_{dataSetType}_feat.h5\"\r\ntest_code_feat.to_hdf(path, index=None, key='data', complevel=9)\r\nprint(\"done\")\r\n","execution_count":null},{"metadata":{"id":"29703FE4AAA943889BA368CF0F2E72BA"},"cell_type":"code","outputs":[],"source":"","execution_count":null},{"metadata":{"id":"98E6C8B6D2A54FAB8AF2665FA6744831"},"cell_type":"code","outputs":[],"source":"# len特征计算\n# my_feat_calc_func 函数是并行计算特征的函数\n# 只需要将包含queryid、query、title列的dataframe传入函数my_feat_calc_func()，即可计算出该类特征\n# 将前一亿、后一亿、test1、test2传入dataframe","execution_count":null},{"metadata":{"id":"03606BA5E1424A13B4CD07443CB7ABFA","hide_input":true},"cell_type":"code","outputs":[],"source":"# -*- coding: utf-8 -*- \r\n'''\r\nCreated on Jul 2, 2019\r\n\r\n@author: Greatpan\r\n'''\r\n\r\nimport pandas as pd\r\npd.set_option('display.max_rows', 500)\r\npd.set_option('display.max_columns', 500)\r\n\r\nimport numpy as np\r\n\r\nfrom tqdm import tqdm\r\ntqdm.pandas(desc='Progress')\r\n\r\nfrom multiprocessing import Pool\r\n\r\ndef apply_fun(df):\r\n    basic_feat=pd.DataFrame()\r\n    \r\n    df[\"query\"]=df[\"query\"].apply(str.strip)\r\n    df[\"title\"]=df[\"title\"].apply(str.strip)\r\n    \r\n    basic_feat['querykw_num']=df['query'].apply(lambda x:len(x.split(sep=\" \")))\r\n    basic_feat['titlekw_num']=df['title'].apply(lambda x:len(x.split(sep=\" \")))\r\n    \r\n    return basic_feat\r\n\r\ndef my_feat_calc_func(RawData):\r\n    '''  关键词长度相关特征  a'''\r\n    multiprocessing_nums=16\r\n    df_parts = np.array_split(RawData, multiprocessing_nums)\r\n    with Pool(processes=multiprocessing_nums) as pool:\r\n        result_parts = pool.map(apply_fun, df_parts)\r\n    pool.join()\r\n    \r\n    result_parallel = pd.concat(result_parts)\r\n    \r\n    return result_parallel\r\n\r\n\r\n\r\n# train_df_post4kw_flag=True\r\n# train_df_post4kw_flag_path=f'./post_4kw_data/train/len_train_feat.h5'\r\n\r\n# train_df_pre3billion_flag=True\r\n# train_df_pre3billion_result_path=f'./pre_3billion_data/train/len_train_feat.h5'\r\n\r\n# test_df_flag=True\r\n# test_df_flag_path=f'/home/kesci/work/post_4kw_data/test1/len_test1_feat.h5'\r\n\r\n# test2_df_flag=True\r\n# test2_df_flag_path=f'/home/kesci/work/post_4kw_data/test2/len_test2_feat.h5'\r\n\r\n# if train_df_pre3billion_flag:\r\n#     parts_num=10\r\n#     result_parts=[]\r\n#     for index in range(parts_num):\r\n#         print(\"start 前3亿的train...\")\r\n#         path=f\"/home/kesci/input/bytedance/train_final.csv\"\r\n#         train_df_pre_3billion = pd.read_csv(path,\r\n#                                             header=None,\r\n#                                             skiprows=10000000*index,\r\n#                                             nrows=10000000,\r\n#                                             usecols=[0,1,3],\r\n#                                             names=['query_id','query','title'],\r\n#                                             )\r\n#         print(train_df_pre_3billion.__len__())\r\n    \r\n#         result_part = my_feat_calc_func(train_df_pre_3billion)\r\n#         print(f\"part{index} done！\")\r\n    \r\n#         result_parts.append(result_part)\r\n    \r\n#     train_df_pre_3billion_result = pd.concat(result_parts)\r\n#     print(\"done！\")\r\n\r\n#     train_df_pre_3billion_result = reduce_mem_usage(train_df_pre_3billion_result,verbose=True)\r\n#     train_df_pre_3billion_result.to_hdf(train_df_pre3billion_result_path, index=None, key='data', complevel=9)\r\n#     print(\"save ok, len=\",train_df_pre_3billion_result.__len__())\r\n    \r\n# if train_df_post4kw_flag:\r\n#     print(\"start 后4kw的train...\")\r\n#     path=f\"/home/kesci/work/post_4kw_data/train_post_4kw.csv\"\r\n#     train_post4kw_df = pd.read_csv(path,\r\n#                         header=None,\r\n#                         usecols=[0,1,3],\r\n#                         names=['query_id','query','title'],\r\n#                         )\r\n#     print(train_post4kw_df.__len__())\r\n    \r\n#     train_post4kw_df_result = my_feat_calc_func(train_post4kw_df)\r\n#     print(\"done！\")\r\n    \r\n#     train_post4kw_df_result=reduce_mem_usage(train_post4kw_df_result,verbose=True)\r\n#     train_post4kw_df_result.to_hdf(train_df_post4kw_flag_path, index=None, key='data', complevel=9)\r\n#     print(\"save ok\")\r\n\r\n# if test_df_flag:\r\n#     print(\"start test1\")\r\n#     path=f\"/home/kesci/input/bytedance/test_final_part1.csv\"\r\n#     test_df = pd.read_csv(path,\r\n#                         header=None,\r\n#                         usecols=[0,1,3],\r\n#                         names=['query_id','query','title'],\r\n#                         )\r\n#     print(test_df.__len__())\r\n    \r\n#     test_df_result = my_feat_calc_func(test_df)\r\n#     print(\"done！\")\r\n\r\n#     test_df_result = reduce_mem_usage(test_df_result,verbose=True)\r\n#     test_df_result.to_hdf(test_df_flag_path, index=None, key='data', complevel=9)\r\n#     print(\"save ok\")\r\n\r\n# if test2_df_flag:\r\n#     print(\"start test2\")\r\n#     path=f\"/home/kesci/input/bytedance/test_final_part2.csv\"\r\n#     test2_df = pd.read_csv(path,\r\n#                         header=None,\r\n#                         usecols=[0,1,3],\r\n#                         names=['query_id','query','title'],\r\n#                         )\r\n#     print(test2_df.__len__())\r\n    \r\n#     test2_df_result = my_feat_calc_func(test2_df)\r\n#     print(\"done！\")\r\n\r\n#     test2_df_result = reduce_mem_usage(test2_df_result,verbose=True)\r\n#     test2_df_result.to_hdf(test2_df_flag_path, index=None, key='data', complevel=9)\r\n#     print(\"save ok\")\r\n","execution_count":null},{"metadata":{"id":"1432040281DC4ABA834CB50813AA6C09"},"cell_type":"code","outputs":[],"source":"","execution_count":null},{"metadata":{"id":"209984FB69B84BE98FBF60223995CC4A"},"cell_type":"code","outputs":[],"source":"# cross特征计算\n# my_feat_calc_func 函数是并行计算特征的函数\n# 只需要将包含queryid、query、title列的dataframe传入函数my_feat_calc_func()，即可计算出该类特征\n# 将前一亿、后一亿、test1、test2传入dataframe","execution_count":null},{"metadata":{"id":"95A936D354C74266BE8466F341E42B4B","hide_input":true},"cell_type":"code","outputs":[],"source":"# -*- coding: utf-8 -*- \r\n'''\r\nCreated on Jul 2, 2019\r\n\r\n@author: Greatpan\r\n'''\r\n\r\nimport pandas as pd\r\npd.set_option('display.max_rows', 500)\r\npd.set_option('display.max_columns', 500)\r\n\r\nimport numpy as np\r\n\r\nfrom tqdm import tqdm\r\ntqdm.pandas(desc='Progress')\r\n\r\ndef reduce_mem_usage(D,verbose=True):\r\n    start_mem = D.memory_usage().sum() / 1024**2\r\n    for c, d in zip(D.columns, D.dtypes):\r\n        if d.kind == 'f':\r\n            D[c] = pd.to_numeric(D[c], downcast='float')\r\n        elif d.kind == 'i':\r\n            D[c] = pd.to_numeric(D[c], downcast='signed')\r\n    end_mem = D.memory_usage().sum() / 1024**2\r\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\r\n    return D\r\n\r\nfrom multiprocessing import Pool\r\n\r\ndef func(row):\r\n    return row['lcsubstr_lens'] + row['lcseque_lens']\r\n\r\ndef apply_fun(df):\r\n    basic_feat=pd.DataFrame()\r\n    \r\n    df[\"query\"]=df[\"query\"].apply(str.strip)\r\n    df[\"title\"]=df[\"title\"].apply(str.strip)\r\n    \r\n    basic_feat['query_in_title'] = df.apply(lambda row: len(row['query']) / (len(row['title']) + 1) \\\r\n                                        if row['query'] in row['title'] \r\n                                        else 0, \r\n                                        axis=1)\r\n    \r\n    basic_feat['query_title_pos']= df.apply(lambda row: row['title'].find(row['query']), \r\n                                            axis=1)\r\n    \r\n    return basic_feat\r\n\r\ndef my_feat_calc_func(RawData):\r\n    '''  关键词长度相关特征  a'''\r\n    multiprocessing_nums=16\r\n    df_parts = np.array_split(RawData, multiprocessing_nums)\r\n    with Pool(processes=multiprocessing_nums) as pool:\r\n        result_parts = pool.map(apply_fun, df_parts)\r\n    pool.join()\r\n    \r\n    result_parallel = pd.concat(result_parts)\r\n    \r\n    return result_parallel\r\n\r\n# train_df_post4kw_flag=False\r\n# train_df_post4kw_flag_path=f'./post_4kw_data/train/cross_train_feat.h5'\r\n\r\n# train_df_pre3billion_flag=False\r\n# train_df_pre3billion_result_path=f'./pre_3billion_data/train/cross_train_feat.h5'\r\n\r\n# test_df_flag=False\r\n# test_df_flag_path=f'/home/kesci/work/post_4kw_data/test1/cross_test1_feat.h5'\r\n\r\n# test2_df_flag=True\r\n# test2_df_flag_path=f'/home/kesci/work/post_4kw_data/test2/cross_test2_feat.h5'\r\n\r\n# if train_df_pre3billion_flag:\r\n#     parts_num=10\r\n#     result_parts=[]\r\n#     for index in range(parts_num):\r\n#         print(\"start 前3亿的train...\")\r\n#         path=f\"/home/kesci/input/bytedance/train_final.csv\"\r\n#         train_df_pre_3billion = pd.read_csv(path,\r\n#                                             header=None,\r\n#                                             skiprows=10000000*index,\r\n#                                             nrows=10000000,\r\n#                                             usecols=[0,1,3],\r\n#                                             names=['query_id','query','title'],\r\n#                                             )\r\n#         print(train_df_pre_3billion.__len__())\r\n    \r\n#         result_part = my_feat_calc_func(train_df_pre_3billion)\r\n#         print(f\"part{index} done！\")\r\n    \r\n#         result_parts.append(result_part)\r\n    \r\n#     train_df_pre_3billion_result = pd.concat(result_parts)\r\n#     print(\"done！\")\r\n\r\n#     train_df_pre_3billion_result = reduce_mem_usage(train_df_pre_3billion_result,verbose=True)\r\n#     train_df_pre_3billion_result.to_hdf(train_df_pre3billion_result_path, index=None, key='data', complevel=9)\r\n#     print(\"save ok, len=\",train_df_pre_3billion_result.__len__())\r\n    \r\n# if train_df_post4kw_flag:\r\n#     print(\"start 后4kw的train...\")\r\n#     path=f\"/home/kesci/work/post_4kw_data/train_post_4kw.csv\"\r\n#     train_post4kw_df = pd.read_csv(path,\r\n#                         header=None,\r\n#                         usecols=[0,1,3],\r\n#                         names=['query_id','query','title'],\r\n#                         )\r\n#     print(train_post4kw_df.__len__())\r\n    \r\n#     train_post4kw_df_result = my_feat_calc_func(train_post4kw_df)\r\n#     print(\"done！\")\r\n    \r\n#     train_post4kw_df_result=reduce_mem_usage(train_post4kw_df_result,verbose=True)\r\n#     train_post4kw_df_result.to_hdf(train_df_post4kw_flag_path, index=None, key='data', complevel=9)\r\n#     print(\"save ok\")\r\n\r\n# if test_df_flag:\r\n#     print(\"start test\")\r\n#     path=f\"/home/kesci/input/bytedance/test_final_part1.csv\"\r\n#     test_df = pd.read_csv(path,\r\n#                         header=None,\r\n#                         usecols=[0,1,3],\r\n#                         names=['query_id','query','title'],\r\n#                         )\r\n#     print(test_df.__len__())\r\n    \r\n#     test_df_result = my_feat_calc_func(test_df)\r\n#     print(\"done！\")\r\n\r\n#     test_df_result = reduce_mem_usage(test_df_result,verbose=True)\r\n#     test_df_result.to_hdf(test_df_flag_path, index=None, key='data', complevel=9)\r\n#     print(\"save ok\")\r\n\r\n# if test2_df_flag:\r\n#     print(\"start test2\")\r\n#     path=f\"/home/kesci/input/bytedance/test_final_part2.csv\"\r\n#     test2_df = pd.read_csv(path,\r\n#                         header=None,\r\n#                         usecols=[0,1,3],\r\n#                         names=['query_id','query','title'],\r\n#                         )\r\n#     print(test2_df.__len__())\r\n    \r\n#     test2_df_result = my_feat_calc_func(test2_df)\r\n#     print(\"done！\")\r\n\r\n#     test2_df_result = reduce_mem_usage(test2_df_result,verbose=True)\r\n#     test2_df_result.to_hdf(test2_df_flag_path, index=None, key='data', complevel=9)\r\n#     print(\"save ok\")\r\n","execution_count":null},{"metadata":{"id":"8326B2FB986246F8B3E11F0E729570A6"},"cell_type":"code","outputs":[],"source":"","execution_count":null},{"metadata":{"id":"444C3DEA18FA4CA089120A29150638BA"},"cell_type":"code","outputs":[],"source":"# query_pos_{1-10}_feat\n# my_feat_calc_func 函数是并行计算特征的函数\n# 只需要将包含queryid、query、title列的dataframe传入函数my_feat_calc_func()，即可计算出该类特征\n# 将前一亿、后一亿、test1、test2传入dataframe","execution_count":null},{"metadata":{"id":"C3DAE0C8E99E4216841CF841B67403AA","hide_input":true},"cell_type":"code","outputs":[],"source":"# -*- coding: utf-8 -*- \r\n'''\r\nCreated on Jul 4, 2019\r\n\r\n@author: Greatpan\r\n'''\r\n\r\nimport pandas as pd\r\npd.set_option('display.max_rows', 500)\r\npd.set_option('display.max_columns', 500)\r\n\r\nimport numpy as np\r\n\r\nfrom tqdm import tqdm\r\ntqdm.pandas(desc='Progress')\r\n\r\nfrom multiprocessing import Pool\r\n\r\ndef reduce_mem_usage(D,verbose=True):\r\n    start_mem = D.memory_usage().sum() / 1024**2\r\n    for c, d in zip(D.columns, D.dtypes):\r\n        if d.kind == 'f':\r\n            D[c] = pd.to_numeric(D[c], downcast='float')\r\n        elif d.kind == 'i':\r\n            D[c] = pd.to_numeric(D[c], downcast='signed')\r\n    end_mem = D.memory_usage().sum() / 1024**2\r\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\r\n    return D\r\n\r\ndef keyWordMatchfunc(doc1,doc2,pos_i):\r\n    doc1=doc1.split(sep=\" \")\r\n    doc2=doc2.split(sep=\" \")\r\n    \r\n    if pos_i<len(doc1):\r\n        title_len=min(len(doc2),25) # title只匹配前25个关键词\r\n        for pos_j in range(title_len):\r\n            if doc1[pos_i]==doc2[pos_j]:\r\n                q_pos=pos_j+1       # 如果匹配上了 记录匹配的位置\r\n                break\r\n            elif pos_j==title_len-1:\r\n                q_pos=0             # 如果没有匹配上 赋值为0\r\n    else:\r\n        q_pos=-1                    # 如果后续长度不存在 赋值为-1 \r\n    \r\n    return q_pos\r\n\r\ndef apply_fun(df):\r\n    basic_feat=pd.DataFrame()\r\n    \r\n    df[\"query\"]=df[\"query\"].apply(str.strip)\r\n    df[\"title\"]=df[\"title\"].apply(str.strip)\r\n    \r\n    query_pos_feat=pd.DataFrame()\r\n    \r\n    for pos_i in range(10):\r\n        query_pos_feat['query_pos_'+str(pos_i+1)] = df.apply(\r\n                lambda row:keyWordMatchfunc(row[\"query\"],row.title,pos_i),axis=1).astype(np.int8)\r\n    \r\n    return query_pos_feat\r\n\r\ndef my_feat_calc_func(RawData):\r\n    '''  关键词长度相关特征  a'''\r\n    multiprocessing_nums=16\r\n    df_parts = np.array_split(RawData, multiprocessing_nums)\r\n    with Pool(processes=multiprocessing_nums) as pool:\r\n        result_parts = pool.map(apply_fun, df_parts)\r\n    pool.join()\r\n    \r\n    result_parallel = pd.concat(result_parts)\r\n    \r\n    return result_parallel\r\n\r\n# train_df_post4kw_flag=False\r\n# train_df_post4kw_flag_path=f'./post_4kw_data/train/query_pos_train_feat.h5'\r\n\r\n# train_df_pre3billion_flag=True\r\n# train_df_pre3billion_result_path=f'./pre_3billion_data/train/query_pos_train_feat.h5'\r\n\r\n# test_df_flag=False\r\n# test_df_flag_path=f'/home/kesci/work/post_4kw_data/test1/query_pos_test1_feat.h5'\r\n\r\n# test2_df_flag=False\r\n# test2_df_flag_path=f'/home/kesci/work/post_4kw_data/test2/query_pos_test2_feat.h5'\r\n\r\n# if train_df_pre3billion_flag:\r\n#     parts_num=10\r\n#     result_parts=[]\r\n#     for index in range(parts_num):\r\n#         print(\"start 前3亿的train...\")\r\n#         path=f\"/home/kesci/input/bytedance/train_final.csv\"\r\n#         train_df_pre_3billion = pd.read_csv(path,\r\n#                                             header=None,\r\n#                                             skiprows=10000000*index,\r\n#                                             nrows=10000000,\r\n#                                             usecols=[0,1,3],\r\n#                                             names=['query_id','query','title'],\r\n#                                             )\r\n#         print(train_df_pre_3billion.__len__())\r\n    \r\n#         result_part = my_feat_calc_func(train_df_pre_3billion)\r\n#         print(f\"part{index} done！\")\r\n    \r\n#         result_parts.append(result_part)\r\n    \r\n#     train_df_pre_3billion_result = pd.concat(result_parts)\r\n#     print(\"done！\")\r\n\r\n#     train_df_pre_3billion_result = reduce_mem_usage(train_df_pre_3billion_result,verbose=True)\r\n#     train_df_pre_3billion_result.to_hdf(train_df_pre3billion_result_path, index=None, key='data', complevel=9)\r\n#     print(\"save ok, len=\",train_df_pre_3billion_result.__len__())\r\n    \r\n# if train_df_post4kw_flag:\r\n#     print(\"start 后4kw的train...\")\r\n#     path=f\"/home/kesci/work/post_4kw_data/train_post_4kw.csv\"\r\n#     train_post4kw_df = pd.read_csv(path,\r\n#                         header=None,\r\n#                         usecols=[0,1,3],\r\n#                         names=['query_id','query','title'],\r\n#                         )\r\n#     print(train_post4kw_df.__len__())\r\n    \r\n#     train_post4kw_df_result = my_feat_calc_func(train_post4kw_df)\r\n#     print(\"done！\")\r\n    \r\n#     train_post4kw_df_result=reduce_mem_usage(train_post4kw_df_result,verbose=True)\r\n#     train_post4kw_df_result.to_hdf(train_df_post4kw_flag_path, index=None, key='data', complevel=9)\r\n#     print(\"save ok\")\r\n\r\n# if test_df_flag:\r\n#     print(\"start test\")\r\n#     path=f\"/home/kesci/input/bytedance/test_final_part1.csv\"\r\n#     test_df = pd.read_csv(path,\r\n#                         header=None,\r\n#                         usecols=[0,1,3],\r\n#                         names=['query_id','query','title'],\r\n#                         )\r\n#     print(test_df.__len__())\r\n    \r\n#     test_df_result = my_feat_calc_func(test_df)\r\n#     print(\"done！\")\r\n\r\n#     test_df_result = reduce_mem_usage(test_df_result,verbose=True)\r\n#     test_df_result.to_hdf(test_df_flag_path, index=None, key='data', complevel=9)\r\n#     print(\"save ok\")\r\n\r\n# if test2_df_flag:\r\n#     print(\"start test2\")\r\n#     path=f\"/home/kesci/input/bytedance/test_final_part2.csv\"\r\n#     test2_df = pd.read_csv(path,\r\n#                         header=None,\r\n#                         usecols=[0,1,3],\r\n#                         names=['query_id','query','title'],\r\n#                         )\r\n#     print(test2_df.__len__())\r\n    \r\n#     test2_df_result = my_feat_calc_func(test2_df)\r\n#     print(\"done！\")\r\n\r\n#     test2_df_result = reduce_mem_usage(test2_df_result,verbose=True)\r\n#     test2_df_result.to_hdf(test2_df_flag_path, index=None, key='data', complevel=9)\r\n#     print(\"save ok\")\r\n","execution_count":null},{"metadata":{"id":"C8EE8955587147BE88B477590339CDE8"},"cell_type":"code","outputs":[],"source":"","execution_count":null},{"metadata":{"id":"3710DACDE9114BB481564C600BF51533"},"cell_type":"code","outputs":[],"source":"# title_pos_{1-25}_feat\n# my_feat_calc_func 函数是并行计算特征的函数\n# 只需要将包含queryid、query、title列的dataframe传入函数my_feat_calc_func()，即可计算出该类特征\n# 将前一亿、后一亿、test1、test2传入dataframe","execution_count":null},{"metadata":{"id":"048572E550A941E38E15A98ABED70D65","hide_input":true},"cell_type":"code","outputs":[],"source":"# -*- coding: utf-8 -*- \r\n'''\r\nCreated on Jul 4, 2019\r\n\r\n@author: Greatpan\r\n'''\r\n\r\nimport pandas as pd\r\npd.set_option('display.max_rows', 500)\r\npd.set_option('display.max_columns', 500)\r\n\r\nimport numpy as np\r\n\r\nfrom tqdm import tqdm\r\ntqdm.pandas(desc='Progress')\r\n\r\nfrom multiprocessing import Pool\r\n\r\ndef reduce_mem_usage(D,verbose=True):\r\n    start_mem = D.memory_usage().sum() / 1024**2\r\n    for c, d in zip(D.columns, D.dtypes):\r\n        if d.kind == 'f':\r\n            D[c] = pd.to_numeric(D[c], downcast='float')\r\n        elif d.kind == 'i':\r\n            D[c] = pd.to_numeric(D[c], downcast='signed')\r\n    end_mem = D.memory_usage().sum() / 1024**2\r\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\r\n    return D\r\n\r\ndef keyWordMatchfunc(doc2,doc1,pos_i):\r\n    doc1=doc1.split(sep=\" \")\r\n    doc2=doc2.split(sep=\" \")\r\n    \r\n    if pos_i<len(doc1):\r\n        \r\n        title_len=len(doc2)         # title只匹配前25个关键词\r\n        for pos_j in range(title_len):\r\n            if doc1[pos_i]==doc2[pos_j]:\r\n                t_pos=pos_j+1       # 如果匹配上了 记录匹配的位置\r\n                break\r\n            elif pos_j==title_len-1:\r\n                t_pos=0             # 如果没有匹配上 赋值为0\r\n    else:\r\n        t_pos=-1                    # 如果后续长度不存在 赋值为-1 \r\n    \r\n    return t_pos\r\n\r\ndef apply_fun(df):\r\n    basic_feat=pd.DataFrame()\r\n    \r\n    df[\"query\"]=df[\"query\"].apply(str.strip)\r\n    df[\"title\"]=df[\"title\"].apply(str.strip)\r\n    \r\n    title_pos_feat=pd.DataFrame()\r\n    \r\n    for pos_i in range(15):\r\n        title_pos_feat['title_pos_'+str(pos_i+1)] = df.apply(\r\n                lambda row:keyWordMatchfunc(row[\"query\"],row.title,pos_i),axis=1).astype(np.int)\r\n    \r\n    return title_pos_feat\r\n\r\ndef my_feat_calc_func(RawData):\r\n    '''  关键词长度相关特征  a'''\r\n    multiprocessing_nums=16\r\n    df_parts = np.array_split(RawData, multiprocessing_nums)\r\n    with Pool(processes=multiprocessing_nums) as pool:\r\n        result_parts = pool.map(apply_fun, df_parts)\r\n    pool.join()\r\n    \r\n    result_parallel = pd.concat(result_parts)\r\n    \r\n    return result_parallel\r\n\r\n# train_df_post4kw_flag=False\r\n# train_df_post4kw_flag_path=f'./post_4kw_data/train/title_pos_train_feat.h5'\r\n\r\n# train_df_pre3billion_flag=False\r\n# train_df_pre3billion_result_path=f'./pre_3billion_data/train/title_pos_train_feat.h5'\r\n\r\n# test_df_flag=False\r\n# test_df_flag_path=f'/home/kesci/work/post_4kw_data/test1/title_pos_test1_feat.h5'\r\n\r\n# test2_df_flag=True\r\n# test2_df_flag_path=f'/home/kesci/work/post_4kw_data/test2/title_pos_test2_feat.h5'\r\n\r\n# if train_df_pre3billion_flag:\r\n#     parts_num=10\r\n#     result_parts=[]\r\n#     for index in range(parts_num):\r\n#         print(\"start 前3亿的train...\")\r\n#         path=f\"/home/kesci/input/bytedance/train_final.csv\"\r\n#         train_df_pre_3billion = pd.read_csv(path,\r\n#                                             header=None,\r\n#                                             skiprows=10000000*index,\r\n#                                             nrows=10000000,\r\n#                                             usecols=[0,1,3],\r\n#                                             names=['query_id','query','title'],\r\n#                                             )\r\n#         print(train_df_pre_3billion.__len__())\r\n    \r\n#         result_part = my_feat_calc_func(train_df_pre_3billion)\r\n#         print(f\"part{index} done！\")\r\n    \r\n#         result_parts.append(result_part)\r\n    \r\n#     train_df_pre_3billion_result = pd.concat(result_parts)\r\n#     print(\"done！\")\r\n\r\n#     train_df_pre_3billion_result = reduce_mem_usage(train_df_pre_3billion_result,verbose=True)\r\n#     train_df_pre_3billion_result.to_hdf(train_df_pre3billion_result_path, index=None, key='data', complevel=9)\r\n#     print(\"save ok, len=\",train_df_pre_3billion_result.__len__())\r\n\r\n# if train_df_sample_flag:\r\n#     print(\"start 随机采样2400kw的train...\")\r\n#     datatype=\"train\"\r\n#     path=f'/home/kesci/work/featureMap/sample_2400w/sampled_ori_data_24398018.h5'\r\n#     train_sample_df=pd.read_hdf(path, key='data')\r\n#     train_sample_df=train_sample_df.reset_index(drop=True)\r\n#     print(train_sample_df.__len__())\r\n    \r\n#     train_sample_df_result = my_feat_calc_func(train_sample_df)\r\n#     print(\"done！\")\r\n\r\n#     train_sample_df_result=reduce_mem_usage(train_sample_df_result,verbose=True)\r\n#     train_sample_df_result.to_hdf(train_df_sample_flag_path, index=None, key='data', complevel=9)\r\n#     print(\"save ok\")\r\n    \r\n# if train_df_post4kw_flag:\r\n#     print(\"start 后4kw的train...\")\r\n#     path=f\"/home/kesci/work/post_4kw_data/train_post_4kw.csv\"\r\n#     train_post4kw_df = pd.read_csv(path,\r\n#                         header=None,\r\n#                         usecols=[0,1,3],\r\n#                         names=['query_id','query','title'],\r\n#                         )\r\n#     print(train_post4kw_df.__len__())\r\n    \r\n#     train_post4kw_df_result = my_feat_calc_func(train_post4kw_df)\r\n#     print(\"done！\")\r\n    \r\n#     train_post4kw_df_result=reduce_mem_usage(train_post4kw_df_result,verbose=True)\r\n#     train_post4kw_df_result.to_hdf(train_df_post4kw_flag_path, index=None, key='data', complevel=9)\r\n#     print(\"save ok\")\r\n\r\n# if test_df_flag:\r\n#     print(\"start test\")\r\n#     path=f\"/home/kesci/input/bytedance/test_final_part1.csv\"\r\n#     test_df = pd.read_csv(path,\r\n#                         header=None,\r\n#                         usecols=[0,1,3],\r\n#                         names=['query_id','query','title'],\r\n#                         )\r\n#     print(test_df.__len__())\r\n    \r\n#     test_df_result = my_feat_calc_func(test_df)\r\n#     print(\"done！\")\r\n\r\n#     test_df_result = reduce_mem_usage(test_df_result,verbose=True)\r\n#     test_df_result.to_hdf(test_df_flag_path, index=None, key='data', complevel=9)\r\n#     print(\"save ok\")    \r\n\r\n# if test2_df_flag:\r\n#     print(\"start test2\")\r\n#     path=f\"/home/kesci/input/bytedance/test_final_part2.csv\"\r\n#     test2_df = pd.read_csv(path,\r\n#                         header=None,\r\n#                         usecols=[0,1,3],\r\n#                         names=['query_id','query','title'],\r\n#                         )\r\n#     print(test2_df.__len__())\r\n    \r\n#     test2_df_result = my_feat_calc_func(test2_df)\r\n#     print(\"done！\")\r\n\r\n#     test2_df_result = reduce_mem_usage(test2_df_result,verbose=True)\r\n#     test2_df_result.to_hdf(test2_df_flag_path, index=None, key='data', complevel=9)\r\n#     print(\"save ok\")\r\n    ","execution_count":null},{"metadata":{"id":"4A7C7056F124439A88733FB9DF29187F"},"cell_type":"code","outputs":[],"source":"","execution_count":null},{"metadata":{"id":"90A0C58988F2481D9613F306B4BA2F87"},"cell_type":"code","outputs":[],"source":"# sim_feat\n# my_feat_calc_func 函数是并行计算特征的函数\n# 只需要将包含queryid、query、title列的dataframe传入函数my_feat_calc_func()，即可计算出该类特征\n# 将前一亿、后一亿、test1、test2传入dataframe","execution_count":null},{"metadata":{"id":"897E491E42114830857483698C17A5F0","hide_input":true},"cell_type":"code","outputs":[],"source":"# -*- coding: utf-8 -*- \r\n'''\r\nCreated on Jul 4, 2019\r\n\r\n@author: Greatpan\r\n'''\r\n\r\nimport pandas as pd\r\npd.set_option('display.max_rows', 500)\r\npd.set_option('display.max_columns', 500)\r\n\r\nimport numpy as np\r\n\r\nfrom tqdm import tqdm\r\ntqdm.pandas(desc='Progress')\r\n\r\nfrom multiprocessing import Pool\r\n\r\ndef reduce_mem_usage(D,verbose=True):\r\n    start_mem = D.memory_usage().sum() / 1024**2\r\n    for c, d in zip(D.columns, D.dtypes):\r\n        if d.kind == 'f':\r\n            D[c] = pd.to_numeric(D[c], downcast='float')\r\n        elif d.kind == 'i':\r\n            D[c] = pd.to_numeric(D[c], downcast='signed')\r\n    end_mem = D.memory_usage().sum() / 1024**2\r\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\r\n    return D\r\n\r\nimport distance\r\ndef apply_fun(df):\r\n    basic_feat=pd.DataFrame()\r\n    \r\n    df[\"query\"]=df[\"query\"].apply(str.strip)\r\n    df[\"title\"]=df[\"title\"].apply(str.strip)\r\n    \r\n    sim_feat=pd.DataFrame()\r\n    # qt=[[10,10],[10,15],[15,15],[15,25]]\r\n    # qt=[[3,5],[3,10],[4,5],[4,10],[5,5],[5,10],[10,5],[10,10]]\r\n    qt=[[3,3],[3,5],[5,5],[5,10],[10,10],[10,15],[15,15],[15,25]]\r\n    \r\n    sim_func_dict={\"jaccard\":distance.jaccard,\r\n                    # \"sorensen\":distance.sorensen,\r\n                    \"levenshtein\":distance.levenshtein,\r\n                    # \"ratio\":Levenshtein.ratio\r\n    }\r\n    \r\n    for sim_func in tqdm(sim_func_dict):\r\n        sim_feat[sim_func] = df.apply(lambda row:sim_func_dict[sim_func](row[\"query\"].split(sep=\" \"),\r\n                                                row[\"title\"].split(sep=\" \")),\r\n                                                axis=1)\r\n        for qt_len in tqdm(qt):\r\n            if qt_len[0]==3 and sim_func==\"levenshtein\":\r\n                pass\r\n            else:\r\n                sim_feat[sim_func+'_q'+str(qt_len[0])+'_t'+str(qt_len[1])] = df.apply(\r\n                    lambda row:sim_func_dict[sim_func]( row[\"query\"].split(sep=\" \")[:qt_len[0]],\r\n                                                        row[\"title\"].split(sep=\" \")[:qt_len[1]]),\r\n                                                        axis=1)\r\n    \r\n    return sim_feat\r\n\r\ndef my_feat_calc_func(RawData):\r\n    '''  关键词长度相关特征  a'''\r\n    multiprocessing_nums=16\r\n    df_parts = np.array_split(RawData, multiprocessing_nums)\r\n    with Pool(processes=multiprocessing_nums) as pool:\r\n        result_parts = pool.map(apply_fun, df_parts)\r\n    pool.join()\r\n    \r\n    result_parallel = pd.concat(result_parts)\r\n    \r\n    return result_parallel\r\n\r\n# train_df_post4kw_flag=False\r\n# train_df_post4kw_flag_path=f'./post_4kw_data/train/sim_j_train_feat.h5'\r\n\r\n# train_df_pre3billion_flag=True\r\n# train_df_pre3billion_result_path=f'./pre_3billion_data/train/sim_510_train_feat_new.h5'\r\n\r\n# test_df_flag=False\r\n# test_df_flag_path=f'/home/kesci/work/post_4kw_data/test1/sim_j_test1_feat.h5'\r\n\r\n# test2_df_flag=False\r\n# test2_df_flag_path=f'/home/kesci/work/post_4kw_data/test2/sim_j_test2_feat.h5'\r\n\r\n# if train_df_pre3billion_flag:\r\n#     parts_num=10\r\n#     result_parts=[]\r\n#     for index in range(5,parts_num):\r\n#         print(\"start 前3亿的train...\")\r\n#         path=f\"/home/kesci/input/bytedance/train_final.csv\"\r\n#         train_df_pre_3billion = pd.read_csv(path,\r\n#                                             header=None,\r\n#                                             skiprows=10000000*index,\r\n#                                             nrows=10000000,\r\n#                                             usecols=[0,1,3],\r\n#                                             names=['query_id','query','title'],\r\n#                                             )\r\n#         print(train_df_pre_3billion.__len__())\r\n    \r\n#         result_part = my_feat_calc_func(train_df_pre_3billion)\r\n#         print(f\"part{index} done！\")\r\n    \r\n#         result_parts.append(result_part)\r\n    \r\n#     train_df_pre_3billion_result = pd.concat(result_parts)\r\n#     print(\"done！\")\r\n\r\n#     train_df_pre_3billion_result = reduce_mem_usage(train_df_pre_3billion_result,verbose=True)\r\n#     train_df_pre_3billion_result.to_hdf(train_df_pre3billion_result_path, index=None, key='data', complevel=9)\r\n#     print(\"save ok, len=\",train_df_pre_3billion_result.__len__())\r\n    \r\n# if train_df_post4kw_flag:\r\n#     print(\"start 后4kw的train...\")\r\n#     path=f\"/home/kesci/work/post_4kw_data/train_post_4kw.csv\"\r\n#     train_post4kw_df = pd.read_csv(path,\r\n#                         header=None,\r\n#                         usecols=[0,1,3],\r\n#                         names=['query_id','query','title'],\r\n#                         )\r\n#     print(train_post4kw_df.__len__())\r\n    \r\n#     train_post4kw_df_result = my_feat_calc_func(train_post4kw_df)\r\n#     print(\"done！\")\r\n    \r\n#     train_post4kw_df_result=reduce_mem_usage(train_post4kw_df_result,verbose=True)\r\n#     train_post4kw_df_result.to_hdf(train_df_post4kw_flag_path, index=None, key='data', complevel=9)\r\n#     print(\"save ok\")\r\n\r\n# if test_df_flag:\r\n#     print(\"start test\")\r\n#     path=f\"/home/kesci/input/bytedance/test_final_part1.csv\"\r\n#     test_df = pd.read_csv(path,\r\n#                         header=None,\r\n#                         usecols=[0,1,3],\r\n#                         names=['query_id','query','title'],\r\n#                         )\r\n#     print(test_df.__len__())\r\n    \r\n#     test_df_result = my_feat_calc_func(test_df)\r\n#     print(\"done！\")\r\n\r\n#     test_df_result = reduce_mem_usage(test_df_result,verbose=True)\r\n#     test_df_result.to_hdf(test_df_flag_path, index=None, key='data', complevel=9)\r\n#     print(\"save ok\")\r\n    \r\n# if test2_df_flag:\r\n#     print(\"start test2\")\r\n#     path=f\"/home/kesci/input/bytedance/test_final_part2.csv\"\r\n#     test2_df = pd.read_csv(path,\r\n#                         header=None,\r\n#                         usecols=[0,1,3],\r\n#                         names=['query_id','query','title'],\r\n#                         )\r\n#     print(test2_df.__len__())\r\n    \r\n#     test2_df_result = my_feat_calc_func(test2_df)\r\n#     print(\"done！\")\r\n\r\n#     test2_df_result = reduce_mem_usage(test2_df_result,verbose=True)\r\n#     test2_df_result.to_hdf(test2_df_flag_path, index=None, key='data', complevel=9)\r\n#     print(\"save ok\")    \r\n","execution_count":null},{"metadata":{"id":"16A375CC835E43D1B4A3EB1D4C2501C1"},"cell_type":"code","outputs":[],"source":"","execution_count":null},{"metadata":{"id":"42B1B5FF2CBC4A3D95BBB2F25770EE22"},"cell_type":"code","outputs":[],"source":"# Editdistance、relative_pos 特征\n# my_feat_calc_func 函数是并行计算特征的函数\n# 只需要将包含queryid、query、title列的dataframe传入函数my_feat_calc_func()，即可计算出该类特征\n# 将前一亿、后一亿、test1、test2传入dataframe","execution_count":null},{"metadata":{"id":"67E0FEBA3B82489A80B3907A9271CD60","hide_input":true},"cell_type":"code","outputs":[],"source":"# -*- coding: utf-8 -*- \r\n'''\r\nCreated on Jul 4, 2019\r\n\r\n@author: Greatpan\r\n'''\r\n\r\nimport pandas as pd\r\npd.set_option('display.max_rows', 500)\r\npd.set_option('display.max_columns', 500)\r\n\r\nimport numpy as np\r\n\r\nfrom tqdm import tqdm\r\ntqdm.pandas(desc='Progress')\r\n\r\nfrom multiprocessing import Pool\r\n\r\ndef reduce_mem_usage(D,verbose=True):\r\n    start_mem = D.memory_usage().sum() / 1024**2\r\n    for c, d in zip(D.columns, D.dtypes):\r\n        if d.kind == 'f':\r\n            D[c] = pd.to_numeric(D[c], downcast='float')\r\n        elif d.kind == 'i':\r\n            D[c] = pd.to_numeric(D[c], downcast='signed')\r\n    end_mem = D.memory_usage().sum() / 1024**2\r\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\r\n    return D\r\n\r\ndef editdistance(str1, str2):\r\n    edit = [[i + j for j in range(len(str2) + 1)] for i in range(len(str1) + 1)]\r\n\r\n    for i in range(1, len(str1) + 1):\r\n        for j in range(1, len(str2) + 1):\r\n            if str1[i - 1] == str2[j - 1]:\r\n                d = 0\r\n            else:\r\n                d = 1\r\n            edit[i][j] = min(edit[i - 1][j] + 1, edit[i][j - 1] + 1, edit[i - 1][j - 1] + d)\r\n\r\n    return edit[len(str1)][len(str2)]\r\n\r\ndef relative_position(query,title):\r\n    query=query.strip().split()\r\n    title=title.strip().split()\r\n    query_set=set(query)\r\n    title_set=set(title)\r\n    query_title_set=query_set & title_set\r\n    if len(query_title_set)!=0:\r\n        relative_p=0\r\n        for q in query_title_set:\r\n            relative_p+=title.index(q)\r\n        relative_p=relative_p/len(query_title_set)\r\n    else:\r\n        relative_p=0\r\n    return relative_p\r\n\r\ndef apply_fun(df):\r\n    basic_feat=pd.DataFrame()\r\n    \r\n    df[\"query\"]=df[\"query\"].apply(str.strip)\r\n    df[\"title\"]=df[\"title\"].apply(str.strip)\r\n    \r\n    my_feat=pd.DataFrame()\r\n    my_feat['editdistance'] = df.apply(lambda x: \r\n                                editdistance(x['query'].strip().split(' '), x['title'].strip().split(' ')),\r\n                                axis=1)\r\n    \r\n    my_feat['relative_pos'] = df.apply(lambda x: \r\n                                relative_position(x['query'], x['title']),\r\n                                axis=1)\r\n    \r\n    return my_feat\r\n\r\ndef my_feat_calc_func(RawData):\r\n    '''  关键词长度相关特征  a'''\r\n    multiprocessing_nums=16\r\n    df_parts = np.array_split(RawData, multiprocessing_nums)\r\n    with Pool(processes=multiprocessing_nums) as pool:\r\n        result_parts = pool.map(apply_fun, df_parts)\r\n    pool.join()\r\n    \r\n    result_parallel = pd.concat(result_parts)\r\n    \r\n    return result_parallel\r\n\r\n# train_df_post4kw_flag=False\r\n# train_df_post4kw_flag_path=f'./post_4kw_data/train/editdistance_relativepos_train_feat.h5'\r\n\r\n# train_df_pre3billion_flag=False\r\n# train_df_pre3billion_result_path=f'./pre_3billion_data/train/editdistance_relativepos_train_feat.h5'\r\n\r\n# test_df_flag=False\r\n# test_df_flag_path=f'/home/kesci/work/post_4kw_data/test1/editdistance_relativepos_test1_feat.h5'\r\n\r\n# test2_df_flag=True\r\n# test2_df_flag_path=f'/home/kesci/work/post_4kw_data/test2/editdistance_relativepos_test2_feat.h5'\r\n\r\n# if train_df_pre3billion_flag:\r\n#     parts_num=10\r\n#     result_parts=[]\r\n#     for index in range(parts_num):\r\n#         print(\"start 前3亿的train...\")\r\n#         path=f\"/home/kesci/input/bytedance/train_final.csv\"\r\n#         train_df_pre_3billion = pd.read_csv(path,\r\n#                                             header=None,\r\n#                                             skiprows=10000000*index,\r\n#                                             nrows=10000000,\r\n#                                             usecols=[0,1,3],\r\n#                                             names=['query_id','query','title'],\r\n#                                             )\r\n#         print(train_df_pre_3billion.__len__())\r\n    \r\n#         result_part = my_feat_calc_func(train_df_pre_3billion)\r\n#         print(f\"part{index} done！\")\r\n    \r\n#         result_parts.append(result_part)\r\n    \r\n#     train_df_pre_3billion_result = pd.concat(result_parts)\r\n#     print(\"done！\")\r\n\r\n#     train_df_pre_3billion_result = reduce_mem_usage(train_df_pre_3billion_result,verbose=True)\r\n#     train_df_pre_3billion_result.to_hdf(train_df_pre3billion_result_path, index=None, key='data', complevel=9)\r\n#     print(\"save ok, len=\",train_df_pre_3billion_result.__len__())\r\n\r\n# if train_df_post4kw_flag:\r\n#     print(\"start 后4kw的train...\")\r\n#     path=f\"/home/kesci/work/post_4kw_data/train_post_4kw.csv\"\r\n#     train_post4kw_df = pd.read_csv(path,\r\n#                         header=None,\r\n#                         usecols=[0,1,3],\r\n#                         names=['query_id','query','title'],\r\n#                         )\r\n#     print(train_post4kw_df.__len__())\r\n    \r\n#     train_post4kw_df_result = my_feat_calc_func(train_post4kw_df)\r\n#     print(\"done！\")\r\n    \r\n#     train_post4kw_df_result=reduce_mem_usage(train_post4kw_df_result,verbose=True)\r\n#     train_post4kw_df_result.to_hdf(train_df_post4kw_flag_path, index=None, key='data', complevel=9)\r\n#     print(\"save ok\")\r\n\r\n# if test_df_flag:\r\n#     print(\"start test\")\r\n#     path=f\"/home/kesci/input/bytedance/test_final_part1.csv\"\r\n#     test_df = pd.read_csv(path,\r\n#                         header=None,\r\n#                         usecols=[0,1,3],\r\n#                         names=['query_id','query','title'],\r\n#                         )\r\n#     print(test_df.__len__())\r\n    \r\n#     test_df_result = my_feat_calc_func(test_df)\r\n#     print(\"done！\")\r\n\r\n#     test_df_result = reduce_mem_usage(test_df_result,verbose=True)\r\n#     test_df_result.to_hdf(test_df_flag_path, index=None, key='data', complevel=9)\r\n#     print(\"save ok\")\r\n    \r\n# if test2_df_flag:\r\n#     print(\"start test2\")\r\n#     path=f\"/home/kesci/input/bytedance/test_final_part2.csv\"\r\n#     test2_df = pd.read_csv(path,\r\n#                         header=None,\r\n#                         usecols=[0,1,3],\r\n#                         names=['query_id','query','title'],\r\n#                         )\r\n#     print(test2_df.__len__())\r\n    \r\n#     test2_df_result = my_feat_calc_func(test2_df)\r\n#     print(\"done！\")\r\n\r\n#     test2_df_result = reduce_mem_usage(test2_df_result,verbose=True)\r\n#     test2_df_result.to_hdf(test2_df_flag_path, index=None, key='data', complevel=9)\r\n    # print(\"save ok\")","execution_count":null},{"metadata":{"id":"99506C02E352488EA45BD607EFDF01FA"},"cell_type":"code","outputs":[],"source":"","execution_count":null},{"metadata":{"id":"CF1F2B38909C48049596E1A37C23184D"},"cell_type":"code","outputs":[],"source":"# BM25特征\n# my_feat_calc_func 函数是并行计算特征的函数\n# 只需要将包含queryid、query、title列的dataframe传入函数my_feat_calc_func()，即可计算出该类特征\n# 将前一亿、后一亿、test1、test2传入dataframe","execution_count":null},{"metadata":{"id":"E6B8DB06A8C645CEB41A5D42DAC067A4","hide_input":true},"cell_type":"code","outputs":[],"source":"import pandas as pd\r\nfrom gensim.summarization.bm25 import BM25\r\n\r\nfrom tqdm import tqdm\r\n\r\n# -*- coding: utf-8 -*- \r\n'''\r\nCreated on Jul 4, 2019\r\n\r\n@author: Greatpan\r\n'''\r\n\r\nimport pandas as pd\r\npd.set_option('display.max_rows', 500)\r\npd.set_option('display.max_columns', 500)\r\n\r\nimport numpy as np\r\n\r\nfrom tqdm import tqdm\r\ntqdm.pandas(desc='Progress')\r\n\r\nimport time\r\n\r\nfrom multiprocessing import Pool\r\n\r\ndef reduce_mem_usage(D,verbose=True):\r\n    start_mem = D.memory_usage().sum() / 1024**2\r\n    for c, d in zip(D.columns, D.dtypes):\r\n        if d.kind == 'f':\r\n            D[c] = pd.to_numeric(D[c], downcast='float')\r\n        elif d.kind == 'i':\r\n            D[c] = pd.to_numeric(D[c], downcast='signed')\r\n    end_mem = D.memory_usage().sum() / 1024**2\r\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\r\n    return D\r\n\r\ndef apply_fun(df):\r\n    query_id_group = df.groupby(['query_id'])\r\n    bm_list = []\r\n    for name, group in query_id_group:\r\n        corpus = group['title'].values.tolist()\r\n        corpus = [sentence.strip().split() for sentence in corpus]\r\n        query = group['query'].values[0].strip().split()\r\n        bm = BM25(corpus)\r\n        bmscore = bm.get_scores(query)\r\n        bm_list.extend(bmscore)\r\n    \r\n    return pd.DataFrame(bm_list,columns=[\"BM25\"])\r\n\r\ndef my_feat_calc_func(RawData):\r\n    '''  关键词长度相关特征  a'''\r\n    multiprocessing_nums=13\r\n    df_parts = np.array_split(RawData, multiprocessing_nums)\r\n    with Pool(processes=multiprocessing_nums) as pool:\r\n        result_parts = pool.map(apply_fun, df_parts)\r\n    pool.join()\r\n    \r\n    result_parallel = pd.concat(result_parts)\r\n    \r\n    return result_parallel\r\n\r\n# train_df_pre2billion_result_path=f'./pre_2e_data/train/BM25_train_feat.h5'\r\n\r\n# print(\"start 前2亿的train...\")\r\n# path=f\"/home/kesci/input/bytedance/train_final.csv\"\r\n# train_df_pre_2billion = pd.read_csv(path,\r\n#                                     header=None,\r\n#                                     skiprows=100000000,\r\n#                                     nrows=100000000,\r\n#                                     usecols=[0,1,3],\r\n#                                     names=['query_id','query','title'],\r\n#                                     )\r\n# print(train_df_pre_2billion.__len__())\r\n\r\n# parts_num=10\r\n# result_parts=[]\r\n# for index in tqdm(range(parts_num)):\r\n#     result_part = my_feat_calc_func(train_df_pre_2billion[10000000*index:10000000*(index+1)])\r\n#     print(f\"part{index} done！\")\r\n\r\n#     result_parts.append(result_part)\r\n\r\n# train_df_pre_2billion_result = pd.concat(result_parts)\r\n# print(\"done！\")\r\n\r\n# train_df_pre_2billion_result = reduce_mem_usage(train_df_pre_2billion_result,verbose=True)\r\n# train_df_pre_2billion_result.to_hdf(train_df_pre2billion_result_path, index=None, key='data', complevel=9)\r\n# print(\"save ok, len=\",train_df_pre_2billion_result.__len__())\r\n\r\n# result_part = my_feat_calc_func(train_df_pre_2billion[:1000])\r\n# result_part.head()","execution_count":null},{"metadata":{"id":"F6C82097D86845BCBBBE8E4F2C3EF8F3"},"cell_type":"code","outputs":[],"source":"","execution_count":null},{"metadata":{"id":"98BDCE5FA976478F80672920E118A773"},"cell_type":"code","outputs":[],"source":"# 计算query_match_feat 特征\n# my_feat_calc_func 函数是并行计算特征的函数\n# 只需要将包含queryid、query、title列的dataframe传入函数my_feat_calc_func()，即可计算出该类特征\n# 将前一亿、后一亿、test1、test2传入dataframe","execution_count":null},{"metadata":{"id":"742DDA518A854632942F22B77C243740","hide_input":true},"cell_type":"code","outputs":[],"source":"# -*- coding: utf-8 -*- \n'''\nCreated on Jul 4, 2019\n\n@author: Greatpan\n'''\n\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\n\nimport numpy as np\n\nfrom tqdm import tqdm\ntqdm.pandas(desc='Progress')\n\nimport time\n\nfrom multiprocessing import Pool\n\ndef reduce_mem_usage(D,verbose=True):\n    start_mem = D.memory_usage().sum() / 1024**2\n    for c, d in zip(D.columns, D.dtypes):\n        if d.kind == 'f':\n            D[c] = pd.to_numeric(D[c], downcast='float')\n        elif d.kind == 'i':\n            D[c] = pd.to_numeric(D[c], downcast='signed')\n    end_mem = D.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return D\n\ndef wordblockMatchdel(doc1, doc2):\n    doc1 = doc1.split(sep=\" \")\n    doc2 = doc2.split(sep=\" \")\n    q1_start = len(doc1)\n    q1_end = 0\n    count = 0\n    blockcount = 0\n    i = 0\n    j = 0\n    maxMatchBlockLen = 0\n\n    while i < len(doc1):\n        for j in range(len(doc2)):\n            if doc1[i] == doc2[j]:\n\n                q1_start = min(q1_start, j)\n                count = count+1\n                blockcount = blockcount+1\n                maxMatchLen_tmp = 1\n                while i+1 < len(doc1) and j + \\\n                        1 < len(doc2) and doc1[i+1] == doc2[j+1]:\n                    maxMatchLen_tmp = maxMatchLen_tmp+1\n                    count = count+1\n                    i = i+1\n                    j = j+1\n                maxMatchBlockLen = max(maxMatchBlockLen, maxMatchLen_tmp)\n                q1_end = max(q1_end, j)\n                break\n        i = i+1\n    if q1_end-q1_start < 0:\n        q1_start = 0\n        q1_end = 0\n    q1_proximity = q1_end-q1_start\n    return [count, blockcount, q1_proximity,\n            maxMatchBlockLen, q1_start, q1_end]\n\ndef apply_fun(df):\n    match_feat_tmp=pd.DataFrame()\n    \n    df[\"query\"]=df[\"query\"].apply(str.strip)\n    df[\"title\"]=df[\"title\"].apply(str.strip)\n    \n    match_feat_tmp[\"match_tmp\"] = df.apply(\n            lambda row: wordblockMatchdel(\n                row[\"query\"], row.title), axis=1)\n    \n    match_feat=pd.DataFrame()\n    match_feat[\"count_match\"] = match_feat_tmp.apply(\n            lambda row: row[\"match_tmp\"][0], axis=1)\n    \n    match_feat[\"blockcount_match\"] = match_feat_tmp.apply(\n            lambda row: row[\"match_tmp\"][1], axis=1)\n    \n    match_feat[\"proximity\"] = match_feat_tmp.apply(\n            lambda row: row[\"match_tmp\"][2], axis=1)\n    \n    match_feat[\"maxMatchBlockLen\"] = match_feat_tmp.apply(\n            lambda row: row[\"match_tmp\"][3], axis=1)\n    \n    match_feat[\"q1_match_start\"] = match_feat_tmp.apply(\n            lambda row: row[\"match_tmp\"][4], axis=1)\n    \n    match_feat[\"q1_match_end\"] = match_feat_tmp.apply(\n            lambda row: row[\"match_tmp\"][5], axis=1)\n    \n    return match_feat\n\ndef my_feat_calc_func(RawData):\n    '''  关键词长度相关特征  a'''\n    multiprocessing_nums=16\n    df_parts = np.array_split(RawData, multiprocessing_nums)\n    with Pool(processes=multiprocessing_nums) as pool:\n        result_parts = pool.map(apply_fun, df_parts)\n    pool.join()\n    \n    result_parallel = pd.concat(result_parts)\n    \n    return result_parallel\n\n# train_df_post4kw_flag=False\n# train_df_post4kw_flag_path=f'./post_4kw_data/train/query_match_train_feat.h5'\n\n# train_df_pre3billion_flag=False\n# train_df_pre3billion_result_path=f'./pre_3billion_data/train/query_match_train_feat.h5'\n\n# test_df_flag=False\n# test_df_flag_path=f'/home/kesci/work/post_4kw_data/test1/query_match_test1_feat.h5'\n\n# test2_df_flag=True\n# test2_df_flag_path=f'/home/kesci/work/post_4kw_data/test2/query_match_test2_feat.h5'\n\n# if train_df_pre3billion_flag:\n#     parts_num=10\n#     result_parts=[]\n#     for index in range(parts_num):\n#         print(\"start 前3亿的train...\")\n#         path=f\"/home/kesci/input/bytedance/train_final.csv\"\n#         train_df_pre_3billion = pd.read_csv(path,\n#                                             header=None,\n#                                             skiprows=10000000*index,\n#                                             nrows=10000000,\n#                                             usecols=[0,1,3],\n#                                             names=['query_id','query','title'],\n#                                             )\n#         print(train_df_pre_3billion.__len__())\n    \n#         result_part = my_feat_calc_func(train_df_pre_3billion)\n#         print(f\"part{index} done！\")\n    \n#         result_parts.append(result_part)\n    \n#     train_df_pre_3billion_result = pd.concat(result_parts)\n#     print(\"done！\")\n\n#     train_df_pre_3billion_result = reduce_mem_usage(train_df_pre_3billion_result,verbose=True)\n#     train_df_pre_3billion_result.to_hdf(train_df_pre3billion_result_path, index=None, key='data', complevel=9)\n#     print(\"save ok, len=\",train_df_pre_3billion_result.__len__())\n    \n# if train_df_post4kw_flag:\n#     print(\"start 后4kw的train...\")\n#     path=f\"/home/kesci/work/post_4kw_data/train_post_4kw.csv\"\n#     train_post4kw_df = pd.read_csv(path,\n#                         header=None,\n#                         usecols=[0,1,3],\n#                         names=['query_id','query','title'],\n#                         )\n#     print(train_post4kw_df.__len__())\n    \n#     train_post4kw_df_result = my_feat_calc_func(train_post4kw_df)\n#     print(\"done！\")\n    \n#     train_post4kw_df_result=reduce_mem_usage(train_post4kw_df_result,verbose=True)\n#     train_post4kw_df_result.to_hdf(train_df_post4kw_flag_path, index=None, key='data', complevel=9)\n#     print(\"save ok\")\n\n# if test_df_flag:\n#     print(\"start test\")\n#     path=f\"/home/kesci/input/bytedance/test_final_part1.csv\"\n#     test_df = pd.read_csv(path,\n#                         header=None,\n#                         usecols=[0,1,3],\n#                         names=['query_id','query','title'],\n#                         )\n#     print(test_df.__len__())\n    \n#     test_df_result = my_feat_calc_func(test_df)\n#     print(\"done！\")\n\n#     test_df_result = reduce_mem_usage(test_df_result,verbose=True)\n#     test_df_result.to_hdf(test_df_flag_path, index=None, key='data', complevel=9)\n#     print(\"save ok\")\n    \n# if test2_df_flag:\n#     print(\"start test2\")\n#     path=f\"/home/kesci/input/bytedance/test_final_part2.csv\"\n#     test2_df = pd.read_csv(path,\n#                         header=None,\n#                         usecols=[0,1,3],\n#                         names=['query_id','query','title'],\n#                         )\n#     print(test2_df.__len__())\n    \n#     test2_df_result = my_feat_calc_func(test2_df)\n#     print(\"done！\")\n\n#     test2_df_result = reduce_mem_usage(test2_df_result,verbose=True)\n#     test2_df_result.to_hdf(test2_df_flag_path, index=None, key='data', complevel=9)\n#     print(\"save ok\")","execution_count":null},{"metadata":{"id":"211976EDDDA0496585F31761BE23383D"},"cell_type":"code","outputs":[],"source":"","execution_count":null},{"metadata":{"id":"C14DA88226D7496C8F3530A8E44B56E3"},"cell_type":"code","outputs":[],"source":"# fuzz特征\n# my_feat_calc_func 函数是并行计算特征的函数\n# 只需要将包含queryid、query、title列的dataframe传入函数my_feat_calc_func()，即可计算出该类特征\n# 将前一亿、后一亿、test1、test2传入dataframe","execution_count":null},{"metadata":{"id":"E6FF175E2F40421CA45A25CD6E2D5291","hide_input":true},"cell_type":"code","outputs":[],"source":"# -*- coding: utf-8 -*- \r\n'''\r\nCreated on Jul 4, 2019\r\n\r\n@author: Greatpan\r\n'''\r\n\r\nimport pandas as pd\r\npd.set_option('display.max_rows', 500)\r\npd.set_option('display.max_columns', 500)\r\n\r\nimport numpy as np\r\n\r\nfrom tqdm import tqdm\r\ntqdm.pandas(desc='Progress')\r\n\r\nfrom multiprocessing import Pool\r\n\r\ndef reduce_mem_usage(D,verbose=True):\r\n    start_mem = D.memory_usage().sum() / 1024**2\r\n    for c, d in zip(D.columns, D.dtypes):\r\n        if d.kind == 'f':\r\n            D[c] = pd.to_numeric(D[c], downcast='float')\r\n        elif d.kind == 'i':\r\n            D[c] = pd.to_numeric(D[c], downcast='signed')\r\n    end_mem = D.memory_usage().sum() / 1024**2\r\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\r\n    return D\r\n\r\nfrom fuzzywuzzy import fuzz\r\ndef WRatio(row):\r\n    return fuzz.WRatio(row['query'], row['title'])\r\n\r\ndef QRatio(row):\r\n    return fuzz.QRatio(row['query'], row['title'])\r\n\r\ndef partial_token_set_ratio(row):\r\n    return fuzz.partial_token_set_ratio(row['query'], row['title'])\r\n\r\ndef partial_token_sort_ratio(row):\r\n    return fuzz.partial_token_sort_ratio(row['query'], row['title'])\r\n\r\ndef partial_ratio(row):\r\n    return fuzz.partial_ratio(row['query'], row['title'])\r\n\r\ndef token_set_ratio(row):\r\n    return fuzz.token_set_ratio(row['query'], row['title'])\r\n\r\ndef token_sort_ratio(row):\r\n    return fuzz.token_sort_ratio(row['query'], row['title'])\r\n\r\ndef apply_fun(df):\r\n    df[\"query\"]=df[\"query\"].apply(str.strip)\r\n    df[\"title\"]=df[\"title\"].apply(str.strip)\r\n    \r\n    extracted_feature = pd.DataFrame()\r\n    extracted_feature['token_sort_ratio'] = df.apply(token_sort_ratio, axis=1)\r\n    extracted_feature['token_set_ratio'] = df.apply(token_set_ratio, axis=1)\r\n    extracted_feature['partial_ratio'] = df.apply(partial_ratio, axis=1)\r\n    extracted_feature['partial_token_sort_ratio'] = df.apply(partial_token_sort_ratio, axis=1)\r\n    extracted_feature['partial_token_set_ratio'] = df.apply(partial_token_set_ratio, axis=1)\r\n    extracted_feature['QRatio'] = df.apply(QRatio, axis=1)\r\n    extracted_feature['WRatio'] = df.apply(WRatio, axis=1)\r\n    \r\n    return extracted_feature\r\n\r\ndef my_feat_calc_func(RawData):\r\n    '''  关键词长度相关特征  a'''\r\n    multiprocessing_nums=16\r\n    df_parts = np.array_split(RawData, multiprocessing_nums)\r\n    with Pool(processes=multiprocessing_nums) as pool:\r\n        result_parts = pool.map(apply_fun, df_parts)\r\n    pool.join()\r\n    \r\n    result_parallel = pd.concat(result_parts)\r\n    \r\n    return result_parallel\r\n\r\n# train_df_post4kw_flag=True\r\n# train_df_post4kw_flag_path=f'./post_4kw_data/train/fuzz_train_feat.h5'\r\n\r\n# train_df_pre3billion_flag=True\r\n# train_df_pre3billion_result_path=f'./pre_3billion_data/train/fuzz_train_feat.h5'\r\n\r\n# test_df_flag=True\r\n# test_df_flag_path=f'/home/kesci/work/post_4kw_data/test1/fuzz_test1_feat.h5'\r\n\r\n# test2_df_flag=False\r\n# test2_df_flag_path=f'/home/kesci/work/post_4kw_data/test2/fuzz_test2_feat.h5'\r\n\r\n# if train_df_pre3billion_flag:\r\n#     parts_num=10\r\n#     result_parts=[]\r\n#     for index in range(parts_num):\r\n#         print(\"start 前3亿的train...\")\r\n#         path=f\"/home/kesci/input/bytedance/train_final.csv\"\r\n#         train_df_pre_3billion = pd.read_csv(path,\r\n#                                             header=None,\r\n#                                             skiprows=10000000*index,\r\n#                                             nrows=10000000,\r\n#                                             usecols=[0,1,3],\r\n#                                             names=['query_id','query','title'],\r\n#                                             )\r\n#         print(train_df_pre_3billion.__len__())\r\n    \r\n#         result_part = my_feat_calc_func(train_df_pre_3billion)\r\n#         print(f\"part{index} done！\")\r\n    \r\n#         result_parts.append(result_part)\r\n    \r\n#     train_df_pre_3billion_result = pd.concat(result_parts)\r\n#     print(\"done！\")\r\n\r\n#     train_df_pre_3billion_result = reduce_mem_usage(train_df_pre_3billion_result,verbose=True)\r\n#     train_df_pre_3billion_result.to_hdf(train_df_pre3billion_result_path, index=None, key='data', complevel=9)\r\n#     print(\"save ok, len=\",train_df_pre_3billion_result.__len__())\r\n\r\n# if train_df_post4kw_flag:\r\n#     print(\"start 后4kw的train...\")\r\n#     path=f\"/home/kesci/work/post_4kw_data/train_post_4kw.csv\"\r\n#     train_post4kw_df = pd.read_csv(path,\r\n#                         header=None,\r\n#                         usecols=[0,1,3],\r\n#                         names=['query_id','query','title'],\r\n#                         )\r\n#     print(train_post4kw_df.__len__())\r\n    \r\n#     train_post4kw_df_result = my_feat_calc_func(train_post4kw_df)\r\n#     print(\"done！\")\r\n    \r\n#     train_post4kw_df_result=reduce_mem_usage(train_post4kw_df_result,verbose=True)\r\n#     train_post4kw_df_result.to_hdf(train_df_post4kw_flag_path, index=None, key='data', complevel=9)\r\n#     print(\"save ok\")\r\n\r\n# if test_df_flag:\r\n#     print(\"start test\")\r\n#     path=f\"/home/kesci/input/bytedance/test_final_part1.csv\"\r\n#     test_df = pd.read_csv(path,\r\n#                         header=None,\r\n#                         usecols=[0,1,3],\r\n#                         names=['query_id','query','title'],\r\n#                         )\r\n#     print(test_df.__len__())\r\n    \r\n#     test_df_result = my_feat_calc_func(test_df)\r\n#     print(\"done！\")\r\n\r\n#     test_df_result = reduce_mem_usage(test_df_result,verbose=True)\r\n#     test_df_result.to_hdf(test_df_flag_path, index=None, key='data', complevel=9)\r\n#     print(\"save ok\")\r\n    \r\n# if test2_df_flag:\r\n#     print(\"start test2\")\r\n#     path=f\"/home/kesci/input/bytedance/test_final_part2.csv\"\r\n#     test2_df = pd.read_csv(path,\r\n#                         header=None,\r\n#                         usecols=[0,1,3],\r\n#                         names=['query_id','query','title'],\r\n#                         )\r\n#     print(test2_df.__len__())\r\n    \r\n#     test2_df_result = my_feat_calc_func(test2_df)\r\n#     print(\"done！\")\r\n\r\n#     test2_df_result = reduce_mem_usage(test2_df_result,verbose=True)\r\n#     test2_df_result.to_hdf(test2_df_flag_path, index=None, key='data', complevel=9)\r\n#     print(\"save ok\")","execution_count":null},{"metadata":{"id":"C98168687BA34AAD9AE018005EAD2152"},"cell_type":"code","outputs":[],"source":"","execution_count":null},{"metadata":{"id":"AB8E696AB2B6487D9C2D6D25C95314E8"},"cell_type":"code","outputs":[],"source":"# textpair 特征\n# my_feat_calc_func 函数是并行计算特征的函数\n# 只需要将包含queryid、query、title列的dataframe传入函数my_feat_calc_func()，即可计算出该类特征\n# 将前一亿、后一亿、test1、test2传入dataframe","execution_count":null},{"metadata":{"id":"0192FFA60A2644BBA590CDC518938C27","hide_input":false},"cell_type":"code","outputs":[],"source":"# -*- coding: utf-8 -*- \n'''\nCreated on Jul 4, 2019\n\n@author: Greatpan\n'''\n\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\n\nimport numpy as np\n\nfrom tqdm import tqdm\ntqdm.pandas(desc='Progress')\n\nfrom multiprocessing import Pool\n\ndef reduce_mem_usage(D,verbose=True):\n    start_mem = D.memory_usage().sum() / 1024**2\n    for c, d in zip(D.columns, D.dtypes):\n        if d.kind == 'f':\n            D[c] = pd.to_numeric(D[c], downcast='float')\n        elif d.kind == 'i':\n            D[c] = pd.to_numeric(D[c], downcast='signed')\n    end_mem = D.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return D\n\nfrom fuzzywuzzy import fuzz\ndef token_set_diff(row):\n    query = row['query']\n    title = row['title']\n    q_set = set(query.split())\n    t_set = set(title.split())\n    return abs(len(t_set) - len(q_set))\n\n# 词的不同个数之差（有正负）\ndef wc_diff_unique(row):\n    query = row['query']\n    title = row['title']\n    q_set = set(query.split())\n    t_set = set(title.split())\n    return len(set(t_set)) - len(set(q_set))\n\n# 词的不同个数比值\ndef wc_ratio_unique(row):\n    query = row['query']\n    title = row['title']\n    q_set = set(query.split())\n    t_set = set(title.split())\n    l1 = len(q_set) * 1.0\n    l2 = len(t_set) * 1.0\n    return l2 / l1\n\n# 总的不同的词\ndef total_unique_words(row):\n    query = row['query']\n    query = query.split()\n    title = row['title']\n    t_set = set(title.split())\n    return len(t_set.union(query))\n\ndef same_start(row):\n    query = row['query']\n    title = row['title']\n    l = min(len(query), len(title))\n    result = 0\n    for i in range(l):\n        if query[i] == title[i]:\n            result += len(title[i])\n    return result\n\ndef apply_fun(df):\n    df[\"query\"]=df[\"query\"].apply(str.strip)\n    df[\"title\"]=df[\"title\"].apply(str.strip)\n    \n    extracted_feature = pd.DataFrame()\n    \n    extracted_feature['total_unique_words'] = df.apply(total_unique_words, axis=1)\n    extracted_feature['wc_ratio_unique'] = df.apply(wc_ratio_unique, axis=1)\n    extracted_feature['wc_diff_unique'] = df.apply(wc_diff_unique, axis=1)\n    extracted_feature['token_set_diff'] = df.apply(token_set_diff, axis=1)\n    extracted_feature['same_start'] = df.apply(same_start, axis=1)\n    \n    return extracted_feature\n\ndef my_feat_calc_func(RawData):\n    '''  关键词长度相关特征  a'''\n    multiprocessing_nums=16\n    df_parts = np.array_split(RawData, multiprocessing_nums)\n    with Pool(processes=multiprocessing_nums) as pool:\n        result_parts = pool.map(apply_fun, df_parts)\n    pool.join()\n    \n    result_parallel = pd.concat(result_parts)\n    \n    return result_parallel\n\n# train_df_post4kw_flag=True\n# train_df_post4kw_flag_path=f'./post_4kw_data/train/sen_dis2_train_200.h5'\n\n# train_df_pre3billion_flag=True\n# train_df_pre3billion_result_path=f'./pre_3billion_data/train/sen_dis2_train_200.h5'\n\n# test_df_flag=True\n# test_df_flag_path=f'/home/kesci/work/post_4kw_data/test1/sen_dis2_test1_200.h5'\n\n# test2_df_flag=True\n# test2_df_flag_path=f'/home/kesci/work/post_4kw_data/test2/sen_dis2_test2_200.h5'\n\n# train_df_post1billion_pre6kw_flag=True\n# train_df_post1billion_pre6kw_flag_path=f'./post_4kw_data/train_6kw/sen_dis2_train_200.h5'\n\n# import time\n# import gc\n\n# if train_df_post4kw_flag:\n#     print(\"start 后4kw的train...\")\n#     start=time.time()\n#     path=f\"/home/kesci/work/post_4kw_data/train_post_4kw.csv\"\n#     train_post4kw_df = pd.read_csv(path,\n#                         header=None,\n#                         usecols=[0,1,3],\n#                         names=['query_id','query','title'],\n#                         )\n#     print(train_post4kw_df.__len__())\n    \n#     train_post4kw_df_result = my_feat_calc_func(train_post4kw_df)\n#     print(\"done！\", \", Using time:\",time.time()-start)\n    \n#     train_post4kw_df_result=reduce_mem_usage(train_post4kw_df_result,verbose=True)\n#     train_post4kw_df_result.to_hdf(train_df_post4kw_flag_path, index=None, key='data', complevel=9)\n#     print(\"save ok\")\n#     del train_post4kw_df,train_post4kw_df_result\n#     gc.collect()\n\n# if test_df_flag:\n#     print(\"start test1\")\n#     path=f\"/home/kesci/input/bytedance/test_final_part1.csv\"\n#     test_df = pd.read_csv(path,header=None,usecols=[0,1,3],names=['query_id','query','title'])\n#     print(test_df.__len__())\n    \n#     test_df_result = my_feat_calc_func(test_df)\n#     print(\"done！\")\n\n#     test_df_result = reduce_mem_usage(test_df_result,verbose=True)\n#     test_df_result.to_hdf(test_df_flag_path, index=None, key='data', complevel=9)\n#     print(\"save ok\")\n#     del test_df,test_df_result\n#     gc.collect()\n\n# if test2_df_flag:\n#     parts_num=10\n#     result_parts=[]\n#     for index in range(parts_num):\n#         print(\"start 前3亿的train...\")\n#         start=time.time()\n#         path=f\"/home/kesci/input/bytedance/bytedance_contest.final_2.csv\"\n#         train_df_pre_3billion = pd.read_csv(path,\n#                                             header=None,\n#                                             skiprows=10000000*index,\n#                                             nrows=10000000,\n#                                             usecols=[0,1,3],\n#                                             names=['query_id','query','title'],\n#                                             )\n#         print(train_df_pre_3billion.__len__(),\",Read Using time:\",time.time()-start)\n    \n#         result_part = my_feat_calc_func(train_df_pre_3billion)\n#         print(f\"part{index} done！, Using time:\",time.time()-start)\n    \n#         result_parts.append(result_part)\n    \n#     train_df_pre_3billion_result = pd.concat(result_parts)\n#     print(\"done！\")\n\n#     train_df_pre_3billion_result = reduce_mem_usage(train_df_pre_3billion_result,verbose=True)\n#     train_df_pre_3billion_result.to_hdf(test2_df_flag_path, index=None, key='data', complevel=9)\n#     print(\"save ok, len=\",train_df_pre_3billion_result.__len__())\n#     del train_df_pre_3billion,train_df_pre_3billion_result\n#     gc.collect()\n\n# if train_df_pre3billion_flag:\n#     parts_num=10\n#     result_parts=[]\n#     for index in range(parts_num):\n#         print(\"start 前3亿的train...\")\n#         start=time.time()\n#         path=f\"/home/kesci/input/bytedance/train_final.csv\"\n#         train_df_pre_3billion = pd.read_csv(path,\n#                                             header=None,\n#                                             skiprows=10000000*index,\n#                                             nrows=10000000,\n#                                             usecols=[0,1,3],\n#                                             names=['query_id','query','title'],\n#                                             )\n#         print(train_df_pre_3billion.__len__(),\", Using time:\",time.time()-start)\n    \n#         result_part = my_feat_calc_func(train_df_pre_3billion)\n#         print(f\"part{index} done！\")\n    \n#         result_parts.append(result_part)\n    \n#     train_df_pre_3billion_result = pd.concat(result_parts)\n#     print(\"done！\")\n\n#     train_df_pre_3billion_result = reduce_mem_usage(train_df_pre_3billion_result,verbose=True)\n#     train_df_pre_3billion_result.to_hdf(train_df_pre3billion_result_path, index=None, key='data', complevel=9)\n#     print(\"save ok, len=\",train_df_pre_3billion_result.__len__())\n    \n#     del train_df_pre_3billion,train_df_pre_3billion_result\n#     gc.collect()\n\n# if train_df_post1billion_pre6kw_flag:\n#     print(\"start 后1亿前6kw的train...\")\n#     start=time.time()\n#     path=f\"/home/kesci/work/word2vec/post_10kw.csv\"\n#     train_df_post1billion_df = pd.read_csv(path,\n#                                 nrows=60000000\n#                                 )\n#     print(train_df_post1billion_df.__len__())\n    \n#     train_df_post1billion_df_result = my_feat_calc_func(train_df_post1billion_df)\n#     print(\"done！\",\", Using time:\",time.time()-start)\n    \n#     train_df_post1billion_df_result=reduce_mem_usage(train_df_post1billion_df_result,verbose=True)\n#     train_df_post1billion_df_result.to_hdf(train_df_post1billion_pre6kw_flag_path, index=None, key='data', complevel=9)\n#     print(\"save ok\")\n","execution_count":null},{"metadata":{"id":"9569775AFCFE4BDF89BF4757AA62CF6B"},"cell_type":"code","outputs":[],"source":"","execution_count":null},{"metadata":{"id":"85E3903B99444EA18271BF4E88F9600A"},"cell_type":"code","outputs":[],"source":"# sen句向量相似度/距离 特征\n# my_feat_calc_func 函数是并行计算特征的函数\n# 只需要将包含queryid、query、title列的dataframe传入函数my_feat_calc_func()，即可计算出该类特征\n# 将前一亿、后一亿、test1、test2传入dataframe","execution_count":null},{"metadata":{"id":"17CD83C2FB3B41E48C70F4580F35AB93","hide_input":true},"cell_type":"code","outputs":[],"source":"import pandas as pd\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\n\nimport numpy as np\n\nfrom tqdm import tqdm\n\ntqdm.pandas(desc='Progress')\n\nfrom multiprocessing import Pool\n\n\ndef reduce_mem_usage(D, verbose=True):\n    start_mem = D.memory_usage().sum() / 1024 ** 2\n    for c, d in zip(D.columns, D.dtypes):\n        if d.kind == 'f':\n            D[c] = pd.to_numeric(D[c], downcast='float')\n        elif d.kind == 'i':\n            D[c] = pd.to_numeric(D[c], downcast='signed')\n    end_mem = D.memory_usage().sum() / 1024 ** 2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (\n                start_mem - end_mem) / start_mem))\n    return D\n\n\nfrom gensim.corpora import Dictionary\nfrom gensim.models import TfidfModel, KeyedVectors\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n\ntqdm.pandas()\n\ndictionary = Dictionary.load('/home/kesci/work/sunrui/tfidf_final/dictionary_final.dic')\nprint(\"dic load finish!\")\nid2token = {id: token for token, id in dictionary.token2id.items()}\nprint(\"id2token finish!\")\ntf_idf_model = TfidfModel.load('/home/kesci/work/sunrui/tfidf_final/tf_idf_final.model')\nprint(\"tf_idf_model load finish\")\nw2v_model = KeyedVectors.load_word2vec_format('/home/kesci/work/word2vec/word2vec_100.bin', binary=True)\nprint(\"w2v_model load finish\")\n\nfasttext_model = KeyedVectors.load_word2vec_format('./fasttext/fasttext_100.bin', binary=True)\nprint(\"fasttext_model load finish\")\n\n\ndef get_weight_sentence_vec(tf_idf_vec):\n    vec_list = []\n    for token_id, tf_idf in tf_idf_vec:\n        token = id2token[token_id]\n        w2v = w2v_model[token]\n        fasttext_vec = fasttext_model[token]\n        vec = np.concatenate([w2v, fasttext_vec])\n        weighted_vec = vec * tf_idf\n        vec_list.append(weighted_vec)\n    return np.sum(vec_list, axis=0)\n\n\ndef get_tfidf_vec(text):\n    bow = dictionary.doc2bow(text)\n    tf_idf = tf_idf_model[bow]\n    return tf_idf\n\nfrom scipy.stats import skew, kurtosis\n\ndef compute_sen_vec_dis(query, title):\n    tf_idf_q = get_tfidf_vec(query)\n    tf_idf_t = get_tfidf_vec(title)\n    weighted_q_vec = get_weight_sentence_vec(tf_idf_q)\n    weighted_t_vec = get_weight_sentence_vec(tf_idf_t)\n\n    sent_cosine = cosine(weighted_q_vec, weighted_t_vec)\n    sent_cityblock = cityblock(weighted_q_vec, weighted_t_vec)\n    sent_jaccard = jaccard(weighted_q_vec, weighted_t_vec)\n    sent_canberra = canberra(weighted_q_vec, weighted_t_vec)\n    sent_euclidean = euclidean(weighted_q_vec, weighted_t_vec)\n    sent_minkowski = minkowski(weighted_q_vec, weighted_t_vec)\n    sent_braycurtis = braycurtis(weighted_q_vec, weighted_t_vec)\n\n    skew_q = skew(weighted_q_vec)\n    skew_t = skew(weighted_t_vec)\n    kurtosis_q = kurtosis(weighted_q_vec)\n    kurtosis_t = kurtosis(weighted_t_vec)\n\n    return [sent_cosine,sent_cityblock,sent_jaccard,\n            sent_canberra,sent_euclidean,sent_minkowski,sent_braycurtis\n            skew_q,skew_t,kurtosis_q,kurtosis_t]\n\n\ndef apply_fun(df):\n    df[\"query\"] = df[\"query\"].apply(str.strip)\n    df[\"title\"] = df[\"title\"].apply(str.strip)\n\n    sen_dis_feat_tmp = pd.DataFrame()\n    sen_dis_feat_tmp[\"sen_dis_tmp\"] = df.apply(\n        lambda row: compute_sen_vec_dis(\n            row['query'].split(),\n            row['title'].split()), axis=1)\n\n    sen_dis_feat = pd.DataFrame()\n    \n    sen_dis_feat_tmp[\"sen_dis_tmp\"] = df.apply(\n            lambda row: compute_sen_vec_dis(\n                row['query'].split(), \n                row['title'].split()), axis=1)\n    \n    sen_dis_feat=pd.DataFrame()\n    sen_dis_feat[\"sent_cosine\"] = sen_dis_feat_tmp.apply(\n            lambda row: row[\"sen_dis_tmp\"][0], axis=1)\n    sen_dis_feat[\"sent_cityblock\"] = sen_dis_feat_tmp.apply(\n            lambda row: row[\"sen_dis_tmp\"][1], axis=1)\n    sen_dis_feat[\"sent_jaccard\"] = sen_dis_feat_tmp.apply(\n            lambda row: row[\"sen_dis_tmp\"][2], axis=1)\n    sen_dis_feat[\"sent_canberra\"] = sen_dis_feat_tmp.apply(\n            lambda row: row[\"sen_dis_tmp\"][3], axis=1)\n    sen_dis_feat[\"sent_euclidean\"] = sen_dis_feat_tmp.apply(\n            lambda row: row[\"sen_dis_tmp\"][4], axis=1)\n    sen_dis_feat[\"sent_minkowski\"] = sen_dis_feat_tmp.apply(\n            lambda row: row[\"sen_dis_tmp\"][5], axis=1)\n    sen_dis_feat[\"sent_braycurtis\"] = sen_dis_feat_tmp.apply(\n            lambda row: row[\"sen_dis_tmp\"][6], axis=1)\n    sen_dis_feat[\"skew_q\"] = sen_dis_feat_tmp.apply(\n        lambda row: row[\"sen_dis_tmp\"][7], axis=1)\n    sen_dis_feat[\"skew_t\"] = sen_dis_feat_tmp.apply(\n        lambda row: row[\"sen_dis_tmp\"][8], axis=1)\n    sen_dis_feat[\"kurtosis_q\"] = sen_dis_feat_tmp.apply(\n        lambda row: row[\"sen_dis_tmp\"][9], axis=1)\n    sen_dis_feat[\"kurtosis_t\"] = sen_dis_feat_tmp.apply(\n        lambda row: row[\"sen_dis_tmp\"][10], axis=1)\n\n    return sen_dis_feat\n\n\ndef my_feat_calc_func(RawData):\n    \n    multiprocessing_nums = 16\n    df_parts = np.array_split(RawData, multiprocessing_nums)\n    with Pool(processes=multiprocessing_nums) as pool:\n        result_parts = pool.map(apply_fun, df_parts)\n    pool.join()\n\n    result_parallel = pd.concat(result_parts)\n\n    return result_parallel\n\n# train_df_post4kw_flag=True\n# train_df_post4kw_flag_path=f'./post_4kw_data/train/sen_dis2_train_200.h5'\n\n# train_df_pre3billion_flag=True\n# train_df_pre3billion_result_path=f'./pre_3billion_data/train/sen_dis2_train_200.h5'\n\n# test_df_flag=True\n# test_df_flag_path=f'/home/kesci/work/post_4kw_data/test1/sen_dis2_test1_200.h5'\n\n# test2_df_flag=True\n# test2_df_flag_path=f'/home/kesci/work/post_4kw_data/test2/sen_dis2_test2_200.h5'\n\n# train_df_post1billion_pre6kw_flag=True\n# train_df_post1billion_pre6kw_flag_path=f'./post_4kw_data/train_6kw/sen_dis2_train_200.h5'\n\n# import time\n# import gc\n\n# if train_df_post4kw_flag:\n#     print(\"start 后4kw的train...\")\n#     start=time.time()\n#     path=f\"/home/kesci/work/post_4kw_data/train_post_4kw.csv\"\n#     train_post4kw_df = pd.read_csv(path,\n#                         header=None,\n#                         usecols=[0,1,3],\n#                         names=['query_id','query','title'],\n#                         )\n#     print(train_post4kw_df.__len__())\n    \n#     train_post4kw_df_result = my_feat_calc_func(train_post4kw_df)\n#     print(\"done！\", \", Using time:\",time.time()-start)\n    \n#     train_post4kw_df_result=reduce_mem_usage(train_post4kw_df_result,verbose=True)\n#     train_post4kw_df_result.to_hdf(train_df_post4kw_flag_path, index=None, key='data', complevel=9)\n#     print(\"save ok\")\n#     del train_post4kw_df,train_post4kw_df_result\n#     gc.collect()\n\n# if test_df_flag:\n#     print(\"start test1\")\n#     path=f\"/home/kesci/input/bytedance/test_final_part1.csv\"\n#     test_df = pd.read_csv(path,header=None,usecols=[0,1,3],names=['query_id','query','title'])\n#     print(test_df.__len__())\n    \n#     test_df_result = my_feat_calc_func(test_df)\n#     print(\"done！\")\n\n#     test_df_result = reduce_mem_usage(test_df_result,verbose=True)\n#     test_df_result.to_hdf(test_df_flag_path, index=None, key='data', complevel=9)\n#     print(\"save ok\")\n#     del test_df,test_df_result\n#     gc.collect()\n\n# if test2_df_flag:\n#     parts_num=10\n#     result_parts=[]\n#     for index in range(parts_num):\n#         print(\"start 前3亿的train...\")\n#         start=time.time()\n#         path=f\"/home/kesci/input/bytedance/bytedance_contest.final_2.csv\"\n#         train_df_pre_3billion = pd.read_csv(path,\n#                                             header=None,\n#                                             skiprows=10000000*index,\n#                                             nrows=10000000,\n#                                             usecols=[0,1,3],\n#                                             names=['query_id','query','title'],\n#                                             )\n#         print(train_df_pre_3billion.__len__(),\",Read Using time:\",time.time()-start)\n    \n#         result_part = my_feat_calc_func(train_df_pre_3billion)\n#         print(f\"part{index} done！, Using time:\",time.time()-start)\n    \n#         result_parts.append(result_part)\n    \n#     train_df_pre_3billion_result = pd.concat(result_parts)\n#     print(\"done！\")\n\n#     train_df_pre_3billion_result = reduce_mem_usage(train_df_pre_3billion_result,verbose=True)\n#     train_df_pre_3billion_result.to_hdf(test2_df_flag_path, index=None, key='data', complevel=9)\n#     print(\"save ok, len=\",train_df_pre_3billion_result.__len__())\n#     del train_df_pre_3billion,train_df_pre_3billion_result\n#     gc.collect()\n\n# if train_df_pre3billion_flag:\n#     parts_num=10\n#     result_parts=[]\n#     for index in range(parts_num):\n#         print(\"start 前3亿的train...\")\n#         start=time.time()\n#         path=f\"/home/kesci/input/bytedance/train_final.csv\"\n#         train_df_pre_3billion = pd.read_csv(path,\n#                                             header=None,\n#                                             skiprows=10000000*index,\n#                                             nrows=10000000,\n#                                             usecols=[0,1,3],\n#                                             names=['query_id','query','title'],\n#                                             )\n#         print(train_df_pre_3billion.__len__(),\", Using time:\",time.time()-start)\n    \n#         result_part = my_feat_calc_func(train_df_pre_3billion)\n#         print(f\"part{index} done！\")\n    \n#         result_parts.append(result_part)\n    \n#     train_df_pre_3billion_result = pd.concat(result_parts)\n#     print(\"done！\")\n\n#     train_df_pre_3billion_result = reduce_mem_usage(train_df_pre_3billion_result,verbose=True)\n#     train_df_pre_3billion_result.to_hdf(train_df_pre3billion_result_path, index=None, key='data', complevel=9)\n#     print(\"save ok, len=\",train_df_pre_3billion_result.__len__())\n    \n#     del train_df_pre_3billion,train_df_pre_3billion_result\n#     gc.collect()\n\n# if train_df_post1billion_pre6kw_flag:\n#     print(\"start 后1亿前6kw的train...\")\n#     start=time.time()\n#     path=f\"/home/kesci/work/word2vec/post_10kw.csv\"\n#     train_df_post1billion_df = pd.read_csv(path,\n#                                 nrows=60000000\n#                                 )\n#     print(train_df_post1billion_df.__len__())\n    \n#     train_df_post1billion_df_result = my_feat_calc_func(train_df_post1billion_df)\n#     print(\"done！\",\", Using time:\",time.time()-start)\n    \n#     train_df_post1billion_df_result=reduce_mem_usage(train_df_post1billion_df_result,verbose=True)\n#     train_df_post1billion_df_result.to_hdf(train_df_post1billion_pre6kw_flag_path, index=None, key='data', complevel=9)\n#     print(\"save ok\")\n","execution_count":null},{"metadata":{"id":"75790E3CD56043C382DDB5AA99277619"},"cell_type":"code","outputs":[],"source":"","execution_count":null},{"metadata":{"id":"A29441C022BD48778C8AA1664363326A"},"cell_type":"code","outputs":[],"source":"# 以下是生成title count特征","execution_count":null},{"metadata":{"id":"D487FB78AA5647C58634AE3C03AABDB4","hide_input":true},"cell_type":"code","outputs":[],"source":"import pandas as pd\nimport numpy as np\nimport gc\n\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024 ** 2\n    if verbose:\n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (\n                start_mem - end_mem) / start_mem))\n    return df\n\n\ntrain_size = 1000000000\ntest1_size = 20000000\ntest_final_size = 100000000\n\n# todo 需要修改路径\ntrain_df = pd.read_hdf('/home/kesci/work/featureMap/train/title_code_train_feat.h5', key='data')\ntest_1_df = pd.read_hdf('/home/kesci/work/featureMap/test1/title_code_test1_feat.h5', key='data')\ntest_final_df = pd.read_hdf('/home/kesci/work/post_4kw_data/test2/title_code_test2_feat.h5', key='data')\n\nall_data = pd.concat([train_df, test_1_df, test_final_df], ignore_index=True)\ndel train_df, test_1_df, test_final_df\ngc.collect()\n\nall_data = reduce_mem_usage(all_data)\n\ncount_feature = pd.DataFrame()\n\nfor feature in ['title_code']:\n    print('计算' + feature + '点击次数')\n    count_feature[feature + '_count'] = all_data.groupby(feature)[feature].transform('count')\n    count_feature = reduce_mem_usage(count_feature)\n\ntrain_count_feature = count_feature[:train_size]\ntest1_count_feature = count_feature[train_size:train_size + test1_size]\ntest_final_count_feature = count_feature[train_size + test1_size:]\nprint(f'train shape = {train_count_feature.shape}')\nprint(f'test1 shape = {test1_count_feature.shape}')\nprint(f'test_final shape = {test_final_count_feature.shape}')\n\ntrain_count_feature.to_hdf('/home/kesci/work/featureMap/train/title_count_all.h5', index=None, key='data', complevel=9)\ntest1_count_feature.to_hdf('/home/kesci/work/post_4kw_data/test1/test1_title_count_final.h5', index=None, key='data', complevel=9)\ntest_final_count_feature.to_hdf('/home/kesci/work/post_4kw_data/test2/test2_title_count_final.h5', index=None, key='data', complevel=9)","execution_count":null},{"metadata":{"id":"C5F3851A325342878055EFC9725B2F4F"},"cell_type":"code","outputs":[],"source":"","execution_count":null},{"metadata":{"id":"C039B7178DC34E87BB68D059630796F0"},"cell_type":"code","outputs":[],"source":"# 以下是生成 query count 特征","execution_count":null},{"metadata":{"id":"A1CE0E0354EB43A081DED54A2A96D21B","hide_input":true},"cell_type":"code","outputs":[],"source":"import pandas as pd\nimport numpy as np\nimport gc\n\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024 ** 2\n    if verbose:\n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (\n                start_mem - end_mem) / start_mem))\n    return df\n\n\ntrain_size = 1000000000\ntest1_size = 20000000\ntest_final_size = 100000000\n\n# todo 需要修改路径\ntrain_df = pd.read_hdf('/home/kesci/work/featureMap/train/query_code_train_feat.h5')\ntest_1_df = pd.read_hdf('/home/kesci/work/featureMap/test1/query_code_test1_feat.h5')\ntest_final_df = pd.read_hdf('/home/kesci/work/post_4kw_data/test2/query_code_test2_feat.h5')\n\n\nall_data = pd.concat([train_df, test_1_df, test_final_df], ignore_index=True)\ndel train_df, test_1_df, test_final_df\ngc.collect()\n\nall_data = reduce_mem_usage(all_data)\n\ncount_feature = pd.DataFrame()\n\nfor feature in ['query_code']:\n    print('计算' + feature + '点击次数')\n    count_feature[feature + '_count'] = all_data.groupby(feature)[feature].transform('count')\n    count_feature = reduce_mem_usage(count_feature)\n\ntrain_count_feature = count_feature[:train_size]\ntest1_count_feature = count_feature[train_size:train_size + test1_size]\ntest_final_count_feature = count_feature[train_size + test1_size:]\nprint(f'train shape = {train_count_feature.shape}')\nprint(f'test1 shape = {test1_count_feature.shape}')\nprint(f'test_final shape = {test_final_count_feature.shape}')\n\ntest1_count_feature.to_hdf('/home/kesci/work/post_4kw_data/test1/test1_query_count_sr.h5', index=None, key='data', complevel=9)\ntrain_count_feature.to_hdf('/home/kesci/work/featureMap/train/query_count_all_sr.h5', index=None, key='data', complevel=9)\ntest_final_count_feature.to_hdf('/home/kesci/work/post_4kw_data/test2/test2_query_count_sr.h5', index=None, key='data', complevel=9)","execution_count":null},{"metadata":{"id":"7432D3F959D340A8B0844FE11DEF444B"},"cell_type":"code","outputs":[],"source":"","execution_count":null},{"metadata":{"id":"4B79117330714BE89413CA67327E04AB"},"cell_type":"code","outputs":[],"source":"# 以下是生成unique特征","execution_count":null},{"metadata":{"id":"A6CA4ED069504AB488C5116C19470D52","hide_input":true},"cell_type":"code","outputs":[],"source":"import pandas as pd\nimport numpy as np\nimport gc\n\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024 ** 2\n    if verbose:\n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (\n                start_mem - end_mem) / start_mem))\n    return df\n\n\ntrain_size = 1000000000\ntest1_size = 20000000\ntest_final_size = 100000000\n\n# todo 需要修改路径\ntrain_title = pd.read_hdf('/home/kesci/work/featureMap/train/title_code_train_feat.h5', key='data')\ntest_1_title = pd.read_hdf('/home/kesci/work/featureMap/test1/title_code_test1_feat.h5', key='data')\ntest_final_title = pd.read_hdf('/home/kesci/work/post_4kw_data/test2/title_code_test2_feat.h5', key='data')\n\ntrain_query = pd.read_hdf('/home/kesci/work/featureMap/train/query_code_train_feat.h5')\ntest_1_query = pd.read_hdf('/home/kesci/work/featureMap/test1/query_code_test1_feat.h5')\ntest_final_query = pd.read_hdf('/home/kesci/work/post_4kw_data/test2/query_code_test2_feat.h5')\n\ntrain_df = pd.concat([train_query, train_title], axis=1)\ntest_1_df = pd.concat([test_1_query, test_1_title], axis=1)\ntest_final_df = pd.concat([test_final_query, test_final_title], axis=1)\n\ndel train_query, train_title, test_1_query, test_1_title, test_final_query, test_final_title\ngc.collect()\nall_data = pd.concat([train_df, test_1_df, test_final_df], ignore_index=True)\ndel train_df, test_1_df, test_final_df\ngc.collect()\n\nall_data = reduce_mem_usage(all_data)\n\nnunique_feature = pd.DataFrame()\n\n\n# 下面两步 可能需要注释后分两次运行 一次运行可能会炸内存\nnunique_feature['query_nunique_title'] = all_data.groupby('query_code').title_code.transform('nunique')\nnunique_feature = reduce_mem_usage(nunique_feature)\nnunique_feature['title_nunique_query'] = all_data.groupby('title_code').query_code.transform('nunique')\nnunique_feature = reduce_mem_usage(nunique_feature)\n\ntrain_nunique_feature = nunique_feature[:train_size]\ntest1_nunique_feature = nunique_feature[train_size:train_size + test1_size]\ntest_final_nunique_feature = nunique_feature[train_size + test1_size:]\nprint(f'train shape = {train_nunique_feature.shape}')\nprint(f'test1 shape = {test1_nunique_feature.shape}')\nprint(f'test_final shape = {test_final_nunique_feature.shape}')","execution_count":null},{"metadata":{"id":"A0FEEF5E1A2B4B1F9E799B0D204B2462"},"cell_type":"code","outputs":[],"source":"","execution_count":null},{"metadata":{"id":"2C7AA50C84E34FF78C02627DD36B0E98"},"cell_type":"code","outputs":[],"source":"# 以下是生成test1和train的title_code的转化率  注意一下需要改一下路径 就是title_code的","execution_count":null},{"metadata":{"id":"461DD17955C44A08853F2FF7E56C24CE","hide_input":true},"cell_type":"code","outputs":[],"source":"import pandas as pd\ntrain_label = pd.read_csv('/home/kesci/input/bytedance/train_final.csv', usecols=[4], names=['label'])\n\ntrain_data = pd.read_csv('/home/kesci/work/featureMap/train/title_code_train_feat.csv.bz2')\ntest_df = pd.read_csv('/home/kesci/work/featureMap/test1/title_code_test1_feat.csv.bz2')\n\ntrain_df = pd.concat([train_data,train_label],axis=1)\ndel train_data,train_label\nimport gc\ngc.collect()\n\ndef downcast_data(D):\n    for c, d in zip(D.columns, D.dtypes):\n        if d.kind == 'f':\n            D[c] = pd.to_numeric(D[c], downcast='float')\n        elif d.kind == 'i':\n            D[c] = pd.to_numeric(D[c], downcast='signed')\n    return D\ntrain_df = downcast_data(train_df)\ntest_df = downcast_data(test_df)\n\nimport time\n\nimport pandas as pd\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport numpy as np\nimport scipy.special as special\n\n\ndef log(log: str):\n    print(log)\n\n\ndef time_log(time_elapsed):\n    print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))  # 打印出来时间\n\n\ndef log_event(event: str):\n    log(event)\n\n\nclass HyperParam(object):\n    def __init__(self, alpha, beta):\n        self.alpha = alpha\n        self.beta = beta\n\n    def sample_from_beta(self, alpha, beta, num, imp_upperbound):\n        sample = np.random.beta(alpha, beta, num)\n        I = []\n        C = []\n        for click_ratio in sample:\n            imp = np.random.random() * imp_upperbound\n            # imp = imp_upperbound\n            click = imp * click_ratio\n            I.append(imp)\n            C.append(click)\n        return I, C\n\n    def update_from_data_by_FPI(self, tries, success, iter_num, epsilon):\n        '''estimate alpha, beta using fixed point iteration'''\n        for i in range(iter_num):\n            new_alpha, new_beta = self.__fixed_point_iteration(tries, success, self.alpha, self.beta)\n            if abs(new_alpha - self.alpha) < epsilon and abs(new_beta - self.beta) < epsilon:\n                break\n            self.alpha = new_alpha\n            self.beta = new_beta\n\n    def __fixed_point_iteration(self, tries, success, alpha, beta):\n        '''fixed point iteration'''\n        sumfenzialpha = 0.0\n        sumfenzibeta = 0.0\n        sumfenmu = 0.0\n        for i in range(len(tries)):\n            sumfenzialpha += (special.digamma(success[i] + alpha) - special.digamma(alpha))\n            sumfenzibeta += (special.digamma(tries[i] - success[i] + beta) - special.digamma(beta))\n            sumfenmu += (special.digamma(tries[i] + alpha + beta) - special.digamma(alpha + beta))\n\n        return alpha * (sumfenzialpha / sumfenmu), beta * (sumfenzibeta / sumfenmu)\n\n    def update_from_data_by_moment(self, tries, success):  # tries尝试了多少次ctr  success 命中了多少次ctr\n        '''estimate alpha, beta using moment estimation'''\n        mean, var = self.__compute_moment(tries, success)\n        # print 'mean and variance: ', mean, var\n        # self.alpha = mean*(mean*(1-mean)/(var+0.000001)-1)\n        self.alpha = (mean + 0.000001) * ((mean + 0.000001) * (1.000001 - mean) / (var + 0.000001) - 1)\n        # self.beta = (1-mean)*(mean*(1-mean)/(var+0.000001)-1)\n        self.beta = (1.000001 - mean) * ((mean + 0.000001) * (1.000001 - mean) / (var + 0.000001) - 1)\n\n    def __compute_moment(self, tries, success):\n        '''\n        moment estimation\n        '''\n        ctr_list = []\n        var = 0.0\n        for i in range(len(tries)):\n            ctr_list.append(float(success[i]) / (tries[i] + 0.000000001))\n        mean = sum(ctr_list) / len(ctr_list)\n        for ctr in ctr_list:\n            var += pow(ctr - mean, 2)\n\n        return mean, var / (len(ctr_list) - 1)\n\n\ndef merge_test(train_df, test_df, feature_name, is_fill_na):\n    temp = train_df.groupby(feature_name, as_index=False)['label'].agg(\n        {feature_name + '_label_count': 'sum', feature_name + '_all_count': 'count'})\n    HP = HyperParam(1, 1)\n    HP.update_from_data_by_moment(temp[feature_name + '_all_count'].values,\n                                  temp[feature_name + '_label_count'].values)  # 矩估计\n    temp[feature_name + '_convert'] = (temp[feature_name + '_label_count'] + HP.alpha) / (\n            temp[feature_name + '_all_count'] + HP.alpha + HP.beta)\n    temp = temp[[feature_name, feature_name + '_convert', feature_name + '_label_count']].drop_duplicates()\n    test_df = pd.merge(test_df, temp, on=[feature_name], how='left')\n    if is_fill_na:\n        test_df[feature_name + '_convert'].fillna(HP.alpha / (HP.alpha + HP.beta), inplace=True)\n    return test_df\n\n\ndef merge_train(kfold_4_df, kfold_1_df, feature_name, is_fill_na):  # 用其他训练集的四折的数据去merge一折的\n    temp = kfold_4_df.groupby(feature_name, as_index=False)['label'].agg(\n        {feature_name + '_label_count': 'sum', feature_name + '_all_count': 'count'})\n    HP = HyperParam(1, 1)\n    HP.update_from_data_by_moment(temp[feature_name + '_all_count'].values,\n                                  temp[feature_name + '_label_count'].values)  # 矩估计\n    temp[feature_name + '_convert'] = (temp[feature_name + '_label_count'] + HP.alpha) / (\n            temp[feature_name + '_all_count'] + HP.alpha + HP.beta)\n    temp = temp[[feature_name, feature_name + '_convert', feature_name + '_label_count']].drop_duplicates()\n    kfold_1_df = pd.merge(kfold_1_df, temp, on=[feature_name], how='left')\n    if is_fill_na:\n        kfold_1_df[feature_name + '_convert'].fillna(HP.alpha / (HP.alpha + HP.beta), inplace=True)\n    return kfold_1_df\n    \nn_splits = 5\nis_shuffle = False\nfeature_name = 'title_code'\nis_fill_nan = True\n\nlog_event('计算 test ' + feature_name + ' 转换率')\nsince = time.time()\ntest_df = merge_test(train_df, test_df, feature_name, is_fill_nan)\ntime_elapsed = time.time() - since\ntime_log(time_elapsed)\n\nkf = KFold(n_splits=n_splits, shuffle=is_shuffle, random_state=19951024)\n\nif is_shuffle:\n    train_df['old_index'] = train_df.index\n\ntemp_list = []\nfor index, (fold_4, fold_1) in enumerate(kf.split(train_df)):\n    log_event(f'计算 train fold {(index + 1)} ' + feature_name + ' 转换率')\n    since = time.time()\n    temp_df = merge_train(train_df.iloc[fold_4], train_df.iloc[fold_1], feature_name, is_fill_nan)\n    temp_list.append(temp_df)\n    time_elapsed = time.time() - since\n    time_log(time_elapsed)\n\nresult_df = pd.concat(temp_list, ignore_index=True)\n\nif is_shuffle:\n    result_df.sort_values(by='old_index', inplace=True)\n    result_df = result_df.reset_index(drop=True)\n    \nresult_df[['title_code_convert','title_code_label_count']].to_hdf('./sunrui/title_convert_train.h5', index=None, key='data',complevel=9)\ntest_df[['title_code_convert','title_code_label_count']].to_hdf('./sunrui/title_convert_test.h5', index=None, key='data',complevel=9)","execution_count":null},{"metadata":{"id":"DD7C76106A624CB78D03850D20DAD1BC"},"cell_type":"code","outputs":[],"source":"","execution_count":null},{"metadata":{"id":"965DFDED92FA488182F6C1D39D921ABA"},"cell_type":"code","outputs":[],"source":"# 以下是生成test2的title_code转化率","execution_count":null},{"metadata":{"id":"35CF96D9E49A48F280ED7462BAD84AA4","hide_input":true},"cell_type":"code","outputs":[],"source":"import time\n\nimport numpy as np\nimport pandas as pd\nimport scipy.special as special\nimport gc\n\n\ndef log(log: str):\n    print(log)\n\n\ndef time_log(time_elapsed):\n    print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))  # 打印出来时间\n\n\ndef log_event(event: str):\n    log(event)\n\n\nclass HyperParam(object):\n    def __init__(self, alpha, beta):\n        self.alpha = alpha\n        self.beta = beta\n\n    def sample_from_beta(self, alpha, beta, num, imp_upperbound):\n        sample = np.random.beta(alpha, beta, num)\n        I = []\n        C = []\n        for click_ratio in sample:\n            imp = np.random.random() * imp_upperbound\n            # imp = imp_upperbound\n            click = imp * click_ratio\n            I.append(imp)\n            C.append(click)\n        return I, C\n\n    def update_from_data_by_FPI(self, tries, success, iter_num, epsilon):\n        '''estimate alpha, beta using fixed point iteration'''\n        for i in range(iter_num):\n            new_alpha, new_beta = self.__fixed_point_iteration(tries, success, self.alpha, self.beta)\n            if abs(new_alpha - self.alpha) < epsilon and abs(new_beta - self.beta) < epsilon:\n                break\n            self.alpha = new_alpha\n            self.beta = new_beta\n\n    def __fixed_point_iteration(self, tries, success, alpha, beta):\n        '''fixed point iteration'''\n        sumfenzialpha = 0.0\n        sumfenzibeta = 0.0\n        sumfenmu = 0.0\n        for i in range(len(tries)):\n            sumfenzialpha += (special.digamma(success[i] + alpha) - special.digamma(alpha))\n            sumfenzibeta += (special.digamma(tries[i] - success[i] + beta) - special.digamma(beta))\n            sumfenmu += (special.digamma(tries[i] + alpha + beta) - special.digamma(alpha + beta))\n\n        return alpha * (sumfenzialpha / sumfenmu), beta * (sumfenzibeta / sumfenmu)\n\n    def update_from_data_by_moment(self, tries, success):  # tries尝试了多少次ctr  success 命中了多少次ctr\n        '''estimate alpha, beta using moment estimation'''\n        mean, var = self.__compute_moment(tries, success)\n        # print 'mean and variance: ', mean, var\n        # self.alpha = mean*(mean*(1-mean)/(var+0.000001)-1)\n        self.alpha = (mean + 0.000001) * ((mean + 0.000001) * (1.000001 - mean) / (var + 0.000001) - 1)\n        # self.beta = (1-mean)*(mean*(1-mean)/(var+0.000001)-1)\n        self.beta = (1.000001 - mean) * ((mean + 0.000001) * (1.000001 - mean) / (var + 0.000001) - 1)\n\n    def __compute_moment(self, tries, success):\n        '''\n        moment estimation\n        '''\n        ctr_list = []\n        var = 0.0\n        for i in range(len(tries)):\n            ctr_list.append(float(success[i]) / (tries[i] + 0.000000001))\n        mean = sum(ctr_list) / len(ctr_list)\n        for ctr in ctr_list:\n            var += pow(ctr - mean, 2)\n\n        return mean, var / (len(ctr_list) - 1)\n\n\ndef merge_test(train_df, test_df, feature_name, is_fill_na):\n    temp = train_df.groupby(feature_name, as_index=False)['label'].agg(\n        {feature_name + '_label_count': 'sum', feature_name + '_all_count': 'count'})\n    HP = HyperParam(1, 1)\n    HP.update_from_data_by_moment(temp[feature_name + '_all_count'].values,\n                                  temp[feature_name + '_label_count'].values)  # 矩估计\n    temp[feature_name + '_convert'] = (temp[feature_name + '_label_count'] + HP.alpha) / (\n            temp[feature_name + '_all_count'] + HP.alpha + HP.beta)\n    temp = temp[[feature_name, feature_name + '_convert', feature_name + '_label_count']].drop_duplicates()\n    test_df = pd.merge(test_df, temp, on=[feature_name], how='left')\n    if is_fill_na:\n        test_df[feature_name + '_convert'].fillna(HP.alpha / (HP.alpha + HP.beta), inplace=True)\n    return test_df\n\n\ndef merge_train(kfold_4_df, kfold_1_df, feature_name, is_fill_na):  # 用其他训练集的四折的数据去merge一折的\n    temp = kfold_4_df.groupby(feature_name, as_index=False)['label'].agg(\n        {feature_name + '_label_count': 'sum', feature_name + '_all_count': 'count'})\n    HP = HyperParam(1, 1)\n    HP.update_from_data_by_moment(temp[feature_name + '_all_count'].values,\n                                  temp[feature_name + '_label_count'].values)  # 矩估计\n    temp[feature_name + '_convert'] = (temp[feature_name + '_label_count'] + HP.alpha) / (\n            temp[feature_name + '_all_count'] + HP.alpha + HP.beta)\n    temp = temp[[feature_name, feature_name + '_convert', feature_name + '_label_count']].drop_duplicates()\n    kfold_1_df = pd.merge(kfold_1_df, temp, on=[feature_name], how='left')\n    if is_fill_na:\n        kfold_1_df[feature_name + '_convert'].fillna(HP.alpha / (HP.alpha + HP.beta), inplace=True)\n    return kfold_1_df\n\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024 ** 2\n    if verbose:\n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (\n                start_mem - end_mem) / start_mem))\n    return df\n\n\ntrain_title = pd.read_hdf('/home/kesci/work/featureMap/train/title_code_train_feat.h5', key='data')\ntrain_label = pd.read_hdf('/home/kesci/work/label_train.h5', key='data')\n\ntrain_df = pd.concat([train_title, train_label], axis=1)\ndel train_title, train_label\ngc.collect()\ntrain_df = reduce_mem_usage(train_df)\n\ntest_df = pd.read_hdf('/home/kesci/work/post_4kw_data/test2/title_code_test2_feat.h5', key='data')\ntest_df = reduce_mem_usage(test_df)\n\nn_splits = 5\nis_shuffle = False\nfeature_name = 'title_code'\nis_fill_nan = True\n\nlog_event('计算 test ' + feature_name + ' 转换率')\nsince = time.time()\ntest_df = merge_test(train_df, test_df, feature_name, is_fill_nan)\ntime_elapsed = time.time() - since\ntime_log(time_elapsed)\n\ntest_df.drop(['title_code'],axis=1,inplace=True)\ntest_df = reduce_mem_usage(test_df)\ntest_df.to_hdf('/home/kesci/work/post_4kw_data/test2/test2_title_convert.h5', index=None, key='data', complevel=9)","execution_count":null},{"metadata":{"id":"7171FA2CB8974A748E23A1F59ABC8FD7"},"cell_type":"code","outputs":[],"source":"","execution_count":null},{"metadata":{"id":"D2B9AAB5F9484173992E42BE775F74D4"},"cell_type":"code","outputs":[],"source":"# 一篇文章的质量score计算源码（10折生成前一亿）\n# calcScore(dataSet,testSet)\n# 读取queryid、title_code、lable三列特征的dataframe 作为dataSet参数\n# 读取 title_code 该列特征的dataframe 作为testSet参数\n# 这里需要计算四次：\n#    1、第二亿到第十亿的queryid、title_code、lable作为dataSet参数，第一亿的title_code作为testSet参数\n#    2、第一亿到第九亿的queryid、title_code、lable作为dataSet参数，第十亿的title_code作为testSet参数\n#    3、第一亿到第十亿的queryid、title_code、lable作为dataSet参数，test1的title_code作为testSet参数\n#    4、第二亿到第十亿的queryid、title_code、lable作为dataSet参数，test2的title_code作为testSet参数","execution_count":null},{"metadata":{"id":"2618B07D4C764AB18EA7FBD20DED8D84","hide_input":true},"cell_type":"code","outputs":[],"source":"import pandas as pd\nimport gc\n\ndef reduce_mem_usage(D,verbose=True):\n    start_mem = D.memory_usage().sum() / 1024**2\n    for c, d in zip(D.columns, D.dtypes):\n        if d.kind == 'f':\n            D[c] = pd.to_numeric(D[c], downcast='float')\n        elif d.kind == 'i':\n            D[c] = pd.to_numeric(D[c], downcast='signed')\n    end_mem = D.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return D\n\ndef calcScore(dataSet,testSet):\n    dataSet['queryid_num']=dataSet.groupby(by='query_id').label.transform('count')\n    dataSet['queryid_click_num']=dataSet.groupby(by='query_id').label.transform('sum')\n    dataSet=dataSet.drop(columns=[\"query_id\"])\n    print(\"ok1\")\n    gc.collect()\n    dataSet = reduce_mem_usage(dataSet,verbose=True)\n    \n    dataSet['queryid_click_rate'] = dataSet['queryid_click_num']/dataSet['queryid_num']\n    print(\"ok2\")\n    dataSet=dataSet.drop(columns=[\"queryid_num\",\"queryid_click_num\"])\n    gc.collect()\n    dataSet = reduce_mem_usage(dataSet,verbose=True)\n    \n    dataSet['title_queryid_score']= dataSet['label']- dataSet['queryid_click_rate']\n    dataSet=dataSet.drop(columns=[\"label\"])\n    gc.collect()\n    print(\"ok3\")\n    dataSet = reduce_mem_usage(dataSet,verbose=True)\n    \n    feature='title_code'\n    dataSet[feature+'_score'] = dataSet.groupby(by=feature)['title_queryid_score'].transform('sum')\n    dataSet=reduce_mem_usage(dataSet,verbose=True)\n    dataSet=dataSet[[feature,feature+'_score']].drop_duplicates()\n    dataSet = reduce_mem_usage(dataSet,verbose=True)\n    gc.collect()\n    \n    testSet = pd.merge(testSet, dataSet, on=[feature], how='left')\n    \n    return testSet\n\n\n# id_feature='/home/kesci/input/bytedance/train_final.csv'\n# usecols=[0,4]\n# names=['query_id','label']\n\n# print(\"正在读取：\",id_feature)\n# dataSet = pd.read_csv(id_feature,\n#                       header=None,\n#                     #   nrows=100000000,\n#                       usecols=usecols,\n#                       names=names\n#                       )\n\n# path=f'/home/kesci/work/featureMap/train/title_code_train_feat.h5'\n# dataSet=pd.concat([dataSet,pd.read_hdf(path, key='data')], axis=1)\n\n# dataSet=reduce_mem_usage(dataSet,verbose=True)\n\n# testSet=dataSet[:100000000][[\"title_code\"]]\n# print(testSet.shape)\n\n# dataSet=dataSet[100000000:].reset_index(drop=True)\n# print(dataSet.shape)\n\n# gc.collect()\n\n# testSet = calcScore(dataSet,testSet)\n\n# dataSetType=\"train\"\n# path=f\"/home/kesci/work/pre_3billion_data/{dataSetType}/title_code_{dataSetType}_feat.h5\"\n# testSet[\"title_code_score\"]=testSet[\"title_code_score\"].fillna(0)\n# testSet[[\"title_code_score\"]].to_hdf(path, index=None, key='data', complevel=9)\n# print(\"OK\")","execution_count":null},{"metadata":{"id":"D8152DFA9C6543F3AB43B265EC2CC302"},"cell_type":"code","outputs":[],"source":"","execution_count":null},{"metadata":{"id":"58B8FDCAE3F042269C17A8E5AC39C716"},"cell_type":"code","outputs":[],"source":"# 一篇文章的点击数、出现数目 计算源码（10折生成前一亿）\n# calcScorecount(dataSet,testSet)\n# 读取 queryid、title_code、lable三列特征的dataframe 作为dataSet参数\n# 读取 title_code 该列特征的dataframe 作为testSet参数\n# 这里需要计算四次：\n#    1、第二亿到第十亿的queryid、title_code、lable作为dataSet参数，第一亿的title_code作为testSet参数\n#    2、第一亿到第九亿的queryid、title_code、lable作为dataSet参数，第十亿的title_code作为testSet参数\n#    3、第一亿到第十亿的queryid、title_code、lable作为dataSet参数，test1的title_code作为testSet参数\n#    4、第二亿到第十亿的queryid、title_code、lable作为dataSet参数，test2的title_code作为testSet参数","execution_count":null},{"metadata":{"id":"B6B42597A776412E86A8E027368FB063","hide_input":true},"cell_type":"code","outputs":[],"source":"import pandas as pd\nimport gc\n\ndef reduce_mem_usage(D,verbose=True):\n    start_mem = D.memory_usage().sum() / 1024**2\n    for c, d in zip(D.columns, D.dtypes):\n        if d.kind == 'f':\n            D[c] = pd.to_numeric(D[c], downcast='float')\n        elif d.kind == 'i':\n            D[c] = pd.to_numeric(D[c], downcast='signed')\n    end_mem = D.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return D\n\ndef calcScorecount(dataSet,testSet):\n    dataSet[feature+'_count']=dataSet.groupby(by=feature).label.transform('count')\n    print(\"Ok1\")\n    dataSet[feature+'_click_num']=dataSet.groupby(by=feature).label.transform('sum')\n    print(\"ok2\")\n    dataSet=dataSet[[feature,feature+'_count',feature+'_click_num']].drop_duplicates()\n    dataSet = reduce_mem_usage(dataSet,verbose=True)\n    gc.collect()\n    \n    testSet = pd.merge(testSet, dataSet, on=[feature], how='left')\n    return testSet\n\n\n# id_feature='/home/kesci/input/bytedance/train_final.csv'\n# usecols=[4]\n# names=['label']\n\n# print(\"正在读取：\",id_feature)\n# dataSet = pd.read_csv(id_feature,\n#                       header=None,\n#                     #   nrows=100000000,\n#                       usecols=usecols,\n#                       names=names\n#                       )\n\n# path=f'/home/kesci/work/featureMap/train/title_code_train_feat.h5'\n# dataSet=pd.concat([dataSet,pd.read_hdf(path, key='data')], axis=1)\n\n# dataSet=reduce_mem_usage(dataSet,verbose=True)\n\n# feature=\"title_code\"\n\n# testSet=dataSet[:100000000][[feature]]\n# print(testSet.shape)\n\n# dataSet=dataSet[100000000:].reset_index(drop=True)\n# print(dataSet.shape)\n\n# gc.collect()\n\n","execution_count":null},{"metadata":{"id":"14642AB1CA2C478C82A2D8EEB2FF2542"},"cell_type":"code","outputs":[],"source":"","execution_count":null},{"metadata":{"id":"B0EA1A63296141CB82232DA59E126E00"},"cell_type":"code","outputs":[],"source":"# 以下是生成tag转化率","execution_count":null},{"metadata":{"id":"3D22A88DB136436C876709DC4396711D","hide_input":true},"cell_type":"code","outputs":[],"source":"# tag的转化率 \nimport pandas as pd\n\n# train_label = pd.read_csv('/home/kesci/input/bytedance/train_final.csv', usecols=[4], names=['label'])\n\ntrain_label = pd.read_hdf('/home/kesci/work/label_pan.h5', key='data')\n\ntrain_data = pd.read_csv('/home/kesci/work/featureMap/train/tag_train_feat.csv.bz2')\ntest_df = pd.read_csv('/home/kesci/work/featureMap/test1/tag_test1_feat.csv.bz2')\n\ndef downcast_data(D, verbose=True):\n    start_mem = D.memory_usage().sum() / 1024 ** 2\n    for c, d in zip(D.columns, D.dtypes):\n        if d.kind == 'f':\n            D[c] = pd.to_numeric(D[c], downcast='float')\n        elif d.kind == 'i':\n            D[c] = pd.to_numeric(D[c], downcast='signed')\n    end_mem = D.memory_usage().sum() / 1024 ** 2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (\n                start_mem - end_mem) / start_mem))\n    return D\ntrain_data = downcast_data(train_data)\ntest_df = downcast_data(test_df)\ntrain_label = downcast_data(train_label)\ntrain_df = pd.concat([train_data, train_label], axis=1)\ndel train_data, train_label\n\nimport gc\ngc.collect()\n\nimport time\n\nimport pandas as pd\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport numpy as np\nimport scipy.special as special\n\n\ndef log(log: str):\n    print(log)\n\n\ndef time_log(time_elapsed):\n    print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))  # 打印出来时间\n\n\ndef log_event(event: str):\n    log(event)\n\n\nclass HyperParam(object):\n    def __init__(self, alpha, beta):\n        self.alpha = alpha\n        self.beta = beta\n\n    def sample_from_beta(self, alpha, beta, num, imp_upperbound):\n        sample = np.random.beta(alpha, beta, num)\n        I = []\n        C = []\n        for click_ratio in sample:\n            imp = np.random.random() * imp_upperbound\n            # imp = imp_upperbound\n            click = imp * click_ratio\n            I.append(imp)\n            C.append(click)\n        return I, C\n\n    def update_from_data_by_FPI(self, tries, success, iter_num, epsilon):\n        '''estimate alpha, beta using fixed point iteration'''\n        for i in range(iter_num):\n            new_alpha, new_beta = self.__fixed_point_iteration(tries, success, self.alpha, self.beta)\n            if abs(new_alpha - self.alpha) < epsilon and abs(new_beta - self.beta) < epsilon:\n                break\n            self.alpha = new_alpha\n            self.beta = new_beta\n\n    def __fixed_point_iteration(self, tries, success, alpha, beta):\n        '''fixed point iteration'''\n        sumfenzialpha = 0.0\n        sumfenzibeta = 0.0\n        sumfenmu = 0.0\n        for i in range(len(tries)):\n            sumfenzialpha += (special.digamma(success[i] + alpha) - special.digamma(alpha))\n            sumfenzibeta += (special.digamma(tries[i] - success[i] + beta) - special.digamma(beta))\n            sumfenmu += (special.digamma(tries[i] + alpha + beta) - special.digamma(alpha + beta))\n\n        return alpha * (sumfenzialpha / sumfenmu), beta * (sumfenzibeta / sumfenmu)\n\n    def update_from_data_by_moment(self, tries, success):  # tries尝试了多少次ctr  success 命中了多少次ctr\n        '''estimate alpha, beta using moment estimation'''\n        mean, var = self.__compute_moment(tries, success)\n        # print 'mean and variance: ', mean, var\n        # self.alpha = mean*(mean*(1-mean)/(var+0.000001)-1)\n        self.alpha = (mean + 0.000001) * ((mean + 0.000001) * (1.000001 - mean) / (var + 0.000001) - 1)\n        # self.beta = (1-mean)*(mean*(1-mean)/(var+0.000001)-1)\n        self.beta = (1.000001 - mean) * ((mean + 0.000001) * (1.000001 - mean) / (var + 0.000001) - 1)\n\n    def __compute_moment(self, tries, success):\n        '''\n        moment estimation\n        '''\n        ctr_list = []\n        var = 0.0\n        for i in range(len(tries)):\n            ctr_list.append(float(success[i]) / (tries[i] + 0.000000001))\n        mean = sum(ctr_list) / len(ctr_list)\n        for ctr in ctr_list:\n            var += pow(ctr - mean, 2)\n\n        return mean, var / (len(ctr_list) - 1)\n\n\ndef merge_test(train_df, test_df, feature_name, is_fill_na):\n    temp = train_df.groupby(feature_name, as_index=False)['label'].agg(\n        {feature_name + '_label_count': 'sum', feature_name + '_all_count': 'count'})\n    HP = HyperParam(1, 1)\n    HP.update_from_data_by_moment(temp[feature_name + '_all_count'].values,\n                                  temp[feature_name + '_label_count'].values)  # 矩估计\n    temp[feature_name + '_convert'] = (temp[feature_name + '_label_count'] + HP.alpha) / (\n            temp[feature_name + '_all_count'] + HP.alpha + HP.beta)\n    temp = temp[[feature_name, feature_name + '_convert', feature_name + '_label_count']].drop_duplicates()\n    test_df = pd.merge(test_df, temp, on=[feature_name], how='left')\n    if is_fill_na:\n        test_df[feature_name + '_convert'].fillna(HP.alpha / (HP.alpha + HP.beta), inplace=True)\n    return test_df\n\n\ndef merge_train(kfold_4_df, kfold_1_df, feature_name, is_fill_na):  # 用其他训练集的四折的数据去merge一折的\n    temp = kfold_4_df.groupby(feature_name, as_index=False)['label'].agg(\n        {feature_name + '_label_count': 'sum', feature_name + '_all_count': 'count'})\n    HP = HyperParam(1, 1)\n    HP.update_from_data_by_moment(temp[feature_name + '_all_count'].values,\n                                  temp[feature_name + '_label_count'].values)  # 矩估计\n    temp[feature_name + '_convert'] = (temp[feature_name + '_label_count'] + HP.alpha) / (\n            temp[feature_name + '_all_count'] + HP.alpha + HP.beta)\n    temp = temp[[feature_name, feature_name + '_convert', feature_name + '_label_count']].drop_duplicates()\n    kfold_1_df = pd.merge(kfold_1_df, temp, on=[feature_name], how='left')\n    if is_fill_na:\n        kfold_1_df[feature_name + '_convert'].fillna(HP.alpha / (HP.alpha + HP.beta), inplace=True)\n    return kfold_1_df\n\n\nn_splits = 5\nis_shuffle = False\nfeature_name = 'tag'\nis_fill_nan = True\n\nlog_event('计算 test ' + feature_name + ' 转换率')\nsince = time.time()\ntest_df = merge_test(train_df, test_df, feature_name, is_fill_nan)\ntime_elapsed = time.time() - since\ntime_log(time_elapsed)\n\nkf = KFold(n_splits=n_splits, shuffle=is_shuffle, random_state=19951024)\n\nif is_shuffle:\n    train_df['old_index'] = train_df.index\n\ntemp_list = []\nfor index, (fold_4, fold_1) in enumerate(kf.split(train_df)):\n    log_event(f'计算 train fold {(index + 1)} ' + feature_name + ' 转换率')\n    since = time.time()\n    temp_df = merge_train(train_df.iloc[fold_4], train_df.iloc[fold_1], feature_name, is_fill_nan)\n    temp_list.append(temp_df) \n    time_elapsed = time.time() - since\n    time_log(time_elapsed)\n\nresult_df = pd.concat(temp_list, ignore_index=True)\n\nif is_shuffle:\n    result_df.sort_values(by='old_index', inplace=True)\n    result_df = result_df.reset_index(drop=True)\n    \nresult_df[['tag_convert','tag_label_count']].to_hdf('./sunrui/tag_convert_train.h5', index=None, key='data',complevel=9)\ntest_df[['tag_convert','tag_label_count']].to_hdf('./sunrui/tag_convert_test.h5', index=None, key='data',complevel=9)","execution_count":null},{"metadata":{"id":"10691417BC4144AE8A5D606DA126E064"},"cell_type":"code","outputs":[],"source":"# 以下是生成test2的tag的代码","execution_count":null},{"metadata":{"id":"06961B62190B40F28E8ABF01B100D97A","hide_input":true},"cell_type":"code","outputs":[],"source":"import pandas as pd\ntrain = pd.read_hdf('/home/kesci/work/featureMap/train/tag_train_feat.h5')\n\nlabel_ = pd.read_hdf('/home/kesci/work/label_train.h5')\ntrain_df = pd.concat([train,label_],axis=1)\ndel train,label_\nimport gc\ngc.collect()\ntest_df = pd.read_hdf('/home/kesci/work/post_4kw_data/test2/tag_test2.h5')\n\ndef downcast_data(D, verbose=True):\n    start_mem = D.memory_usage().sum() / 1024 ** 2\n    for c, d in zip(D.columns, D.dtypes):\n        if d.kind == 'f':\n            D[c] = pd.to_numeric(D[c], downcast='float')\n        elif d.kind == 'i':\n            D[c] = pd.to_numeric(D[c], downcast='signed')\n    end_mem = D.memory_usage().sum() / 1024 ** 2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (\n                start_mem - end_mem) / start_mem))\n    return D\ntrain_df = downcast_data(train_df)\ntest_df = downcast_data(test_df)\n\nimport time\n\nimport pandas as pd\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport numpy as np\nimport scipy.special as special\n\n\ndef log(log: str):\n    print(log)\n\n\ndef time_log(time_elapsed):\n    print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))  # 打印出来时间\n\n\ndef log_event(event: str):\n    log(event)\n\n\nclass HyperParam(object):\n    def __init__(self, alpha, beta):\n        self.alpha = alpha\n        self.beta = beta\n\n    def sample_from_beta(self, alpha, beta, num, imp_upperbound):\n        sample = np.random.beta(alpha, beta, num)\n        I = []\n        C = []\n        for click_ratio in sample:\n            imp = np.random.random() * imp_upperbound\n            # imp = imp_upperbound\n            click = imp * click_ratio\n            I.append(imp)\n            C.append(click)\n        return I, C\n\n    def update_from_data_by_FPI(self, tries, success, iter_num, epsilon):\n        '''estimate alpha, beta using fixed point iteration'''\n        for i in range(iter_num):\n            new_alpha, new_beta = self.__fixed_point_iteration(tries, success, self.alpha, self.beta)\n            if abs(new_alpha - self.alpha) < epsilon and abs(new_beta - self.beta) < epsilon:\n                break\n            self.alpha = new_alpha\n            self.beta = new_beta\n\n    def __fixed_point_iteration(self, tries, success, alpha, beta):\n        '''fixed point iteration'''\n        sumfenzialpha = 0.0\n        sumfenzibeta = 0.0\n        sumfenmu = 0.0\n        for i in range(len(tries)):\n            sumfenzialpha += (special.digamma(success[i] + alpha) - special.digamma(alpha))\n            sumfenzibeta += (special.digamma(tries[i] - success[i] + beta) - special.digamma(beta))\n            sumfenmu += (special.digamma(tries[i] + alpha + beta) - special.digamma(alpha + beta))\n\n        return alpha * (sumfenzialpha / sumfenmu), beta * (sumfenzibeta / sumfenmu)\n\n    def update_from_data_by_moment(self, tries, success):  # tries尝试了多少次ctr  success 命中了多少次ctr\n        '''estimate alpha, beta using moment estimation'''\n        mean, var = self.__compute_moment(tries, success)\n        # print 'mean and variance: ', mean, var\n        # self.alpha = mean*(mean*(1-mean)/(var+0.000001)-1)\n        self.alpha = (mean + 0.000001) * ((mean + 0.000001) * (1.000001 - mean) / (var + 0.000001) - 1)\n        # self.beta = (1-mean)*(mean*(1-mean)/(var+0.000001)-1)\n        self.beta = (1.000001 - mean) * ((mean + 0.000001) * (1.000001 - mean) / (var + 0.000001) - 1)\n\n    def __compute_moment(self, tries, success):\n        '''\n        moment estimation\n        '''\n        ctr_list = []\n        var = 0.0\n        for i in range(len(tries)):\n            ctr_list.append(float(success[i]) / (tries[i] + 0.000000001))\n        mean = sum(ctr_list) / len(ctr_list)\n        for ctr in ctr_list:\n            var += pow(ctr - mean, 2)\n\n        return mean, var / (len(ctr_list) - 1)\n\n\ndef merge_test(train_df, test_df, feature_name, is_fill_na):\n    temp = train_df.groupby(feature_name, as_index=False)['label'].agg(\n        {feature_name + '_label_count': 'sum', feature_name + '_all_count': 'count'})\n    HP = HyperParam(1, 1)\n    HP.update_from_data_by_moment(temp[feature_name + '_all_count'].values,\n                                  temp[feature_name + '_label_count'].values)  # 矩估计\n    temp[feature_name + '_convert'] = (temp[feature_name + '_label_count'] + HP.alpha) / (\n            temp[feature_name + '_all_count'] + HP.alpha + HP.beta)\n    temp = temp[[feature_name, feature_name + '_convert', feature_name + '_label_count']].drop_duplicates()\n    test_df = pd.merge(test_df, temp, on=[feature_name], how='left')\n    if is_fill_na:\n        test_df[feature_name + '_convert'].fillna(HP.alpha / (HP.alpha + HP.beta), inplace=True)\n    return test_df\n\n\ndef merge_train(kfold_4_df, kfold_1_df, feature_name, is_fill_na):  # 用其他训练集的四折的数据去merge一折的\n    temp = kfold_4_df.groupby(feature_name, as_index=False)['label'].agg(\n        {feature_name + '_label_count': 'sum', feature_name + '_all_count': 'count'})\n    HP = HyperParam(1, 1)\n    HP.update_from_data_by_moment(temp[feature_name + '_all_count'].values,\n                                  temp[feature_name + '_label_count'].values)  # 矩估计\n    temp[feature_name + '_convert'] = (temp[feature_name + '_label_count'] + HP.alpha) / (\n            temp[feature_name + '_all_count'] + HP.alpha + HP.beta)\n    temp = temp[[feature_name, feature_name + '_convert', feature_name + '_label_count']].drop_duplicates()\n    kfold_1_df = pd.merge(kfold_1_df, temp, on=[feature_name], how='left')\n    if is_fill_na:\n        kfold_1_df[feature_name + '_convert'].fillna(HP.alpha / (HP.alpha + HP.beta), inplace=True)\n    return kfold_1_df\n\n\nn_splits = 5\nis_shuffle = False\nfeature_name = 'tag'\nis_fill_nan = True\n\nlog_event('计算 test ' + feature_name + ' 转换率')\nsince = time.time()\ntest_df = merge_test(train_df, test_df, feature_name, is_fill_nan)\ntime_elapsed = time.time() - since\ntime_log(time_elapsed)\n\ntest_df=downcast_data(test_df, verbose=True)\n\npath=\"/home/kesci/work/post_4kw_data/test2/tag_convert_test2.h5\"\ntest_df[['tag_convert','tag_label_count']].to_hdf(path, index=None, key='data',complevel=9)","execution_count":null},{"metadata":{"id":"3765421E913B4396825B68800C12E42B"},"cell_type":"code","outputs":[],"source":"","execution_count":null},{"metadata":{"id":"C980E9722D0444EC81C955566543260F"},"cell_type":"code","outputs":[],"source":"# 一篇文章的tag_score计算源码\n# calctagScore(dataSet,testSet)\n# 读取queryid、tag、lable三列特征的dataframe 作为dataSet参数\n# 读取 tag 该列特征的dataframe 作为testSet参数\n# 这里需要计算四次：\n#    1、第二亿到第十亿的queryid、tag、lable作为dataSet参数，第一亿的tag作为testSet参数\n#    2、第一亿到第九亿的queryid、tag、lable作为dataSet参数，第十亿的tag作为testSet参数\n#    3、第一亿到第十亿的queryid、tag、lable作为dataSet参数，test1的tag作为testSet参数\n#    4、第二亿到第十亿的queryid、tag、lable作为dataSet参数，test2的tag作为testSet参数","execution_count":null},{"metadata":{"id":"FC56942038FA4FC28168EA0DAFAB7B75","hide_input":true},"cell_type":"code","outputs":[],"source":"import pandas as pd\nimport gc\n\ndef reduce_mem_usage(D,verbose=True):\n    start_mem = D.memory_usage().sum() / 1024**2\n    for c, d in zip(D.columns, D.dtypes):\n        if d.kind == 'f':\n            D[c] = pd.to_numeric(D[c], downcast='float')\n        elif d.kind == 'i':\n            D[c] = pd.to_numeric(D[c], downcast='signed')\n    end_mem = D.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return D\n\ndef calctagScore(dataSet,testSet):\n    dataSet['queryid_num']=dataSet.groupby(by='query_id').label.transform('count')\n    dataSet['queryid_click_num']=dataSet.groupby(by='query_id').label.transform('sum')\n    dataSet=dataSet.drop(columns=[\"query_id\"])\n    print(\"ok1\")\n    gc.collect()\n    dataSet = reduce_mem_usage(dataSet,verbose=True)\n    \n    dataSet['queryid_click_rate'] = dataSet['queryid_click_num']/dataSet['queryid_num']\n    print(\"ok2\")\n    dataSet=dataSet.drop(columns=[\"queryid_num\",\"queryid_click_num\"])\n    gc.collect()\n    dataSet = reduce_mem_usage(dataSet,verbose=True)\n    \n    dataSet['title_queryid_score']= dataSet['label']- dataSet['queryid_click_rate']\n    dataSet=dataSet.drop(columns=[\"label\"])\n    gc.collect()\n    print(\"ok3\")\n    dataSet = reduce_mem_usage(dataSet,verbose=True)\n    \n    feature='tag'\n    dataSet[feature+'_score'] = dataSet.groupby(by=feature)['title_queryid_score'].transform('sum')\n    dataSet=reduce_mem_usage(dataSet,verbose=True)\n    dataSet=dataSet[[feature,feature+'_score']].drop_duplicates()\n    dataSet = reduce_mem_usage(dataSet,verbose=True)\n    gc.collect()\n    \n    testSet = pd.merge(testSet, dataSet, on=[feature], how='left')\n    \n    return testSet\n\n\n# id_feature='/home/kesci/input/bytedance/train_final.csv'\n# usecols=[0,4]\n# names=['query_id','label']\n\n# print(\"正在读取：\",id_feature)\n# dataSet = pd.read_csv(id_feature,\n#                       header=None,\n#                     #   nrows=100000000,\n#                       usecols=usecols,\n#                       names=names\n#                       )\n\n# path=f'/home/kesci/work/featureMap/train/tag_train_feat.h5'\n# dataSet=pd.concat([dataSet,pd.read_hdf(path, key='data')], axis=1)\n\n# dataSet=reduce_mem_usage(dataSet,verbose=True)\n\n# testSet=dataSet[:100000000][[\"tag\"]]\n# print(testSet.shape)\n\n# dataSet=dataSet[100000000:].reset_index(drop=True)\n# print(dataSet.shape)\n\n# gc.collect()\n\n# dataSetType=\"train\"\n# path=f\"/home/kesci/work/pre_3billion_data/{dataSetType}/tag_score_{dataSetType}_feat.h5\"\n# testSet[\"tag_score\"]=testSet[\"tag_score\"].fillna(0)\n# testSet[[\"tag_score\"]].to_hdf(path, index=None, key='data', complevel=9)\n# print(\"OK\")","execution_count":null},{"metadata":{"id":"8E3F3F85FB1740A3A7FD4ABF51CFEA03"},"cell_type":"code","outputs":[],"source":"","execution_count":null},{"metadata":{"id":"31A3EE2DA411400E821A07CFB2FD1CE8"},"cell_type":"code","outputs":[],"source":"# 以下是生成SIF特征的词频表","execution_count":null},{"metadata":{"id":"049A9D6F436746F28585275D1C9FF835","hide_input":true},"cell_type":"code","outputs":[],"source":"import pandas as pd\nimport gc\nfrom tqdm import tqdm\n\n\nwords_dict = {}\n\n\ntext_data1 = pd.read_csv('/home/kesci/input/bytedance/train_final.csv', usecols=[1, 3],\n                         names=['query', 'title'], nrows=100000000)\n\n\nquery_list = text_data1['query'].drop_duplicates().values.tolist()\n\nfor item in tqdm(query_list):\n    query = item.split()\n    for word in query:\n        words_dict[word] = words_dict.get(word, 0) + 1\n\ndel query_list\ngc.collect()\n\ntitle_list = text_data1['title'].drop_duplicates().values.tolist()\n\nfor item in tqdm(title_list):\n    title = item.split()\n    for word in title:\n        words_dict[word] = words_dict.get(word, 0) + 1\n\ndel title_list\n\ndel text_data1\ngc.collect()\n\ntext_data2 = pd.read_csv('/home/kesci/work/word2vec/post_10kw.csv')\n\n\nquery_list = text_data2['query'].drop_duplicates().values.tolist()\n\nfor item in tqdm(query_list):\n    query = item.split()\n    for word in query:\n        words_dict[word] = words_dict.get(word, 0) + 1\n\ndel query_list\ngc.collect()\n\ntitle_list = text_data2['title'].drop_duplicates().values.tolist()\n\nfor item in tqdm(title_list):\n    title = item.split()\n    for word in title:\n        words_dict[word] = words_dict.get(word, 0) + 1\n\ndel title_list\ndel text_data2\ngc.collect()\n\ntest = pd.read_csv('/home/kesci/input/bytedance/test_final_part1.csv', usecols=[1, 3],\n                   names=['query', 'title'])\n\n\nquery_list = test['query'].drop_duplicates().values.tolist()\n\nfor item in tqdm(query_list):\n    query = item.split()\n    for word in query:\n        words_dict[word] = words_dict.get(word, 0) + 1\n\ndel query_list\ngc.collect()\n\ntitle_list = test['title'].drop_duplicates().values.tolist()\n\nfor item in tqdm(title_list):\n    title = item.split()\n    for word in title:\n        words_dict[word] = words_dict.get(word, 0) + 1\n\ndel title_list\ndel test\ngc.collect()\n\ntest2 = pd.read_csv('/home/kesci/input/bytedance/bytedance_contest.final_2.csv', usecols=[1, 3],\n                    names=['query', 'title'])\n\n\nquery_list = test2['query'].drop_duplicates().values.tolist()\n\nfor item in tqdm(query_list):\n    query = item.split()\n    for word in query:\n        words_dict[word] = words_dict.get(word, 0) + 1\n\ndel query_list\ngc.collect()\n\ntitle_list = test2['title'].drop_duplicates().values.tolist()\n\nfor item in tqdm(title_list):\n    title = item.split()\n    for word in title:\n        words_dict[word] = words_dict.get(word, 0) + 1\n\ndel title_list\ndel test2\ngc.collect()\n\nimport operator\n\nsorted_words_dict = sorted(words_dict.items(), key=operator.itemgetter(1), reverse=True)\n\nfrom tqdm import tqdm\n\nwith open('/home/kesci/work/sif_embbeding/word_freq_final.txt', 'w') as f: # trianpre10kw post10kw test1 test2\n    for word, freq in tqdm(sorted_words_dict):\n        f.write(word + ' ' + str(freq) + '\\n')","execution_count":null},{"metadata":{"id":"AC497DFEBF224F1A8BBBFA2EE021B955"},"cell_type":"code","outputs":[],"source":"","execution_count":null},{"metadata":{"id":"99A64717D4754B77BCBC851EE03814F6"},"cell_type":"code","outputs":[],"source":"# 以下生成SIF特征 sif_cos sif_l2  需要修改几次路径 分别是前1e数据 后1e数据 test1 test2 对应数据的变量是data_train\n# \n# 后1e数据路径为 /home/kesci/work/word2vec/post_10kw.csv 是我在最开始保存的 \n# 前1e数据 test1 test2路径只需读取原始的数据即可，前1e记得 nrows=100000000\n# 下面的cell以test2为例 ","execution_count":null},{"metadata":{"id":"C0FB250055434E4189A7A44C3F90E41C","hide_input":true},"cell_type":"code","outputs":[],"source":"import numpy as np\nfrom gensim.models import KeyedVectors\nfrom sklearn.decomposition import TruncatedSVD\n\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024 ** 2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (\n            start_mem - end_mem) / start_mem))\n    return df\n\n\nclass Params(object):\n    def __init__(self):\n        self.LW = 1e-5\n        self.LC = 1e-5\n        self.eta = 0.05\n\n    def __str__(self):\n        t = \"LW\", self.LW, \", LC\", self.LC, \", eta\", self.eta\n        t = list(map(str, t))\n        return ' '.join(t)\n\n\ndef getWordmap(w2v_file_name):\n    w2v_model = KeyedVectors.load_word2vec_format(w2v_file_name, binary=True)\n    word2ids = w2v_model.wv.vocab\n    words = {}\n    for word, vocab in word2ids.items():\n        words[word] = vocab.index\n    return (words, w2v_model.vectors)\n\n\ndef getWordWeight(weightfile, a=1e-3):\n    if a <= 0:  # when the parameter makes no sense, use unweighted\n        a = 1.0\n\n    word2weight = {}\n    with open(weightfile) as f:\n        lines = f.readlines()\n    N = 0\n    for i in lines:\n        i = i.strip()\n        if (len(i) > 0):\n            i = i.split()\n            if (len(i) == 2):\n                word2weight[i[0]] = float(i[1])\n                N += float(i[1])\n            else:\n                print(i)\n    for key, value in word2weight.items():\n        word2weight[key] = a / (a + value / N)\n    return word2weight\n\n\ndef getWeight(words, word2weight):\n    weight4ind = {}\n    for word, ind in words.items():\n        if word in word2weight:\n            weight4ind[ind] = word2weight[word]\n        else:\n            weight4ind[ind] = 1.0\n    return weight4ind\n\n\ndef prepare_data(list_of_seqs):\n    lengths = [len(s) for s in list_of_seqs]\n    n_samples = len(list_of_seqs)\n    maxlen = np.max(lengths)\n    x = np.zeros((n_samples, maxlen)).astype('int32')\n    x_mask = np.zeros((n_samples, maxlen)).astype('float32')\n    for idx, s in enumerate(list_of_seqs):\n        x[idx, :lengths[idx]] = s\n        x_mask[idx, :lengths[idx]] = 1.\n    x_mask = np.asarray(x_mask, dtype='float32')\n    return x, x_mask\n\n\ndef lookupIDX(words, w):\n    return words.get(w, len(words) - 1)\n\n\ndef getSeq(p1, words):\n    p1 = p1.split()\n    X1 = []\n    for i in p1:\n        X1.append(lookupIDX(words, i))\n    return X1\n\n\ndef sentences2idx(sentences, words):\n    \"\"\"\n    Given a list of sentences, output array of word indices that can be fed into the algorithms.\n    :param sentences: a list of sentences\n    :param words: a dictionary, words['str'] is the indices of the word 'str'\n    :return: x1, m1. x1[i, :] is the word indices in sentence i, m1[i,:] is the mask for sentence i (0 means no word at the location)\n    \"\"\"\n    seq1 = []\n    for i in sentences:\n        seq1.append(getSeq(i, words))\n    x1, m1 = prepare_data(seq1)\n    return x1, m1\n\n\ndef seq2weight(seq, mask, weight4ind):\n    weight4ind_np = np.zeros((len(weight4ind),))\n    for index, weight in weight4ind.items():\n        weight4ind_np[index] = weight\n\n    weight = np.zeros(seq.shape).astype('float32')\n    for index, row in enumerate(seq):\n        weight[index] = weight4ind_np[row]\n    weight = weight * mask\n    return weight\n\n\ndef get_weighted_average(We, x, w):\n    \"\"\"\n    Compute the weighted average vectors\n    :param We: We[i,:] is the vector for word i\n    :param x: x[i, :] are the indices of the words in sentence i\n    :param w: w[i, :] are the weights for the words in sentence i\n    :return: emb[i, :] are the weighted average vector for sentence i\n    \"\"\"\n    n_samples = x.shape[0]\n    emb = np.zeros((n_samples, We.shape[1]))\n    for i in range(n_samples):\n        emb[i, :] = w[i, :].dot(We[x[i, :], :]) / np.count_nonzero(w[i, :])\n    return emb\n\n\ndef compute_pc(X, npc=1):\n    \"\"\"\n    Compute the principal components. DO NOT MAKE THE DATA ZERO MEAN!\n    :param X: X[i,:] is a data point\n    :param npc: number of principal components to remove\n    :return: component_[i,:] is the i-th pc\n    \"\"\"\n    svd = TruncatedSVD(n_components=npc, n_iter=7, random_state=0)\n    svd.fit(X)\n    return svd.components_\n\n\ndef remove_pc(X, npc=1):\n    \"\"\"\n    Remove the projection on the principal components\n    :param X: X[i,:] is a data point\n    :param npc: number of principal components to remove\n    :return: XX[i, :] is the data point after removing its projection\n    \"\"\"\n    pc = compute_pc(X, npc)\n    if npc == 1:\n        XX = X - X.dot(pc.transpose()) * pc\n    else:\n        XX = X - X.dot(pc.transpose()).dot(pc)\n    return XX\n\n\ndef SIF_embedding(We, x, w, params):\n    \"\"\"\n    Compute the scores between pairs of sentences using weighted average + removing the projection on the first principal component\n    :param We: We[i,:] is the vector for word i\n    :param x: x[i, :] are the indices of the words in the i-th sentence\n    :param w: w[i, :] are the weights for the words in the i-th sentence\n    :param params.rmpc: if >0, remove the projections of the sentence embeddings to their first principal component\n    :return: emb, emb[i, :] is the embedding for sentence i\n    \"\"\"\n    emb = get_weighted_average(We, x, w)\n    if params.rmpc > 0:\n        emb = remove_pc(emb, params.rmpc)\n    return emb\n\n\nwordfile = '/home/kesci/work/word2vec/word2vec_100.bin'  # word vector file, can be downloaded from GloVe website\nweightfile = '/home/kesci/work/sif_embbeding/word_freq_final.txt'  # each line is a word and its frequency\nweightpara = 1e-3  # the parameter in the SIF weighting scheme, usually in the range [3e-5, 3e-3]\nrmpc = 1  # number of principal components to remove in SIF weighting scheme\n\n# load word vectors\n(words, We) = getWordmap(wordfile)\nprint('getWordmap')\n# load word weights\nword2weight = getWordWeight(weightfile, weightpara)  # word2weight['str'] is the weight for the word 'str'\nweight4ind = getWeight(words, word2weight)  # weight4ind[i] is the weight for the i-th word\nprint('weight4ind')\n\nimport pandas as pd\nimport gc\n\nchunk_size = 1000000\n\ndata_train = pd.read_csv('/home/kesci/input/bytedance/bytedance_contest.final_2.csv', usecols=[1, 3],\n                    names=['query', 'title'], chunksize=chunk_size)\n\nfrom tqdm import tqdm\n\nparams = Params()\nparams.rmpc = rmpc\n\nimport time\n\nfor index, current_data in enumerate(data_train):\n    print(index)\n    train_cos_sim = []\n    train_l2_sim = []\n\n    sentences = current_data['query'].values.tolist() + current_data['title'].values.tolist()  # 前半部分是query 后半部分是title\n\n    since = time.time()\n\n    x, m = sentences2idx(sentences,\n                         words)  # x is the array of word indices, m is the binary mask indicating whether there is a word in that location\n    time_elapsed = time.time() - since\n    print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n\n    since = time.time()\n\n    w = seq2weight(x, m, weight4ind)  # get word weights\n    # set parameters\n\n    time_elapsed = time.time() - since\n    print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))  # 打印出来时间\n    # get SIF embedding\n\n    since = time.time()\n\n    embedding = SIF_embedding(We, x, w, params)\n\n    time_elapsed = time.time() - since\n    print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))  # 打印出来时间\n    all_len = embedding.shape[0]\n    query_emb = embedding[:all_len // 2]\n    title_emb = embedding[all_len // 2:]\n\n    for i in tqdm(range(current_data.shape[0])):\n        v1 = query_emb[i]\n        v2 = title_emb[i]\n        cosine_distance = np.dot(v1, v2) / (np.linalg.norm(v1) * (np.linalg.norm(v2)))\n        l2_distance = np.linalg.norm(v1 - v2)\n        train_cos_sim.append(cosine_distance)\n        train_l2_sim.append(l2_distance)\n\n    temp_l2 = pd.DataFrame()\n    temp_l2['sif_l2'] = train_l2_sim\n    temp_l2 = reduce_mem_usage(temp_l2)\n    temp_l2.to_hdf(f'/home/kesci/work/sif_embbeding/sif_100_test2/l2/sif_l2_test2_part{index}.h5', index=None,\n                   key='data', complevel=9)\n\n    temp_cos = pd.DataFrame()\n    temp_cos['sif_cos'] = train_cos_sim\n    temp_cos = reduce_mem_usage(temp_cos)\n    temp_cos.to_hdf(f'/home/kesci/work/sif_embbeding/sif_100_test2/cos/sif_cos_test2_part{index}.h5', index=None,\n                    key='data', complevel=9)\n\n    del sentences, current_data, embedding, query_emb, title_emb, x, m, w, temp_l2, temp_cos\n    gc.collect()","execution_count":null},{"metadata":{"id":"83DD5E10B39C41EA934AD81CF43D1C43"},"cell_type":"code","outputs":[],"source":"","execution_count":null},{"metadata":{"id":"9C43B932007442E59594AD1F6FD7F049"},"cell_type":"code","outputs":[],"source":"# 以下是生成 NN_SIM特征  NN_SIM是纯文本esim对其他数据的文本相似度的计算（训练是使用的另外部分的数据）\n# 需要修改几次路径 分别是前1e数据 后1e数据 test1 test2 对应数据的变量是data_reader 需要修改这个数据的读取路径\n# \n# 后1e数据路径为 /home/kesci/work/word2vec/post_10kw.csv 是我在最开始保存的 \n# 前1e数据 test1 test2路径只需读取原始的数据即可，前1e记得 nrows=100000000\n# 下面的cell以test2为例 防止跑失败，分chunk进行预测，然后每次将当前chunk的保存为pickle二进制文件 \n# 使用时需要进行拼接 当然也要注意保持pickle的路径","execution_count":null},{"metadata":{"id":"03A5722F773149C8934067A8E2982784","hide_input":true},"cell_type":"code","outputs":[],"source":"import pickle\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom keras.backend.tensorflow_backend import set_session\nfrom sklearn.metrics import roc_auc_score\nfrom tensorflow import set_random_seed\n\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nsess = tf.Session(config=config)\nset_session(sess)\n\nseed = 2019\n\nset_random_seed(seed)  # Tensorflow\nnp.random.seed(seed)  # NumPy\n\nW2V_DIM = 200\n\nmax_seq_len = 25\nepochs = 20\n\nimport keras.backend as K\nfrom keras.engine import Layer\n\n\nclass DotProductAttention(Layer):\n    \"\"\"\n    dot-product-attention mechanism, supporting masking\n    \"\"\"\n\n    def __init__(self, return_attend_weight=False, keep_mask=True, **kwargs):\n        self.return_attend_weight = return_attend_weight\n        self.keep_mask = keep_mask\n        self.supports_masking = True\n        super(DotProductAttention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert isinstance(input_shape, list)\n        input_shape_a, input_shape_b = input_shape\n\n        if len(input_shape_a) != 3 or len(input_shape_b) != 3:\n            raise ValueError('Inputs into DotProductAttention should be 3D tensors')\n\n        if input_shape_a[-1] != input_shape_b[-1]:\n            raise ValueError('Inputs into DotProductAttention should have the same dimensionality at the last axis')\n\n    def call(self, inputs, mask=None):\n        assert isinstance(inputs, list)\n        inputs_a, inputs_b = inputs\n\n        if mask is not None:\n            mask_a, mask_b = mask\n        else:\n            mask_a, mask_b = None, None\n\n        e = K.exp(K.batch_dot(inputs_a, inputs_b, axes=2))  # similarity between a & b\n\n        # apply mask before normalization (softmax)\n        if mask_a is not None:\n            e *= K.expand_dims(K.cast(mask_a, K.floatx()), 2)\n        if mask_b is not None:\n            e *= K.expand_dims(K.cast(mask_b, K.floatx()), 1)\n\n        e_b = e / K.cast(K.sum(e, axis=2, keepdims=True) + K.epsilon(), K.floatx())  # attention weight over b\n        e_a = e / K.cast(K.sum(e, axis=1, keepdims=True) + K.epsilon(), K.floatx())  # attention weight over a\n\n        if self.return_attend_weight:\n            return [e_b, e_a]\n\n        a_attend = K.batch_dot(e_b, inputs_b, axes=(2, 1))  # a attend to b\n        b_attend = K.batch_dot(e_a, inputs_a, axes=(1, 1))  # b attend to a\n        return [a_attend, b_attend]\n\n    def compute_mask(self, inputs, mask=None):\n        if self.keep_mask:\n            return mask\n        else:\n            return [None, None]\n\n    def compute_output_shape(self, input_shape):\n        if self.return_attend_weight:\n            input_shape_a, input_shape_b = input_shape\n            return [(input_shape_a[0], input_shape_a[1], input_shape_b[1]),\n                    (input_shape_a[0], input_shape_a[1], input_shape_b[1])]\n        return input_shape\n\n\ndef auc(y_true, y_pred):\n    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)\n\n\ndef seq_padding(X, max_len=20):\n    return [x + [PAD] * (max_len - len(x)) if len(x) < max_len else x[:max_len] for x in X]\n\n\nbatch_size = 1024 * 10\n\nPAD = 0\nUNK = 1\n\nfrom keras.models import load_model\n\nprint('gen nn_sim feature')\nmodel_file_name = './sunrui/nn/weights-04_esim_64_nn_sim.hdf5'\n\nmodel = load_model(model_file_name, custom_objects={\n    'DotProductAttention': DotProductAttention,\n    'auc': auc\n})\n\nwith open('/home/kesci/work/sunrui/NN_second_2e/second_6kw_nn_sim.pkl', 'rb') as f:\n    word2id = pickle.load(f)\n\ndata_reader = pd.read_csv('/home/kesci/input/bytedance/bytedance_contest.final_2.csv', usecols=[1, 3],\n                             names=['query', 'title'],chunksize=5000000)\n                             \nfrom tqdm import tqdm\nimport pickle\n\npreds_list = []\n\nfor index, data in enumerate(data_reader):\n    print(f'index = {index}')\n    Q_val = []\n    D_val = []\n    for query in tqdm(data['query']):\n        query = query.split()\n        Q_val.append([word2id[w] for w in query])  # 没有命中就返回UNK\n\n    for title in tqdm(data['title']):\n        title = title.split()\n        D_val.append([word2id[w] for w in title])\n\n    query_input = seq_padding(Q_val, max_seq_len)\n    title_input = seq_padding(D_val, max_seq_len)\n\n    preds = model.predict([query_input, title_input], batch_size=batch_size)\n\n    preds_np = np.squeeze(preds)\n    with open(f'/home/kesci/work/sunrui/nn_sim/test2/nn_sim_test2_part{index}.pkl', 'wb') as f:\n        pickle.dump(preds_np, f)\n        \n    preds_list.append(preds_np)\n    del query_input, title_input, data, Q_val, D_val","execution_count":null}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":0}