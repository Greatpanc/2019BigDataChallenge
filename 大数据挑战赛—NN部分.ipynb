{"cells":[{"outputs":[],"execution_count":null,"source":"# 查看当前挂载的数据集目录\n!ls /home/kesci/input/","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"2EE3E9584470429781251BF00E53FE6A"}},{"outputs":[],"execution_count":null,"source":"# 查看个人持久化工作区文件\n!ls /home/kesci/work/","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"4B999D8BFB254C7E8C9AB520F5ECC353"}},{"outputs":[],"execution_count":null,"source":"# 查看当前kernerl下的package\n!pip list --format=columns","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"924677182710411187744D4C448CE3D5"}},{"outputs":[],"execution_count":null,"source":"# 显示cell运行时长\n%load_ext klab-autotime","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"8F15C29F68DA40A78D7B21E6D1384253"}},{"metadata":{"id":"BE0F08FE72B8405F9EE6CE0321571297"},"cell_type":"code","outputs":[],"source":"","execution_count":null},{"metadata":{"id":"998A01F887FC4E678F16941D0E49CB79"},"cell_type":"code","outputs":[],"source":"# 以下代码是保存最后1e的数据到工作区，因为训练集数据量过大，直接通过跳行的方式读取会很慢","execution_count":null},{"metadata":{"id":"69E914AB01D94D4784FB026577FFF29D","hide_input":true},"cell_type":"code","outputs":[],"source":"import pandas as pd\nimport gc\n\nchunksize = 300000000\n\ndata_reader = pd.read_csv('', header=None, usecols=[0, 1, 3, 4], names=['query_id', 'query', 'title', 'label'],\n                          chunksize=chunksize)\n\npost_10kw_data = None\n\nfor index, data in enumerate(data_reader):\n    if index == 3:\n        post_10kw_data = data\n    else:\n        del data\n        gc.collect()\n\n# 单独保存后1e的数据 用于训练词向量 训练模型等\npost_10kw_data.to_csv('/home/kesci/work/word2vec/post_10kw.csv', index=None)","execution_count":null},{"metadata":{"id":"385AF9A76B984B0BBD6EB872232B7FC0"},"cell_type":"code","outputs":[],"source":"","execution_count":null},{"metadata":{"id":"CFA5C1554985470692EB5A4C6F42CB5C"},"cell_type":"code","outputs":[],"source":"# final word2vec 训练","execution_count":null},{"metadata":{"id":"4A837308AC1E4BB8A675B71C9CC7FBE6","hide_input":true},"cell_type":"code","outputs":[],"source":"import time\r\n\r\nimport pandas as pd\r\nimport gc\r\n\r\nfrom gensim.models.callbacks import CallbackAny2Vec\r\nfrom gensim.models.word2vec import Word2Vec\r\n\r\n\r\nclass EpochSaver(CallbackAny2Vec):\r\n    '''用于保存模型, 打印损失函数等等'''\r\n\r\n    def __init__(self):\r\n        self.epoch = 0\r\n        self.pre_loss = 0\r\n        self.best_loss = 999999999.9\r\n        self.since = time.time()\r\n\r\n    def on_epoch_end(self, model):\r\n        self.epoch += 1\r\n        cum_loss = model.get_latest_training_loss()  # 返回的是从第一个epoch累计的\r\n        epoch_loss = cum_loss - self.pre_loss\r\n        time_taken = time.time() - self.since\r\n        print(\"Epoch %d, loss: %.2f, time: %dmin %ds\" %\r\n              (self.epoch, epoch_loss, time_taken // 60, time_taken % 60))\r\n        if self.best_loss > epoch_loss:\r\n            self.best_loss = epoch_loss\r\n            print(\"Better model. Best loss: %.2f\" % self.best_loss)\r\n            model.save('./word2vec/word2vec_100.model')\r\n            model.wv.save_word2vec_format('./word2vec/word2vec_100.bin', binary=True)\r\n            print(\"Model %s save done!\" % './word2vec/word2vec_100.model')\r\n\r\n        self.pre_loss = cum_loss\r\n        self.since = time.time()\r\n\r\n\r\ndef log(log: str):\r\n    print(log)\r\n\r\n\r\ndef time_log(time_elapsed):\r\n    print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))  # 打印时间\r\n\r\n\r\ndef log_event(event: str):\r\n    log(event)\r\n\r\n\r\ndef get_sentence(train_path_1: str, train_path2: str, test_2kw_path: str, test_1e_path: str, chunk_size: int):\r\n    for index, data in enumerate(\r\n            pd.read_csv(train_path_1, chunksize=chunk_size, header=None, usecols=[1, 3], names=['query', 'title'],\r\n                        nrows=200000000)):\r\n\r\n        print(f'train 1 path = {train_path_1} index = {index}')\r\n        query_df = data['query'].drop_duplicates()\r\n\r\n        query_list = query_df.values.tolist()\r\n        for item in query_list:\r\n            yield item\r\n\r\n        title_list = data['title'].values.tolist()\r\n        for item in title_list:\r\n            yield item\r\n        del query_list, title_list, query_df, data\r\n        gc.collect()\r\n\r\n    for index, data in enumerate(\r\n            pd.read_csv(train_path2, chunksize=chunk_size)):\r\n\r\n        print(f'train 2 path = {train_path2} index = {index}')\r\n        query_df = data['query'].drop_duplicates()\r\n\r\n        query_list = query_df.values.tolist()\r\n        for item in query_list:\r\n            yield item\r\n\r\n        title_list = data['title'].values.tolist()\r\n        for item in title_list:\r\n            yield item\r\n\r\n        del query_list, title_list, query_df, data\r\n        gc.collect()\r\n\r\n    for index, data in enumerate(\r\n            pd.read_csv(test_2kw_path, chunksize=chunk_size, header=None, usecols=[1, 3], names=['query', 'title'])):\r\n        print(f'test path = {test_2kw_path} index = {index}')\r\n\r\n        query_df = data['query'].drop_duplicates()\r\n\r\n        query_list = query_df.values.tolist()\r\n        for item in query_list:\r\n            yield item\r\n\r\n        title_list = data['title'].values.tolist()\r\n        for item in title_list:\r\n            yield item\r\n\r\n        del query_list, title_list, query_df, data\r\n        gc.collect()\r\n\r\n    for index, data in enumerate(\r\n            pd.read_csv(test_1e_path, chunksize=chunk_size, header=None, usecols=[1, 3], names=['query', 'title'])):\r\n        print(f'path = {test_1e_path} index = {index}')\r\n        query_df = data['query'].drop_duplicates()\r\n\r\n        query_list = query_df.values.tolist()\r\n        for item in query_list:\r\n            yield item\r\n\r\n        title_list = data['title'].values.tolist()\r\n        for item in title_list:\r\n            yield item\r\n\r\n        del query_list, title_list, query_df, data\r\n        gc.collect()\r\n\r\n\r\nclass Sentence(object):\r\n    def __init__(self, train_path_1: str, train_path2: str, test_2kw_path: str, test_1e_path: str, chunk_size: int):\r\n        self.train_path_1 = train_path_1\r\n        self.train_path2 = train_path2\r\n        self.test_2kw_path = test_2kw_path\r\n        self.test_1e_path = test_1e_path\r\n        self.chunk_size = chunk_size\r\n\r\n    def __iter__(self):\r\n        for sentence in get_sentence(self.train_path_1, self.train_path2, self.test_2kw_path, self.test_1e_path,\r\n                                     self.chunk_size):\r\n            seg_list = sentence.split()\r\n            yield seg_list\r\n\r\n\r\nsentences = Sentence('/home/kesci/input/bytedance/train_final.csv', '/home/kesci/work/word2vec/post_10kw.csv',\r\n                     '/home/kesci/input/bytedance/test_final_part1.csv',\r\n                     '/home/kesci/input/bytedance/bytedance_contest.final_2.csv',\r\n                     chunk_size=5000000)\r\n\r\nword2vec_dim = 100\r\n\r\nmodel = Word2Vec(size=word2vec_dim, window=5, sg=1, min_count=1, workers=4)\r\n\r\nmodel.build_vocab(sentences)\r\nmodel.train(sentences, total_examples=model.corpus_count, epochs=10, compute_loss=True, report_delay=5 * 60,\r\n            callbacks=[EpochSaver()])","execution_count":null},{"metadata":{"id":"00FA90DEC8DF43A8AF1C7A6FAB2C72B9"},"cell_type":"code","outputs":[],"source":"","execution_count":null},{"metadata":{"id":"68F245621EAA41499249CA0A1C5FB4EB"},"cell_type":"code","outputs":[],"source":"# 训练fasttext gensim中fasttext不能打印loss 就没有callback","execution_count":null},{"metadata":{"id":"C889A9982A884014A2ECEB1FE6580560","hide_input":true},"cell_type":"code","outputs":[],"source":"import pandas as pd\nfrom gensim.models.fasttext import FastText\nimport gc\n\n\ndef log(log: str):\n    print(log)\n\n\ndef time_log(time_elapsed):\n    print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))  # 打印时间\n\n\ndef log_event(event: str):\n    log(event)\n\n\ndef get_sentence(train_path_1: str, train_path2: str, test_2kw_path: str, test_1e_path: str, chunk_size: int):\n    for index, data in enumerate(\n            pd.read_csv(train_path_1, chunksize=chunk_size, header=None, usecols=[1, 3], names=['query', 'title'],\n                        nrows=200000000)):\n\n        print(f'train 1 path = {train_path_1} index = {index}')\n        query_df = data['query'].drop_duplicates()\n\n        query_list = query_df.values.tolist()\n        for item in query_list:\n            yield item\n\n        title_list = data['title'].values.tolist()\n        for item in title_list:\n            yield item\n        del query_list, title_list, query_df, data\n        gc.collect()\n\n    for index, data in enumerate(\n            pd.read_csv(train_path2, chunksize=chunk_size)):\n\n        print(f'train 2 path = {train_path2} index = {index}')\n        query_df = data['query'].drop_duplicates()\n\n        query_list = query_df.values.tolist()\n        for item in query_list:\n            yield item\n\n        title_list = data['title'].values.tolist()\n        for item in title_list:\n            yield item\n\n        del query_list, title_list, query_df, data\n        gc.collect()\n\n    for index, data in enumerate(\n            pd.read_csv(test_2kw_path, chunksize=chunk_size, header=None, usecols=[1, 3], names=['query', 'title'])):\n        print(f'test path = {test_2kw_path} index = {index}')\n\n        query_df = data['query'].drop_duplicates()\n\n        query_list = query_df.values.tolist()\n        for item in query_list:\n            yield item\n\n        title_list = data['title'].values.tolist()\n        for item in title_list:\n            yield item\n\n        del query_list, title_list, query_df, data\n        gc.collect()\n\n    for index, data in enumerate(\n            pd.read_csv(test_1e_path, chunksize=chunk_size, header=None, usecols=[1, 3], names=['query', 'title'])):\n        print(f'path = {test_1e_path} index = {index}')\n        query_df = data['query'].drop_duplicates()\n\n        query_list = query_df.values.tolist()\n        for item in query_list:\n            yield item\n\n        title_list = data['title'].values.tolist()\n        for item in title_list:\n            yield item\n\n        del query_list, title_list, query_df, data\n        gc.collect()\n\n\nclass Sentence(object):\n    def __init__(self, train_path_1: str, train_path2: str, test_2kw_path: str, test_1e_path: str, chunk_size: int):\n        self.train_path_1 = train_path_1\n        self.train_path2 = train_path2\n        self.test_2kw_path = test_2kw_path\n        self.test_1e_path = test_1e_path\n        self.chunk_size = chunk_size\n\n    def __iter__(self):\n        for sentence in get_sentence(self.train_path_1, self.train_path2, self.test_2kw_path, self.test_1e_path,\n                                     self.chunk_size):\n            seg_list = sentence.split()\n            yield seg_list\n\n\nsentences = Sentence('/home/kesci/input/bytedance/train_final.csv', '/home/kesci/work/word2vec/post_10kw.csv',\n                     '/home/kesci/input/bytedance/test_final_part1.csv',\n                     '/home/kesci/input/bytedance/bytedance_contest.final_2.csv',\n                     chunk_size=5000000)\n\nword2vec_dim = 100\n\nmodel = FastText(size=word2vec_dim, window=5, sg=1, min_count=1, workers=4)\n\nmodel.build_vocab(sentences)\nmodel.train(sentences, total_examples=model.corpus_count, epochs=5)\n\nmodel.save('./fasttext/fasttext_100.model')\nmodel.wv.save_word2vec_format('./fasttext/fasttext_100.bin', binary=True)","execution_count":null},{"metadata":{"id":"61DAD8B0CAAD4959851E510646178F3A"},"cell_type":"code","outputs":[],"source":"","execution_count":null},{"metadata":{"id":"3EB5DC1DD6274B2EA1978F33F65E44E7"},"cell_type":"code","outputs":[],"source":"# 以下cell是制作词表与保存词表 前2e数据和后1e数据和test1 test2","execution_count":null},{"metadata":{"id":"C8FBD00C80AE49AF87F182B343A03E91","hide_input":true},"cell_type":"code","outputs":[],"source":"import pandas as pd\nimport gc\nfrom tqdm import tqdm\n\n\nwords_dict = {}\n\n\ntext_data1 = pd.read_csv('/home/kesci/input/bytedance/train_final.csv', usecols=[1, 3],\n                         names=['query', 'title'], nrows=200000000)\n\n\nquery_list = text_data1['query'].drop_duplicates().values.tolist()\n\nfor item in tqdm(query_list):\n    query = item.split()\n    for word in query:\n        words_dict[word] = words_dict.get(word, 0) + 1\n\ndel query_list\ngc.collect()\n\ntitle_list = text_data1['title'].drop_duplicates().values.tolist()\n\nfor item in tqdm(title_list):\n    title = item.split()\n    for word in title:\n        words_dict[word] = words_dict.get(word, 0) + 1\n\ndel title_list\n\ndel text_data1\ngc.collect()\n\ntext_data2 = pd.read_csv('/home/kesci/work/word2vec/post_10kw.csv')\n\n\nquery_list = text_data2['query'].drop_duplicates().values.tolist()\n\nfor item in tqdm(query_list):\n    query = item.split()\n    for word in query:\n        words_dict[word] = words_dict.get(word, 0) + 1\n\ndel query_list\ngc.collect()\n\ntitle_list = text_data2['title'].drop_duplicates().values.tolist()\n\nfor item in tqdm(title_list):\n    title = item.split()\n    for word in title:\n        words_dict[word] = words_dict.get(word, 0) + 1\n\ndel title_list\ndel text_data2\ngc.collect()\n\ntest = pd.read_csv('/home/kesci/input/bytedance/test_final_part1.csv', usecols=[1, 3],\n                   names=['query', 'title'])\n\n\nquery_list = test['query'].drop_duplicates().values.tolist()\n\nfor item in tqdm(query_list):\n    query = item.split()\n    for word in query:\n        words_dict[word] = words_dict.get(word, 0) + 1\n\ndel query_list\ngc.collect()\n\ntitle_list = test['title'].drop_duplicates().values.tolist()\n\nfor item in tqdm(title_list):\n    title = item.split()\n    for word in title:\n        words_dict[word] = words_dict.get(word, 0) + 1\n\ndel title_list\ndel test\ngc.collect()\n\ntest2 = pd.read_csv('/home/kesci/input/bytedance/bytedance_contest.final_2.csv', usecols=[1, 3],\n                    names=['query', 'title'])\n\n\nquery_list = test2['query'].drop_duplicates().values.tolist()\n\nfor item in tqdm(query_list):\n    query = item.split()\n    for word in query:\n        words_dict[word] = words_dict.get(word, 0) + 1\n\ndel query_list\ngc.collect()\n\ntitle_list = test2['title'].drop_duplicates().values.tolist()\n\nfor item in tqdm(title_list):\n    title = item.split()\n    for word in title:\n        words_dict[word] = words_dict.get(word, 0) + 1\n\ndel title_list\ndel test2\ngc.collect()","execution_count":null},{"metadata":{"id":"807DC5BFC5A8456D9FE3839150EF3CAA","hide_input":true},"cell_type":"code","outputs":[],"source":"min_count = 1\r\n\r\nprint(f'words_dict len = {len(words_dict)}')\r\nwords = {i: j for i, j in list(words_dict.items()) if j >= min_count}\r\nid2words = {i + 2: j for i, j in enumerate(words)}  # padding: 0, unk: 1\r\nwords2id = {j: i for i, j in list(id2words.items())}\r\nprint(f'words2id len = {len(words2id)}')\r\n\r\n# 保存word2id\r\nimport pickle\r\n\r\n# 保存词表\r\nwith open('/home/kesci/work/sunrui/NN_second_2e/second_6kw_nn_sim.pkl','wb') as f:\r\n    pickle.dump(words2id,f)","execution_count":null},{"metadata":{"id":"BF5F43BBD8B84DF982FEB3216259A4B1"},"cell_type":"code","outputs":[],"source":"","execution_count":null},{"metadata":{"id":"BBBC3AE7E3C24E2C97535359CEACF6E0"},"cell_type":"code","outputs":[],"source":"# 以下的cell使用训练好的词向量和使用的词表 制作神经网络需要的词向量矩阵 使用100维的word2vec和fasttext拼接","execution_count":null},{"metadata":{"id":"5D83162A195B4B3D842CB3E030443962","hide_input":true},"cell_type":"code","outputs":[],"source":"from gensim.models import KeyedVectors\r\n\r\nword2vec_file = '/home/kesci/work/word2vec/word2vec_100.bin' # word2vec\r\nfast_text_file = './fasttext/fasttext_100.bin'  # fasttext\r\nEMBEDDING_DIM = 200\r\n\r\nemb_list = []\r\nw2v_model = KeyedVectors.load_word2vec_format(word2vec_file, binary=True)\r\nfasttext_model = KeyedVectors.load_word2vec_format(fast_text_file, binary=True)\r\n\r\nimport numpy as np\r\n\r\nnum_words = len(words2id) + 2\r\n\r\nembedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\r\nfrom tqdm import tqdm\r\nfor word, i in tqdm(words2id.items()):  # 因为训练的词向量没用过滤低频词 所以都可以命中\r\n    w2v = w2v_model[word]\r\n    fasttext = fasttext_model[word]\r\n    embedding_matrix[i] = np.concatenate([w2v, fasttext])\r\n\r\nnp.save('/home/kesci/work/sunrui/NN_second_2e/word2vec_fasttext_6kw_nn_sim.npy', embedding_matrix)","execution_count":null},{"metadata":{"id":"6414C8CD5E034FCD8C39024147E07F32"},"cell_type":"code","outputs":[],"source":"","execution_count":null},{"metadata":{"id":"E3B98189DB2B4D228FAD36D992B4DBDB"},"cell_type":"code","outputs":[],"source":"# 以下的cell是使用第2e的数据训练纯文本的esim网络 在本次比赛中选择第四轮的模型\n# 这个模型即用于计算相似度特征，又用于计算用于最后的模型融合","execution_count":null},{"metadata":{"id":"61DE2FE0ADBF4755BDC8CC8B7AA0B88A","hide_input":true},"cell_type":"code","outputs":[],"source":"import math\nimport os\nimport pickle\nfrom collections import OrderedDict\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom keras import Input, Model\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.layers import Embedding, Dense, Dropout, Lambda, concatenate, GlobalAveragePooling1D, subtract, multiply, \\\n    TimeDistributed, LSTM, Bidirectional\nfrom keras.regularizers import l2\nfrom tensorflow import set_random_seed\n\nfrom keras.backend.tensorflow_backend import set_session\n\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nsess = tf.Session(config=config)\nset_session(sess)\n\nseed = 2019\n\nset_random_seed(seed)  # Tensorflow\nnp.random.seed(seed)  # NumPy\n\nW2V_DIM = 200\n\nmax_seq_len = 25\nepochs = 20\n\nfrom keras.callbacks import Callback\nimport keras\nimport keras.backend as K\nfrom keras.engine import Layer\n\n# esim使用的attention层 支持mask\nclass DotProductAttention(Layer):\n    def __init__(self, return_attend_weight=False, keep_mask=True, **kwargs):\n        self.return_attend_weight = return_attend_weight\n        self.keep_mask = keep_mask\n        self.supports_masking = True\n        super(DotProductAttention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert isinstance(input_shape, list)\n        input_shape_a, input_shape_b = input_shape\n\n        if len(input_shape_a) != 3 or len(input_shape_b) != 3:\n            raise ValueError('Inputs into DotProductAttention should be 3D tensors')\n\n        if input_shape_a[-1] != input_shape_b[-1]:\n            raise ValueError('Inputs into DotProductAttention should have the same dimensionality at the last axis')\n\n    def call(self, inputs, mask=None):\n        assert isinstance(inputs, list)\n        inputs_a, inputs_b = inputs\n\n        if mask is not None:\n            mask_a, mask_b = mask\n        else:\n            mask_a, mask_b = None, None\n\n        e = K.exp(K.batch_dot(inputs_a, inputs_b, axes=2))  # similarity between a & b\n\n        # apply mask before normalization (softmax)\n        if mask_a is not None:\n            e *= K.expand_dims(K.cast(mask_a, K.floatx()), 2)\n        if mask_b is not None:\n            e *= K.expand_dims(K.cast(mask_b, K.floatx()), 1)\n\n        e_b = e / K.cast(K.sum(e, axis=2, keepdims=True) + K.epsilon(), K.floatx())  # attention weight over b\n        e_a = e / K.cast(K.sum(e, axis=1, keepdims=True) + K.epsilon(), K.floatx())  # attention weight over a\n\n        if self.return_attend_weight:\n            return [e_b, e_a]\n\n        a_attend = K.batch_dot(e_b, inputs_b, axes=(2, 1))  # a attend to b\n        b_attend = K.batch_dot(e_a, inputs_a, axes=(1, 1))  # b attend to a\n        return [a_attend, b_attend]\n\n    def compute_mask(self, inputs, mask=None):\n        if self.keep_mask:\n            return mask\n        else:\n            return [None, None]\n\n    def compute_output_shape(self, input_shape):\n        if self.return_attend_weight:\n            input_shape_a, input_shape_b = input_shape\n            return [(input_shape_a[0], input_shape_a[1], input_shape_b[1]),\n                    (input_shape_a[0], input_shape_a[1], input_shape_b[1])]\n        return input_shape\n\n# SWA算法\nclass SWA(Callback):\n    def __init__(self, checkpoint_dir, model_name, swa_start=1):\n        super(SWA, self).__init__()\n        self.checkpoint_dir = checkpoint_dir\n        self.model_name = model_name\n        self.swa_start = swa_start\n        self.swa_model = None  # the model that we will use to store the average of the weights once SWA begins\n\n    def on_train_begin(self, logs=None):\n        self.epoch = 0\n        self.swa_n = 0\n        # self.swa_model = copy.deepcopy(self.model)  # make a copy of the model we're training\n        # Note: I found deep copy of a model with customized layer would give errors\n        self.swa_model = keras.models.clone_model(self.model)\n        self.swa_model.set_weights(self.model.get_weights())  # see: https://github.com/keras-team/keras/issues/1765\n\n    def on_epoch_end(self, epoch, logs=None):\n        if (self.epoch + 1) >= self.swa_start:\n            self.update_average_model()\n            self.swa_n += 1\n\n        self.epoch += 1\n\n    def update_average_model(self):\n        alpha = 1. / (self.swa_n + 1)\n        for layer, swa_layer in zip(self.model.layers, self.swa_model.layers):\n            weights = []\n            for w1, w2 in zip(swa_layer.get_weights(), layer.get_weights()):\n                weights.append((1 - alpha) * w1 + alpha * w2)\n            swa_layer.set_weights(weights)\n\n    def on_train_end(self, logs=None):\n        print('Logging Info - Saving SWA model checkpoint: %s_swa.hdf5\\n' % self.model_name)\n        self.swa_model.save_weights(os.path.join(self.checkpoint_dir, '{}_swa.hdf5'.format(self.model_name)))\n        print('Logging Info - SWA model Saved')\n\n\n# 周期学习率callback\nclass CyclicLR(Callback):\n    def __init__(\n            self,\n            base_lr=0.001,\n            max_lr=0.006,\n            step_size=2000.,\n            mode='triangular',\n            gamma=1.,\n            scale_fn=None,\n            scale_mode='cycle'):\n        super(CyclicLR, self).__init__()\n\n        if mode not in ['triangular', 'triangular2',\n                        'exp_range']:\n            raise KeyError(\"mode must be one of 'triangular', \"\n                           \"'triangular2', or 'exp_range'\")\n        self.base_lr = base_lr\n        self.max_lr = max_lr\n        self.step_size = step_size\n        self.mode = mode\n        self.gamma = gamma\n        if scale_fn is None:\n            if self.mode == 'triangular':\n                self.scale_fn = lambda x: 1.\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = lambda x: 1 / (2. ** (x - 1))\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = lambda x: gamma ** x\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n        self.clr_iterations = 0.\n        self.trn_iterations = 0.\n        self.history = {}\n\n        self._reset()\n\n    def _reset(self, new_base_lr=None, new_max_lr=None,\n               new_step_size=None):\n        \"\"\"Resets cycle iterations.\n        Optional boundary/step size adjustment.\n        \"\"\"\n        if new_base_lr is not None:\n            self.base_lr = new_base_lr\n        if new_max_lr is not None:\n            self.max_lr = new_max_lr\n        if new_step_size is not None:\n            self.step_size = new_step_size\n        self.clr_iterations = 0.\n\n    def clr(self):\n        cycle = np.floor(1 + self.clr_iterations / (2 * self.step_size))\n        x = np.abs(self.clr_iterations / self.step_size - 2 * cycle + 1)\n        if self.scale_mode == 'cycle':\n            return self.base_lr + (self.max_lr - self.base_lr) * \\\n                   np.maximum(0, (1 - x)) * self.scale_fn(cycle)\n        else:\n            return self.base_lr + (self.max_lr - self.base_lr) * \\\n                   np.maximum(0, (1 - x)) * self.scale_fn(self.clr_iterations)\n\n    def on_train_begin(self, logs={}):\n        logs = logs or {}\n\n        if self.clr_iterations == 0:\n            K.set_value(self.model.optimizer.lr, self.base_lr)\n        else:\n            K.set_value(self.model.optimizer.lr, self.clr())\n\n    def on_batch_end(self, epoch, logs=None):\n\n        logs = logs or {}\n        self.trn_iterations += 1\n        self.clr_iterations += 1\n        K.set_value(self.model.optimizer.lr, self.clr())\n\n        self.history.setdefault(\n            'lr', []).append(\n            K.get_value(\n                self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.trn_iterations)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n\n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        logs['lr'] = K.get_value(self.model.optimizer.lr)\n\n# 通过转入列表获得所有手工特征的input\ndef get_dense_feature_inputs(dense_features_: list):\n    dense_input = OrderedDict()\n    for feature in dense_features_:\n        dense_input[feature] = Input(shape=(1,), name=feature + '_input')\n    return dense_input\n\n# 通过传入input列表获得所有的fc的输出\ndef get_dense_feature_fc_list(dense_input_: OrderedDict, fc_dim=8, use_bias=False, l2_reg=1e-5):\n    dense_input = list(dense_input_.values())\n    fc_out_list = list(map(Dense(fc_dim, use_bias=use_bias, kernel_regularizer=l2(l2_reg)), dense_input))\n    return fc_out_list\n\n# 构建模型\ndef build_model(lstm_dim=64, emb_mat=None):\n    print('Build model...')\n\n    query_input = Input(shape=(max_seq_len,))\n    title_input = Input(shape=(max_seq_len,))\n\n    embedding = Embedding(emb_mat.shape[0], W2V_DIM, weights=[emb_mat], trainable=False, mask_zero=True)\n\n    query_emb = embedding(query_input)\n    query_emb = Dropout(0.2)(query_emb)\n\n    title_emb = embedding(title_input)\n    title_emb = Dropout(0.2)(title_emb)\n\n    bilstm_1 = LSTM(units=lstm_dim, return_sequences=True)\n\n    query_hidden = bilstm_1(query_emb)\n    title_hidden = bilstm_1(title_emb)\n\n    query_attend, title_attend = DotProductAttention()([query_hidden, title_hidden])\n\n    query_enhance = concatenate([query_hidden, query_attend, subtract([query_hidden, query_attend]),\n                                 multiply([query_hidden, query_attend])])  # [?,25,256]\n\n    title_enhance = concatenate([title_hidden, title_attend,\n                                 subtract([title_hidden, title_attend]),\n                                 multiply([title_hidden, title_attend])])  # [?,25,256]\n                                 \n    feed_forward = TimeDistributed(Dense(units=lstm_dim, activation='relu'))\n\n    bilstm_2 = LSTM(units=lstm_dim, return_sequences=True)\n\n    query_compose = bilstm_2(feed_forward(query_enhance))  # [?,25,32]\n    title_compose = bilstm_2(feed_forward(title_enhance))\n\n    global_max_pooling = Lambda(lambda x: K.max(x, axis=1))\n    query_avg = GlobalAveragePooling1D()(query_compose)\n    query_max = global_max_pooling(query_compose)\n    title_avg = GlobalAveragePooling1D()(title_compose)\n    title_max = global_max_pooling(title_compose)\n\n    inference_compose = concatenate([query_avg, query_max, title_avg, title_max])\n\n    dense = Dense(units=lstm_dim, activation='tanh')(inference_compose)\n\n    output = Dense(1, activation='sigmoid')(dense)\n    model = Model(inputs=[query_input, title_input], outputs=output)\n    return model\n\n\ndef auc(y_true, y_pred):\n    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)\n\n\nfrom joblib import Parallel, delayed\nfrom sklearn.metrics import roc_auc_score\n\n\ndef cal_AUC(labels, prob):\n    try:\n        return roc_auc_score(labels, prob)\n    except:\n        return 0.5\n\n\n# 计算各个组的qauc值\ndef sum_AUC(mycombinedata):\n    grouplist = mycombinedata[0]\n    y_true = mycombinedata[1]\n    y_pred = mycombinedata[2]\n\n    if len(y_true) != sum(grouplist):\n        print(\"评分函数中len(y_true)!=sum(group)\")\n        return\n    start = 0\n    sum_AUC = 0\n    for group in grouplist:\n        roc_auc = cal_AUC(y_true[start:start + group], y_pred[start:start + group])\n        start = start + group\n        sum_AUC = sum_AUC + roc_auc\n    return sum_AUC\n\n# 并行计算QAUC的值\ndef QAUC_parallel(y_true, y_pred, group):\n    groupnum = 4\n    import math\n    group_len = math.ceil(len(group) / groupnum)\n    groups = [group[i * group_len:(i + 1) * group_len] for i in range(groupnum)]\n    mycombines = []\n\n    start = 0\n    for agroup in groups:\n        mycombinedata = []\n        mycombinedata.append(agroup)\n        mycombinedata.append(y_true[start:start + sum(agroup)])\n        mycombinedata.append(y_pred[start:start + sum(agroup)])\n        start = start + sum(agroup)\n        mycombines.append(mycombinedata)\n\n    sum_AUC_ = Parallel(n_jobs=groupnum)(delayed(sum_AUC)(mycombinedata) for mycombinedata in mycombines)\n\n    return sum(sum_AUC_) / len(group)\n\n\nclass Evaluation(Callback):\n    def __init__(self, validation_data=(), interval=1):\n        super(Callback, self).__init__()\n        self.interval = interval\n        self.x_val, self.val_group, self.y_val = validation_data\n        self.best_score = 0.\n        self.best_epoch = 0\n        self.best_auc = 0.0\n        self.best_auc_epoch = 0\n        self.auc_list = []\n        self.q_auc_list = []\n\n    def on_epoch_end(self, epoch, log={}):\n        if epoch % self.interval == 0:\n            y_pred = self.model.predict(self.x_val, verbose=0, batch_size=batch_size)\n            auc_score = roc_auc_score(self.y_val, y_pred)\n            self.auc_list.append(auc_score)\n            if auc_score > self.best_auc:\n                self.best_auc = auc_score\n                self.best_auc_epoch = epoch + 1\n\n            score_parallel = QAUC_parallel(self.y_val, y_pred, self.val_group)\n            self.q_auc_list.append(score_parallel)\n            if score_parallel > self.best_score:\n                self.best_score = score_parallel\n                self.best_epoch = epoch + 1\n\n            print('\\n ROC_AUC - epoch:%d - score:%.6f' % (epoch + 1, auc_score))\n            print('\\n Q_AUC - epoch:%d - score:%.6f' % (epoch + 1, score_parallel))\n\n    def get_best_score_epoch(self):\n        return self.best_score, self.best_epoch\n\n    def get_best_auc_score_epoch(self):\n        return self.best_auc, self.best_auc_epoch\n\n    def show_result_list(self):\n        print('auc', self.auc_list)\n        print('\\n')\n        print('qauc', self.q_auc_list)\n\n\ndef seq_padding(X, max_len=20):\n    return [x + [PAD] * (max_len - len(x)) if len(x) < max_len else x[:max_len] for x in X]\n\n\nclass DataGenerator(keras.utils.Sequence):\n\n    def __init__(self, word2id, text_data, batch_size=1024 * 5):\n        self.batch_size = batch_size\n        self.word2id = word2id\n\n        self.text_data = text_data\n\n    def __len__(self):\n        # 计算每一个epoch的迭代次数\n        return math.ceil(len(self.text_data) / float(self.batch_size))\n\n    def __getitem__(self, index):\n        start = index * self.batch_size\n        stop = (index + 1) * self.batch_size\n\n        batch_text_df = self.text_data.iloc[start:stop]\n        y = batch_text_df['label'].values\n\n        Q = []\n        D = []\n        for query in batch_text_df['query']:\n            query = query.split()\n            Q.append([word2id.get(w, UNK) for w in query])\n\n        for title in batch_text_df['title']:\n            title = title.split()\n            D.append([word2id.get(w, UNK) for w in title])\n\n        Q_pad = seq_padding(Q, max_seq_len)\n        D_pad = seq_padding(D, max_seq_len)\n        return [np.array(Q_pad), np.array(D_pad)], y\n\n\nflag = 'train'\nbatch_size = 1024 * 5\n\nPAD = 0\nUNK = 1\n\nif __name__ == \"__main__\":\n    if flag == 'train':\n        from keras.models import load_model\n\n        print('train')\n        train_size = 98000000\n        # 读取词向量矩阵\n        emb_mat = np.load('/home/kesci/work/sunrui/NN_second_2e/word2vec_fasttext_6kw_nn_sim.npy')\n        \n        # 读取文本数据\n        text_data = pd.read_csv('/home/kesci/input/bytedance/train_final.csv', usecols=[0, 1, 3, 4],\n                                names=['query_id', 'query', 'title', 'label'], skiprows=range(0, 100000000),\n                                nrows=100000000)\n        print(f'shape ={text_data.shape}')\n        # 读取 lgb feature 完毕\n        \n        # 读取词表\n        with open('/home/kesci/work/sunrui/NN_second_2e/second_6kw_nn_sim.pkl', 'rb') as f:\n            word2id = pickle.load(f)\n\n        val_text_data = text_data[train_size:]\n        train_text_data = text_data[:train_size]\n\n        Q_val = []\n        D_val = []\n        for query in val_text_data['query']:\n            query = query.split()\n            Q_val.append([word2id.get(w, UNK) for w in query])  # 没有命中就返回UNK\n\n        for title in val_text_data['title']:\n            title = title.split()\n            D_val.append([word2id.get(w, UNK) for w in title])\n\n        val_query_input = seq_padding(Q_val, max_seq_len)\n        val_title_input = seq_padding(D_val, max_seq_len)\n\n        Y_val = val_text_data['label'].values\n\n        train_generator = DataGenerator(word2id=word2id, text_data=train_text_data, batch_size=batch_size)\n\n        val_text_data['query_id_nums'] = val_text_data.groupby(['query_id'])['label'].transform('count')\n\n        val_group_df = val_text_data[['query_id', 'query_id_nums']].drop_duplicates()\n        val_group = val_group_df.query_id_nums.get_values()\n\n        swa = SWA(checkpoint_dir='./sunrui/swa/', model_name='swa.model')\n\n        clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                       step_size=2 * train_size // batch_size, mode='triangular')\n\n        model = build_model(64, emb_mat)\n        print('load end')\n        filepath = \"./sunrui/nn/weights-{epoch:02d}_esim_64_nn_sim.hdf5\"\n        checkpoint = ModelCheckpoint(filepath, verbose=1)\n\n        early_stopping = EarlyStopping(monitor='val_auc', patience=5, verbose=1, mode='max')\n\n        val_model_input = [val_query_input, val_title_input]\n\n        eval_callback = Evaluation(\n            validation_data=(\n                val_model_input, val_group, Y_val))\n\n        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[auc])\n        model.summary()\n        model.fit_generator(train_generator, epochs=epochs,\n                            validation_data=(val_model_input, Y_val),\n                            callbacks=[early_stopping, eval_callback, swa, clr, checkpoint], shuffle=True, workers=8,\n                            use_multiprocessing=True)\n    else:\n        from keras.models import load_model\n\n        \"\"\"\n            在这里修改最好epoch的轮次模型名称和提交的结果文件名\n        \"\"\"\n        model_file_name = './sunrui/nn/weights-04_esim_64_single_pure_text.hdf5'\n        result_file_name = './sunrui/esim_64_single_pure_textesim_64_single_pure_text_epoch4_second_20kw.csv'\n\n        model = load_model(model_file_name, custom_objects={\n            'DotProductAttention': DotProductAttention,\n            'auc': auc\n        })\n\n        test = pd.read_csv('/home/kesci/input/bytedance/test_final_part1.csv', header=None)\n        test.columns = ['query_id', 'query', 'query_title_id', 'title']\n\n        with open('./sunrui/words2id_all.pkl', 'rb') as f:\n            word2id = pickle.load(f)\n\n        Q_val = []\n        D_val = []\n        for query in test['query']:\n            query = query.split()\n            Q_val.append([word2id[w] for w in query])  # 没有命中就返回UNK\n\n        for title in test['title']:\n            title = title.split()\n            D_val.append([word2id[w] for w in title])\n\n        test_query_input = seq_padding(Q_val, max_seq_len)\n        test_title_input = seq_padding(D_val, max_seq_len)\n\n        import pandas as pd\n\n        preds = model.predict([test_query_input, test_title_input], batch_size=batch_size)\n\n        preds_np = np.squeeze(preds)\n\n        test = pd.read_csv('/home/kesci/input/bytedance/test_final_part1.csv', header=None)\n        test.columns = ['query_id', 'query', 'query_title_id', 'title']\n\n        test = test.drop(['query', 'title'], axis=1)\n\n        test_y = {'prediction': preds_np}\n        test_y = pd.DataFrame(test_y)\n\n        test_result = pd.concat([test, test_y], axis=1)\n\n        test_result.to_csv(result_file_name, index=None, header=None)","execution_count":null},{"metadata":{"id":"98678908906548AF85493F47D61F2B98"},"cell_type":"code","outputs":[],"source":"","execution_count":null},{"metadata":{"id":"04FC2A3BF13F4C31800A90592357EE91"},"cell_type":"code","outputs":[],"source":"# 以下是带有特征的esim网络的训练 前1e数据的 隐层个数为128 我们选择了前2轮的模型用于模型融合\n# 下面几个ceil都是相同的代码 只不过读取的是不同的数据 ","execution_count":null},{"metadata":{"id":"ECDAF1F1560440C38AAD7F088CF4F665","hide_input":true},"cell_type":"code","outputs":[],"source":"import math\nimport os\nimport pickle\nfrom collections import OrderedDict\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom keras import Input, Model\nfrom keras.backend.tensorflow_backend import set_session\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import Embedding, Dense, Dropout, Lambda, concatenate, GlobalAveragePooling1D, subtract, multiply, \\\n    TimeDistributed, LSTM\nfrom keras.regularizers import l2\nfrom tensorflow import set_random_seed\n\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nsess = tf.Session(config=config)\nset_session(sess)\n\nseed = 2019\n\nset_random_seed(seed)  # Tensorflow\nnp.random.seed(seed)  # NumPy\n\nfrom keras.callbacks import Callback, ModelCheckpoint\nimport keras\nimport keras.backend as K\nfrom keras.engine import Layer\n\nimport numpy as np\n\nimport logging\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(level=logging.INFO)\nhandler = logging.FileHandler(\"./sunrui/gated_esim_64_pre_1e_64_log.txt\")  # 训练前1e的数据\nhandler.setLevel(logging.INFO)\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nhandler.setFormatter(formatter)\n\nconsole = logging.StreamHandler()\nconsole.setLevel(logging.INFO)\n\nlogger.addHandler(handler)\nlogger.addHandler(console)\n\n\nclass DotProductAttention(Layer):\n    def __init__(self, return_attend_weight=False, keep_mask=True, **kwargs):\n        self.return_attend_weight = return_attend_weight\n        self.keep_mask = keep_mask\n        self.supports_masking = True\n        super(DotProductAttention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert isinstance(input_shape, list)\n        input_shape_a, input_shape_b = input_shape\n\n        if len(input_shape_a) != 3 or len(input_shape_b) != 3:\n            raise ValueError('Inputs into DotProductAttention should be 3D tensors')\n\n        if input_shape_a[-1] != input_shape_b[-1]:\n            raise ValueError('Inputs into DotProductAttention should have the same dimensionality at the last axis')\n\n    def call(self, inputs, mask=None):\n        assert isinstance(inputs, list)\n        inputs_a, inputs_b = inputs\n\n        if mask is not None:\n            mask_a, mask_b = mask\n        else:\n            mask_a, mask_b = None, None\n\n        e = K.exp(K.batch_dot(inputs_a, inputs_b, axes=2))  # similarity between a & b\n\n        if mask_a is not None:\n            e *= K.expand_dims(K.cast(mask_a, K.floatx()), 2)\n        if mask_b is not None:\n            e *= K.expand_dims(K.cast(mask_b, K.floatx()), 1)\n\n        e_b = e / K.cast(K.sum(e, axis=2, keepdims=True) + K.epsilon(), K.floatx())  # attention weight over b\n        e_a = e / K.cast(K.sum(e, axis=1, keepdims=True) + K.epsilon(), K.floatx())  # attention weight over a\n\n        if self.return_attend_weight:\n            return [e_b, e_a]\n\n        a_attend = K.batch_dot(e_b, inputs_b, axes=(2, 1))  # a attend to b\n        b_attend = K.batch_dot(e_a, inputs_a, axes=(1, 1))  # b attend to a\n        return [a_attend, b_attend]\n\n    def compute_mask(self, inputs, mask=None):\n        if self.keep_mask:\n            return mask\n        else:\n            return [None, None]\n\n    def compute_output_shape(self, input_shape):\n        if self.return_attend_weight:\n            input_shape_a, input_shape_b = input_shape\n            return [(input_shape_a[0], input_shape_a[1], input_shape_b[1]),\n                    (input_shape_a[0], input_shape_a[1], input_shape_b[1])]\n        return input_shape\n\n\nclass SWA(Callback):\n    def __init__(self, checkpoint_dir, model_name, swa_start=1):\n        super(SWA, self).__init__()\n        self.checkpoint_dir = checkpoint_dir\n        self.model_name = model_name\n        self.swa_start = swa_start\n        self.swa_model = None  # the model that we will use to store the average of the weights once SWA begins\n\n    def on_train_begin(self, logs=None):\n        self.epoch = 0\n        self.swa_n = 0\n        # self.swa_model = copy.deepcopy(self.model)  # make a copy of the model we're training\n        # Note: I found deep copy of a model with customized layer would give errors\n        self.swa_model = keras.models.clone_model(self.model)\n        self.swa_model.set_weights(self.model.get_weights())  # see: https://github.com/keras-team/keras/issues/1765\n\n    def on_epoch_end(self, epoch, logs=None):\n        if (self.epoch + 1) >= self.swa_start:\n            self.update_average_model()\n            self.swa_n += 1\n\n        self.epoch += 1\n\n    def update_average_model(self):\n        # update running average of parameters\n        alpha = 1. / (self.swa_n + 1)\n        for layer, swa_layer in zip(self.model.layers, self.swa_model.layers):\n            weights = []\n            for w1, w2 in zip(swa_layer.get_weights(), layer.get_weights()):\n                weights.append((1 - alpha) * w1 + alpha * w2)\n            swa_layer.set_weights(weights)\n\n    def on_train_end(self, logs=None):\n        print('Logging Info - Saving SWA model checkpoint: %s_swa.hdf5\\n' % self.model_name)\n        self.swa_model.save_weights(os.path.join(self.checkpoint_dir, '{}_swa.hdf5'.format(self.model_name)))\n        print('Logging Info - SWA model Saved')\n\n\nclass CyclicLR(Callback):\n    def __init__(\n            self,\n            base_lr=0.001,\n            max_lr=0.006,\n            step_size=2000.,\n            mode='triangular',\n            gamma=1.,\n            scale_fn=None,\n            scale_mode='cycle'):\n        super(CyclicLR, self).__init__()\n\n        if mode not in ['triangular', 'triangular2',\n                        'exp_range']:\n            raise KeyError(\"mode must be one of 'triangular', \"\n                           \"'triangular2', or 'exp_range'\")\n        self.base_lr = base_lr\n        self.max_lr = max_lr\n        self.step_size = step_size\n        self.mode = mode\n        self.gamma = gamma\n        if scale_fn is None:\n            if self.mode == 'triangular':\n                self.scale_fn = lambda x: 1.\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = lambda x: 1 / (2. ** (x - 1))\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = lambda x: gamma ** x\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n        self.clr_iterations = 0.\n        self.trn_iterations = 0.\n        self.history = {}\n\n        self._reset()\n\n    def _reset(self, new_base_lr=None, new_max_lr=None,\n               new_step_size=None):\n        \"\"\"Resets cycle iterations.\n        Optional boundary/step size adjustment.\n        \"\"\"\n        if new_base_lr is not None:\n            self.base_lr = new_base_lr\n        if new_max_lr is not None:\n            self.max_lr = new_max_lr\n        if new_step_size is not None:\n            self.step_size = new_step_size\n        self.clr_iterations = 0.\n\n    def clr(self):\n        cycle = np.floor(1 + self.clr_iterations / (2 * self.step_size))\n        x = np.abs(self.clr_iterations / self.step_size - 2 * cycle + 1)\n        if self.scale_mode == 'cycle':\n            return self.base_lr + (self.max_lr - self.base_lr) * \\\n                   np.maximum(0, (1 - x)) * self.scale_fn(cycle)\n        else:\n            return self.base_lr + (self.max_lr - self.base_lr) * \\\n                   np.maximum(0, (1 - x)) * self.scale_fn(self.clr_iterations)\n\n    def on_train_begin(self, logs={}):\n        logs = logs or {}\n\n        if self.clr_iterations == 0:\n            K.set_value(self.model.optimizer.lr, self.base_lr)\n        else:\n            K.set_value(self.model.optimizer.lr, self.clr())\n\n    def on_batch_end(self, epoch, logs=None):\n\n        logs = logs or {}\n        self.trn_iterations += 1\n        self.clr_iterations += 1\n        K.set_value(self.model.optimizer.lr, self.clr())\n\n        self.history.setdefault(\n            'lr', []).append(\n            K.get_value(\n                self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.trn_iterations)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n\n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        logs['lr'] = K.get_value(self.model.optimizer.lr)\n\n\ndef get_dense_feature_inputs(dense_features_: list):\n    dense_input = OrderedDict()\n    for feature in dense_features_:\n        dense_input[feature] = Input(shape=(1,), name=feature + '_input')\n    return dense_input\n\n\ndef get_dense_feature_fc_list(dense_input_: OrderedDict, fc_dim=8, use_bias=True, l2_reg=1e-4):\n    dense_input = list(dense_input_.values())\n    fc_out_list = list(map(Dense(fc_dim, use_bias=use_bias, kernel_regularizer=l2(l2_reg)), dense_input))\n    return fc_out_list\n\n\ndef build_model(lstm_dim=64, emb_mat=None):\n    print('Build model...')\n\n    query_input = Input(shape=(max_seq_len,))\n    title_input = Input(shape=(max_seq_len,))\n\n    embedding = Embedding(emb_mat.shape[0], W2V_DIM, weights=[emb_mat], trainable=False, mask_zero=True)\n\n    query_emb = embedding(query_input)\n    query_emb = Dropout(0.2)(query_emb)\n\n    title_emb = embedding(title_input)\n    title_emb = Dropout(0.2)(title_emb)\n\n    bilstm_1 = LSTM(units=lstm_dim, return_sequences=True)\n\n    query_hidden = bilstm_1(query_emb)\n    title_hidden = bilstm_1(title_emb)\n\n    query_attend, title_attend = DotProductAttention()([query_hidden, title_hidden])\n\n    query_enhance = concatenate([query_hidden, query_attend, subtract([query_hidden, query_attend]),\n                                 multiply([query_hidden, query_attend])])  # [?,25,256]\n\n    title_enhance = concatenate([title_hidden, title_attend,\n                                 subtract([title_hidden, title_attend]),\n                                 multiply([title_hidden, title_attend])])  # [?,25,256]\n\n    # inference composition\n    feed_forward = TimeDistributed(Dense(units=lstm_dim, activation='relu'))\n\n    bilstm_2 = LSTM(units=lstm_dim, return_sequences=True)\n\n    query_compose = bilstm_2(feed_forward(query_enhance))  # [?,25,32]\n    title_compose = bilstm_2(feed_forward(title_enhance))\n\n    global_max_pooling = Lambda(lambda x: K.max(x, axis=1))  # GlobalMaxPooling1D didn't support masking\n    query_avg = GlobalAveragePooling1D()(query_compose)\n    query_max = global_max_pooling(query_compose)\n    title_avg = GlobalAveragePooling1D()(title_compose)\n    title_max = global_max_pooling(title_compose)\n\n    lgb_dense_feature_input = get_dense_feature_inputs(used_lgb_dense_feature)\n\n    dense_fc_list = get_dense_feature_fc_list(lgb_dense_feature_input)\n\n    if len(dense_fc_list) > 1:\n        dense_feature_concat = concatenate(dense_fc_list)\n    else:\n        dense_feature_concat = dense_fc_list[0]\n\n    inference_compose = concatenate([query_avg, query_max, title_avg, title_max])\n\n    # inference_compose = BatchNormalization()(inference_compose)  # 尝试\n\n    dense_esim = Dense(units=lstm_dim)(inference_compose)\n\n    dense_feature_gate = Dense(lstm_dim, activation='sigmoid')(dense_feature_concat)\n\n    gated_esim = Lambda(lambda x: x[0] * x[1])([dense_esim, dense_feature_gate])\n\n    gated_esim = Dense(lstm_dim, activation='elu')(gated_esim)\n    model_dense_input = [lgb_dense_feature_input[feat] for feat in used_lgb_dense_feature]\n\n    # gated_esim = BatchNormalization()(gated_esim)  # 加了BN 训练不稳定 val loss会跳 但是性能尚可\n    # gated_esim = Dropout(0.1)(gated_esim)\n\n    output = Dense(1, activation='sigmoid')(gated_esim)\n    model = Model(inputs=[query_input, title_input] + model_dense_input, outputs=output)\n    return model\n\n\ndef auc(y_true, y_pred):\n    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)\n\n\nfrom joblib import Parallel, delayed\nfrom sklearn.metrics import roc_auc_score\n\n\ndef cal_AUC(labels, prob):\n    try:\n        return roc_auc_score(labels, prob)\n    except:\n        return 0.5\n\n\n# 计算各个组的qauc值\ndef sum_AUC(mycombinedata):\n    grouplist = mycombinedata[0]\n    y_true = mycombinedata[1]\n    y_pred = mycombinedata[2]\n\n    if len(y_true) != sum(grouplist):\n        print(\"评分函数中len(y_true)!=sum(group)\")\n        return\n    start = 0\n    sum_AUC = 0\n    for group in grouplist:\n        roc_auc = cal_AUC(y_true[start:start + group], y_pred[start:start + group])\n        start = start + group\n        sum_AUC = sum_AUC + roc_auc\n    return sum_AUC\n\n\ndef QAUC_parallel(y_true, y_pred, group):\n    groupnum = 4\n    import math\n    group_len = math.ceil(len(group) / groupnum)\n    groups = [group[i * group_len:(i + 1) * group_len] for i in range(groupnum)]\n    mycombines = []\n\n    start = 0\n    for agroup in groups:\n        mycombinedata = []\n        mycombinedata.append(agroup)\n        mycombinedata.append(y_true[start:start + sum(agroup)])\n        mycombinedata.append(y_pred[start:start + sum(agroup)])\n        start = start + sum(agroup)\n        mycombines.append(mycombinedata)\n\n    sum_AUC_ = Parallel(n_jobs=groupnum)(delayed(sum_AUC)(mycombinedata) for mycombinedata in mycombines)\n\n    return sum(sum_AUC_) / len(group)\n\n\nclass Evaluation(Callback):\n    def __init__(self, validation_data=(), interval=1):\n        super(Callback, self).__init__()\n        self.interval = interval\n        self.x_val, self.val_group, self.y_val = validation_data\n        self.best_score = 0.\n        self.best_epoch = 0\n        self.best_auc = 0.0\n        self.best_auc_epoch = 0\n        self.auc_list = []\n        self.q_auc_list = []\n\n    def on_epoch_end(self, epoch, log={}):\n        if epoch % self.interval == 0:\n            y_pred = self.model.predict(self.x_val, verbose=0, batch_size=batch_size)\n            auc_score = roc_auc_score(self.y_val, y_pred)\n            self.auc_list.append(auc_score)\n            if auc_score > self.best_auc:\n                self.best_auc = auc_score\n                self.best_auc_epoch = epoch + 1\n\n            score_parallel = QAUC_parallel(self.y_val, y_pred, self.val_group)\n            self.q_auc_list.append(score_parallel)\n            if score_parallel > self.best_score:\n                self.best_score = score_parallel\n                self.best_epoch = epoch + 1\n            logger.info(f'Q_AUC = {score_parallel} epoch = {epoch + 1}')\n\n            print('\\n ROC_AUC - epoch:%d - score:%.6f' % (epoch + 1, auc_score))\n            print('\\n Q_AUC - epoch:%d - score:%.6f' % (epoch + 1, score_parallel))\n\n    def get_best_score_epoch(self):\n        return self.best_score, self.best_epoch\n\n    def get_best_auc_score_epoch(self):\n        return self.best_auc, self.best_auc_epoch\n\n    def show_result_list(self):\n        print('auc', self.auc_list)\n        print('\\n')\n        print('qauc', self.q_auc_list)\n\n\ndef seq_padding(X, max_len):\n    return [x + [PAD] * (max_len - len(x)) if len(x) < max_len else x[:max_len] for x in X]\n\n\nclass DataGenerator(keras.utils.Sequence):\n\n    def __init__(self, word2id, text_data, lgb_data, batch_size=1024 * 5):\n        self.batch_size = batch_size\n        self.word2id = word2id\n\n        self.text_data = text_data\n        self.lgb_data = lgb_data\n\n    def __len__(self):\n        # 计算每一个epoch的迭代次数\n        return math.ceil(len(self.text_data) / float(self.batch_size))\n\n    def __getitem__(self, index):\n        start = index * self.batch_size\n        stop = (index + 1) * self.batch_size\n\n        batch_lgb_df = self.lgb_data.iloc[start:stop]\n        batch_text_df = self.text_data.iloc[start:stop]\n        y = batch_text_df['label'].values\n\n        train_lgb_input = [batch_lgb_df[feat].values for feat in used_lgb_dense_feature]\n\n        Q = []\n        D = []\n        for query in batch_text_df['query']:\n            query = query.split()\n            Q.append([word2id[w] for w in query])\n\n        for title in batch_text_df['title']:\n            title = title.split()\n            D.append([word2id[w] for w in title])\n\n        Q_pad = seq_padding(Q, max_seq_len)\n        D_pad = seq_padding(D, max_seq_len)\n        return [np.array(Q_pad), np.array(D_pad)] + train_lgb_input, y\n\n\nflag = 'train'\nbatch_size = 1024 * 5\n\nPAD = 0\nUNK = 1\n\nW2V_DIM = 200\n\nmax_seq_len = 25\nepochs = 20\n\n\ndef get_used_feature_names(featurecol_h5):\n    features = []\n    for k, v in featurecol_h5.items():\n        features.extend(v)\n    return features\n\n\nfeaturecol_h5 = {\n    'sim_feat': ['jaccard_q3_t3',\n                 'jaccard_q3_t5',\n                 'jaccard_q5_t5', 'levenshtein_q5_t5',\n                 'jaccard_q5_t10', 'levenshtein_q5_t10',\n                 'jaccard_q10_t10', 'levenshtein_q10_t10',\n                 'jaccard_q15_t25', 'levenshtein_q15_t25',\n                 'jaccard', 'levenshtein'],\n\n    'len_feat': [\"querykw_num\", \"titlekw_num\"],\n\n    \"title_nunique_query\": [\"title_nunique_query\"],\n    \"query_nunique_title\": [\"query_nunique_title\"],\n\n    'title_score_count_feat': [\"title_score_count\", \"title_score_click_num\"],\n    'title_code_score_feat': [\"title_code_score\"],\n    'title_convert_feat': [\"title_code_convert\", 'title_code_label_count'],\n\n    'query_count': [\"query_code_count\"],\n    'title_count': [\"title_count\"],\n\n    \"match_feat\": ['count_match', 'blockcount_match', 'proximity', 'maxMatchBlockLen',\n                   'q1_match_start', 'q1_match_end'],\n\n    \"BM25\": [\"BM25\"],\n}\n\nothercols = [\"titlekw_querykw_diff\", \"titlekw_querykw_rate\"]\n\n\ndef reduce_mem_usage(D, verbose=True):\n    start_mem = D.memory_usage().sum() / 1024 ** 2\n    for c, d in zip(D.columns, D.dtypes):\n        if d.kind == 'f':\n            D[c] = pd.to_numeric(D[c], downcast='float')\n        elif d.kind == 'i':\n            D[c] = pd.to_numeric(D[c], downcast='signed')\n    end_mem = D.memory_usage().sum() / 1024 ** 2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (\n            start_mem - end_mem) / start_mem))\n    return D\n\n\ndef ReadData(datatype='train', nrows=1000000000):\n    if datatype == 'train':\n        id_feature = '/home/kesci/input/bytedance/train_final.csv'\n        usecols = [0, 4]\n        names = ['query_id', 'label']\n\n        print(\"read \", id_feature)\n        DataSet = pd.read_csv(id_feature,\n                              header=None,\n                              nrows=nrows,\n                              usecols=usecols,\n                              names=names\n                              )\n        path_h5 = \"/home/kesci/work/pre_3billion_data/train/\"\n    elif datatype == 'test1':\n        id_feature = '/home/kesci/input/bytedance/test_final_part1.csv'\n        usecols = [0, 2]\n        names = ['query_id', 'query_title_id']\n        path_h5 = \"/home/kesci/work/post_4kw_data/test1/\"\n        print(\"read \", id_feature)\n        DataSet = pd.read_csv(id_feature,\n                              header=None,\n                              nrows=nrows,\n                              usecols=usecols,\n                              names=names\n                              )\n    elif datatype == 'test2':\n        id_feature = '/home/kesci/input/bytedance/bytedance_contest.final_2.csv'\n        usecols = [0, 2]\n        names = ['query_id', 'query_title_id']\n        path_h5 = \"/home/kesci/work/post_4kw_data/test2/\"\n        print(\"read \", id_feature)\n        DataSet = pd.read_csv(id_feature,\n                              header=None,\n                              nrows=nrows,\n                              usecols=usecols,\n                              names=names\n                              )\n\n    print(\"length:\", DataSet.__len__())\n    DataSet = reduce_mem_usage(DataSet, verbose=True)\n\n    featuremap_h5 = {\n        'cross_feat': path_h5 + f'cross_{datatype}_feat.h5',\n\n        'query_pos_feat': path_h5 + f'query_pos_{datatype}_feat.h5',\n        'title_pos_feat': path_h5 + f'title_pos_{datatype}_feat.h5',\n\n        'match_feat': path_h5 + f'query_match_{datatype}_feat.h5',\n        'editDistance_feat': path_h5 + f'editDistance_{datatype}_feat.h5',\n\n        'sim_feat': path_h5 + f'sim_{datatype}_feat.h5',\n        'tag_score_feat': path_h5 + f'tag_score_10foldtime_{datatype}_feat.h5',\n        'title_score_count_feat': path_h5 + f'title_score_count_{datatype}_feat.h5',\n        'title_code_score_feat': path_h5 + f'title_code_score_10foldtime_{datatype}_feat.h5',\n        'title_convert_feat': path_h5 + f'title_convert_{datatype}.h5',\n        'sif_feat': path_h5 + f'sif_{datatype}_post_4kw.h5',\n        'len_feat': path_h5 + f'len_{datatype}_feat.h5',\n\n        'title_count': path_h5 + f'count_feature_{datatype}.h5',\n        'query_count': path_h5 + f'query_count_all_{datatype}.h5',\n\n        \"title_nunique_query\": path_h5 + f'nunique_feature_{datatype}.h5',\n        \"query_nunique_title\": path_h5 + f'query_nunique_title_all_{datatype}.h5',\n\n        'tag': path_h5 + f'tag_{datatype}.h5',\n        \"tag_convert_feat\": path_h5 + f\"tag_convert_{datatype}.h5\",\n        \"query_convert\": path_h5 + f\"query_convert_{datatype}.h5\",\n\n        \"M_cosine\": path_h5 + f\"M_sim_{datatype}_feat.h5\",\n        \"M_tfidf_cosine\": path_h5 + f\"M_tfidf_sim_{datatype}_feat.h5\",\n        \"BM25\": path_h5 + f'BM25_{datatype}_feat.h5',\n        'NN_SIM': path_h5 + f'nn_sim_feature.h5',\n\n        'editdistance_relativepos': path_h5 + f'editdistance_relativepos_{datatype}_feat.h5',\n        'fuzz': path_h5 + f\"fuzz_{datatype}_feat.h5\",\n        'textpair': path_h5 + f\"textpair_{datatype}_feat.h5\",\n\n        'sen_dis': path_h5 + f\"sen_dis_{datatype}_200.h5\",\n        'sen_dis2': path_h5 + f\"sen_dis2_{datatype}_200.h5\",\n    }\n\n    for featurefile in featurecol_h5:\n        print(\"read \", featuremap_h5[featurefile])\n        feature_set = pd.read_hdf(featuremap_h5[featurefile],\n                                  key='data',\n                                  start=0,\n                                  stop=nrows)[featurecol_h5[featurefile]].reset_index(drop=True)\n        print(\"length:\", feature_set.__len__())\n        # print(feature_set.head(1))\n        # feature_set=reduce_mem_usage(feature_set, verbose=True)\n        DataSet = pd.concat([DataSet, feature_set], axis=1)\n\n    DataSet[\"titlekw_querykw_diff\"] = DataSet[\"titlekw_num\"] - DataSet[\"querykw_num\"]\n    DataSet[\"titlekw_querykw_rate\"] = DataSet[\"titlekw_num\"] / DataSet[\"querykw_num\"]\n\n    if \"title_code_score\" in DataSet.columns:\n        DataSet.title_code_score = DataSet.title_code_score.fillna(0)\n    if \"tag_score\" in DataSet.columns:\n        DataSet.tag_score = DataSet.tag_score.fillna(0)\n\n    DataSet = reduce_mem_usage(DataSet, verbose=True)\n    print(\"Data Read Finish!\")\n    return DataSet\n\n\nif __name__ == \"__main__\":\n    if flag == 'train':\n\n        train_size = 98000000\n        emb_mat = np.load('/home/kesci/work/sunrui/NN_second_2e/word2vec_fasttext_6kw_nn_sim.npy')\n        \n        lgb_data = ReadData(datatype='train', nrows=100000000)\n        used_lgb_dense_feature = get_used_feature_names(featurecol_h5) + othercols\n\n        print(used_lgb_dense_feature)\n\n        text_data = pd.read_csv('/home/kesci/input/bytedance/train_final.csv', usecols=[0, 1, 3, 4], header=None,\n            names=['query_id', 'query', 'title', 'label'], nrows=100000000)\n        # 读取 lgb feature\n        print(text_data.shape)\n        lgb_data[used_lgb_dense_feature] = lgb_data[used_lgb_dense_feature].fillna(-1, )\n\n        train_lgb_data = lgb_data[:train_size]\n        val_lgb_data = lgb_data[train_size:]\n        # 读取 lgb feature 完毕\n\n        with open('/home/kesci/work/sunrui/NN_second_2e/second_6kw_nn_sim.pkl', 'rb') as f:\n            word2id = pickle.load(f)\n\n        val_text_data = text_data[train_size:]\n        train_text_data = text_data[:train_size]\n\n        Q_val = []\n        D_val = []\n        for query in val_text_data['query']:\n            query = query.split()\n            Q_val.append([word2id[w] for w in query])  # 没有命中就返回UNK\n\n        for title in val_text_data['title']:\n            title = title.split()\n            D_val.append([word2id[w] for w in title])\n\n        val_query_input = seq_padding(Q_val, max_seq_len)\n        val_title_input = seq_padding(D_val, max_seq_len)\n\n        Y_val = val_text_data['label'].values\n\n        train_generator = DataGenerator(word2id=word2id, text_data=train_text_data,\n                                        lgb_data=train_lgb_data, batch_size=batch_size)\n\n        val_text_data['query_id_nums'] = val_text_data.groupby(['query_id'])['label'].transform('count')\n\n        val_group_df = val_text_data[['query_id', 'query_id_nums']].drop_duplicates()\n        val_group = val_group_df.query_id_nums.get_values()\n\n        swa = SWA(checkpoint_dir='./sunrui/swa/', model_name='swa.model')\n\n        clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                       step_size=2 * train_size // batch_size, mode='triangular')\n\n        model = build_model(lstm_dim=128, emb_mat=emb_mat)\n\n        filepath = \"/home/kesci/work/sunrui/nn/gated_pre_1e/128dim/gated-{epoch:02d}_esim_128_pre1e.hdf5\"\n        checkpoint = ModelCheckpoint(filepath, verbose=1)\n\n        early_stopping = EarlyStopping(monitor='val_auc', patience=5, verbose=1, mode='max')\n\n        val_lgb_input = [val_lgb_data[feat].values for feat in used_lgb_dense_feature]\n\n        # train_model_input = [train_query_input, train_title_input] + train_lgb_input\n        val_model_input = [val_query_input, val_title_input] + val_lgb_input\n\n        eval_callback = Evaluation(\n            validation_data=(\n                val_model_input, val_group, Y_val))\n\n        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[auc])\n        model.summary()\n        model.fit_generator(train_generator, epochs=epochs,\n                            validation_data=(val_model_input, Y_val),\n                            callbacks=[early_stopping, eval_callback, swa, clr, checkpoint], shuffle=True,\n                            workers=2,\n                            use_multiprocessing=True)\n    else:\n        pass","execution_count":null},{"metadata":{"id":"BCFD276150654D7581A59851F71BD4D6"},"cell_type":"code","outputs":[],"source":"","execution_count":null},{"metadata":{"id":"F51D777E0BC04BD99F9A5E21E823FF2A"},"cell_type":"code","outputs":[],"source":"# 以下是带有特征的esim网络 后1e数据的 隐层个数为64 我们选择了第5轮的结果\n#（一开始训练了一轮 中途需要资源就停止了训练 读取再训练了4轮 所以是5轮的结果）","execution_count":null},{"metadata":{"id":"D86EAA19F4214A57B77EFE473845608D","hide_input":true},"cell_type":"code","outputs":[],"source":"import math\nimport os\nimport pickle\nfrom collections import OrderedDict\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom keras import Input, Model\nfrom keras.backend.tensorflow_backend import set_session\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import Embedding, Dense, Dropout, Lambda, concatenate, GlobalAveragePooling1D, subtract, multiply, \\\n    TimeDistributed, LSTM\nfrom keras.regularizers import l2\nfrom tensorflow import set_random_seed\n\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nsess = tf.Session(config=config)\nset_session(sess)\n\nseed = 2019\n\nset_random_seed(seed)  # Tensorflow\nnp.random.seed(seed)  # NumPy\n\nfrom keras.callbacks import Callback, ModelCheckpoint\nimport keras\nimport keras.backend as K\nfrom keras.engine import Layer\n\nimport numpy as np\n\nimport logging\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(level=logging.INFO)\nhandler = logging.FileHandler(\"./sunrui/gated_esim_64_1e_log.txt\")\nhandler.setLevel(logging.INFO)\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nhandler.setFormatter(formatter)\n\nconsole = logging.StreamHandler()\nconsole.setLevel(logging.INFO)\n\nlogger.addHandler(handler)\nlogger.addHandler(console)\n\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024 ** 2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (\n            start_mem - end_mem) / start_mem))\n    return df\n\n\nclass DotProductAttention(Layer):\n    \"\"\"\n    dot-product-attention mechanism, supporting masking\n    \"\"\"\n\n    def __init__(self, return_attend_weight=False, keep_mask=True, **kwargs):\n        self.return_attend_weight = return_attend_weight\n        self.keep_mask = keep_mask\n        self.supports_masking = True\n        super(DotProductAttention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert isinstance(input_shape, list)\n        input_shape_a, input_shape_b = input_shape\n\n        if len(input_shape_a) != 3 or len(input_shape_b) != 3:\n            raise ValueError('Inputs into DotProductAttention should be 3D tensors')\n\n        if input_shape_a[-1] != input_shape_b[-1]:\n            raise ValueError('Inputs into DotProductAttention should have the same dimensionality at the last axis')\n\n    def call(self, inputs, mask=None):\n        assert isinstance(inputs, list)\n        inputs_a, inputs_b = inputs\n\n        if mask is not None:\n            mask_a, mask_b = mask\n        else:\n            mask_a, mask_b = None, None\n\n        e = K.exp(K.batch_dot(inputs_a, inputs_b, axes=2))  # similarity between a & b\n\n        # apply mask before normalization (softmax)\n        if mask_a is not None:\n            e *= K.expand_dims(K.cast(mask_a, K.floatx()), 2)\n        if mask_b is not None:\n            e *= K.expand_dims(K.cast(mask_b, K.floatx()), 1)\n\n        e_b = e / K.cast(K.sum(e, axis=2, keepdims=True) + K.epsilon(), K.floatx())  # attention weight over b\n        e_a = e / K.cast(K.sum(e, axis=1, keepdims=True) + K.epsilon(), K.floatx())  # attention weight over a\n\n        if self.return_attend_weight:\n            return [e_b, e_a]\n\n        a_attend = K.batch_dot(e_b, inputs_b, axes=(2, 1))  # a attend to b\n        b_attend = K.batch_dot(e_a, inputs_a, axes=(1, 1))  # b attend to a\n        return [a_attend, b_attend]\n\n    def compute_mask(self, inputs, mask=None):\n        if self.keep_mask:\n            return mask\n        else:\n            return [None, None]\n\n    def compute_output_shape(self, input_shape):\n        if self.return_attend_weight:\n            input_shape_a, input_shape_b = input_shape\n            return [(input_shape_a[0], input_shape_a[1], input_shape_b[1]),\n                    (input_shape_a[0], input_shape_a[1], input_shape_b[1])]\n        return input_shape\n\n\nclass SWA(Callback):\n    def __init__(self, checkpoint_dir, model_name, swa_start=1):\n        \"\"\"\n        :param checkpoint_dir: the directory where the model will be saved in\n        :param model_name: the name of model we're training\n        :param swa_start: the epoch when averaging begins. We generally pre-train the network for a certain amount of\n                          epochs to start (swa_start > 1), as opposed to starting to track the average from the\n                          very beginning.\n        \"\"\"\n        super(SWA, self).__init__()\n        self.checkpoint_dir = checkpoint_dir\n        self.model_name = model_name\n        self.swa_start = swa_start\n        self.swa_model = None  # the model that we will use to store the average of the weights once SWA begins\n\n    def on_train_begin(self, logs=None):\n        self.epoch = 0\n        self.swa_n = 0\n        # self.swa_model = copy.deepcopy(self.model)  # make a copy of the model we're training\n        # Note: I found deep copy of a model with customized layer would give errors\n        self.swa_model = keras.models.clone_model(self.model)\n        self.swa_model.set_weights(self.model.get_weights())  # see: https://github.com/keras-team/keras/issues/1765\n\n    def on_epoch_end(self, epoch, logs=None):\n        if (self.epoch + 1) >= self.swa_start:\n            self.update_average_model()\n            self.swa_n += 1\n\n        self.epoch += 1\n\n    def update_average_model(self):\n        # update running average of parameters\n        alpha = 1. / (self.swa_n + 1)\n        for layer, swa_layer in zip(self.model.layers, self.swa_model.layers):\n            weights = []\n            for w1, w2 in zip(swa_layer.get_weights(), layer.get_weights()):\n                weights.append((1 - alpha) * w1 + alpha * w2)\n            swa_layer.set_weights(weights)\n\n    def on_train_end(self, logs=None):\n        print('Logging Info - Saving SWA model checkpoint: %s_swa.hdf5\\n' % self.model_name)\n        self.swa_model.save_weights(os.path.join(self.checkpoint_dir, '{}_swa.hdf5'.format(self.model_name)))\n        print('Logging Info - SWA model Saved')\n\n\nclass CyclicLR(Callback):\n    def __init__(\n            self,\n            base_lr=0.001,\n            max_lr=0.006,\n            step_size=2000.,\n            mode='triangular',\n            gamma=1.,\n            scale_fn=None,\n            scale_mode='cycle'):\n        super(CyclicLR, self).__init__()\n\n        if mode not in ['triangular', 'triangular2',\n                        'exp_range']:\n            raise KeyError(\"mode must be one of 'triangular', \"\n                           \"'triangular2', or 'exp_range'\")\n        self.base_lr = base_lr\n        self.max_lr = max_lr\n        self.step_size = step_size\n        self.mode = mode\n        self.gamma = gamma\n        if scale_fn is None:\n            if self.mode == 'triangular':\n                self.scale_fn = lambda x: 1.\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = lambda x: 1 / (2. ** (x - 1))\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = lambda x: gamma ** x\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n        self.clr_iterations = 0.\n        self.trn_iterations = 0.\n        self.history = {}\n\n        self._reset()\n\n    def _reset(self, new_base_lr=None, new_max_lr=None,\n               new_step_size=None):\n        \"\"\"Resets cycle iterations.\n        Optional boundary/step size adjustment.\n        \"\"\"\n        if new_base_lr is not None:\n            self.base_lr = new_base_lr\n        if new_max_lr is not None:\n            self.max_lr = new_max_lr\n        if new_step_size is not None:\n            self.step_size = new_step_size\n        self.clr_iterations = 0.\n\n    def clr(self):\n        cycle = np.floor(1 + self.clr_iterations / (2 * self.step_size))\n        x = np.abs(self.clr_iterations / self.step_size - 2 * cycle + 1)\n        if self.scale_mode == 'cycle':\n            return self.base_lr + (self.max_lr - self.base_lr) * \\\n                   np.maximum(0, (1 - x)) * self.scale_fn(cycle)\n        else:\n            return self.base_lr + (self.max_lr - self.base_lr) * \\\n                   np.maximum(0, (1 - x)) * self.scale_fn(self.clr_iterations)\n\n    def on_train_begin(self, logs={}):\n        logs = logs or {}\n\n        if self.clr_iterations == 0:\n            K.set_value(self.model.optimizer.lr, self.base_lr)\n        else:\n            K.set_value(self.model.optimizer.lr, self.clr())\n\n    def on_batch_end(self, epoch, logs=None):\n\n        logs = logs or {}\n        self.trn_iterations += 1\n        self.clr_iterations += 1\n        K.set_value(self.model.optimizer.lr, self.clr())\n\n        self.history.setdefault(\n            'lr', []).append(\n            K.get_value(\n                self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.trn_iterations)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n\n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        logs['lr'] = K.get_value(self.model.optimizer.lr)\n\n\ndef get_dense_feature_inputs(dense_features_: list):\n    dense_input = OrderedDict()\n    for feature in dense_features_:\n        dense_input[feature] = Input(shape=(1,), name=feature + '_input')\n    return dense_input\n\n\ndef get_dense_feature_fc_list(dense_input_: OrderedDict, fc_dim=8, use_bias=True, l2_reg=1e-4):\n    dense_input = list(dense_input_.values())\n    fc_out_list = list(map(Dense(fc_dim, use_bias=use_bias, kernel_regularizer=l2(l2_reg)), dense_input))\n    return fc_out_list\n\n\ndef build_model(lstm_dim=64, emb_mat=None):\n    print('Build model...')\n\n    query_input = Input(shape=(max_seq_len,))\n    title_input = Input(shape=(max_seq_len,))\n\n    embedding = Embedding(emb_mat.shape[0], W2V_DIM, weights=[emb_mat], trainable=False, mask_zero=True)\n\n    query_emb = embedding(query_input)\n    query_emb = Dropout(0.2)(query_emb)\n\n    title_emb = embedding(title_input)\n    title_emb = Dropout(0.2)(title_emb)\n\n    bilstm_1 = LSTM(units=lstm_dim, return_sequences=True)\n\n    query_hidden = bilstm_1(query_emb)\n    title_hidden = bilstm_1(title_emb)\n\n    query_attend, title_attend = DotProductAttention()([query_hidden, title_hidden])\n\n    query_enhance = concatenate([query_hidden, query_attend, subtract([query_hidden, query_attend]),\n                                 multiply([query_hidden, query_attend])])  # [?,25,256]\n\n    title_enhance = concatenate([title_hidden, title_attend,\n                                 subtract([title_hidden, title_attend]),\n                                 multiply([title_hidden, title_attend])])  # [?,25,256]\n\n    # inference composition\n    feed_forward = TimeDistributed(Dense(units=lstm_dim, activation='relu'))\n\n    bilstm_2 = LSTM(units=lstm_dim, return_sequences=True)\n\n    query_compose = bilstm_2(feed_forward(query_enhance))  # [?,25,32]\n    title_compose = bilstm_2(feed_forward(title_enhance))\n\n    global_max_pooling = Lambda(lambda x: K.max(x, axis=1))  # GlobalMaxPooling1D didn't support masking\n    query_avg = GlobalAveragePooling1D()(query_compose)\n    query_max = global_max_pooling(query_compose)\n    title_avg = GlobalAveragePooling1D()(title_compose)\n    title_max = global_max_pooling(title_compose)\n\n    lgb_dense_feature_input = get_dense_feature_inputs(used_lgb_dense_feature)\n\n    dense_fc_list = get_dense_feature_fc_list(lgb_dense_feature_input)\n\n    if len(dense_fc_list) > 1:\n        dense_feature_concat = concatenate(dense_fc_list)\n    else:\n        dense_feature_concat = dense_fc_list[0]\n\n    inference_compose = concatenate([query_avg, query_max, title_avg, title_max])\n\n    # inference_compose = BatchNormalization()(inference_compose)  # 尝试\n\n    dense_esim = Dense(units=lstm_dim)(inference_compose)\n\n    dense_feature_gate = Dense(lstm_dim, activation='sigmoid')(dense_feature_concat)\n\n    gated_esim = Lambda(lambda x: x[0] * x[1])([dense_esim, dense_feature_gate])\n\n    gated_esim = Dense(lstm_dim, activation='elu')(gated_esim)\n    model_dense_input = [lgb_dense_feature_input[feat] for feat in used_lgb_dense_feature]\n\n    # gated_esim = BatchNormalization()(gated_esim)  # 加了BN 训练不稳定 val loss会跳 但是性能尚可\n    # gated_esim = Dropout(0.1)(gated_esim)\n\n    output = Dense(1, activation='sigmoid')(gated_esim)\n    model = Model(inputs=[query_input, title_input] + model_dense_input, outputs=output)\n    return model\n\n\ndef auc(y_true, y_pred):\n    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)\n\n\nfrom joblib import Parallel, delayed\nfrom sklearn.metrics import roc_auc_score\n\n\ndef cal_AUC(labels, prob):\n    try:\n        return roc_auc_score(labels, prob)\n    except:\n        return 0.5\n\n\n# 计算各个组的qauc值\ndef sum_AUC(mycombinedata):\n    grouplist = mycombinedata[0]\n    y_true = mycombinedata[1]\n    y_pred = mycombinedata[2]\n\n    if len(y_true) != sum(grouplist):\n        print(\"评分函数中len(y_true)!=sum(group)\")\n        return\n    start = 0\n    sum_AUC = 0\n    for group in grouplist:\n        roc_auc = cal_AUC(y_true[start:start + group], y_pred[start:start + group])\n        start = start + group\n        sum_AUC = sum_AUC + roc_auc\n    return sum_AUC\n\n\ndef QAUC_parallel(y_true, y_pred, group):\n    groupnum = 4\n    import math\n    group_len = math.ceil(len(group) / groupnum)\n    groups = [group[i * group_len:(i + 1) * group_len] for i in range(groupnum)]\n    mycombines = []\n\n    start = 0\n    for agroup in groups:\n        mycombinedata = []\n        mycombinedata.append(agroup)\n        mycombinedata.append(y_true[start:start + sum(agroup)])\n        mycombinedata.append(y_pred[start:start + sum(agroup)])\n        start = start + sum(agroup)\n        mycombines.append(mycombinedata)\n\n    sum_AUC_ = Parallel(n_jobs=groupnum)(delayed(sum_AUC)(mycombinedata) for mycombinedata in mycombines)\n\n    return sum(sum_AUC_) / len(group)\n\n\nclass Evaluation(Callback):\n    def __init__(self, validation_data=(), interval=1):\n        super(Callback, self).__init__()\n        self.interval = interval\n        self.x_val, self.val_group, self.y_val = validation_data\n        self.best_score = 0.\n        self.best_epoch = 0\n        self.best_auc = 0.0\n        self.best_auc_epoch = 0\n        self.auc_list = []\n        self.q_auc_list = []\n\n    def on_epoch_end(self, epoch, log={}):\n        if epoch % self.interval == 0:\n            y_pred = self.model.predict(self.x_val, verbose=0, batch_size=batch_size)\n            auc_score = roc_auc_score(self.y_val, y_pred)\n            self.auc_list.append(auc_score)\n            if auc_score > self.best_auc:\n                self.best_auc = auc_score\n                self.best_auc_epoch = epoch + 1\n\n            score_parallel = QAUC_parallel(self.y_val, y_pred, self.val_group)\n            self.q_auc_list.append(score_parallel)\n            if score_parallel > self.best_score:\n                self.best_score = score_parallel\n                self.best_epoch = epoch + 1\n            logger.info(f'Q_AUC = {score_parallel} epoch = {epoch + 1}')\n\n            print('\\n ROC_AUC - epoch:%d - score:%.6f' % (epoch + 1, auc_score))\n            print('\\n Q_AUC - epoch:%d - score:%.6f' % (epoch + 1, score_parallel))\n\n    def get_best_score_epoch(self):\n        return self.best_score, self.best_epoch\n\n    def get_best_auc_score_epoch(self):\n        return self.best_auc, self.best_auc_epoch\n\n    def show_result_list(self):\n        print('auc', self.auc_list)\n        print('\\n')\n        print('qauc', self.q_auc_list)\n\n\ndef seq_padding(X, max_len):\n    return [x + [PAD] * (max_len - len(x)) if len(x) < max_len else x[:max_len] for x in X]\n\n\nclass DataGenerator(keras.utils.Sequence):\n\n    def __init__(self, word2id, text_data, lgb_data, batch_size=1024 * 5):\n        self.batch_size = batch_size\n        self.word2id = word2id\n\n        self.text_data = text_data\n        self.lgb_data = lgb_data\n\n    def __len__(self):\n        # 计算每一个epoch的迭代次数\n        return math.ceil(len(self.text_data) / float(self.batch_size))\n\n    def __getitem__(self, index):\n        start = index * self.batch_size\n        stop = (index + 1) * self.batch_size\n\n        batch_lgb_df = self.lgb_data.iloc[start:stop]\n        batch_text_df = self.text_data.iloc[start:stop]\n        y = batch_text_df['label'].values\n\n        train_lgb_input = [batch_lgb_df[feat].values for feat in used_lgb_dense_feature]\n\n        Q = []\n        D = []\n        for query in batch_text_df['query']:\n            query = query.split()\n            Q.append([word2id[w] for w in query])\n\n        for title in batch_text_df['title']:\n            title = title.split()\n            D.append([word2id[w] for w in title])\n\n        Q_pad = seq_padding(Q, max_seq_len)\n        D_pad = seq_padding(D, max_seq_len)\n        return [np.array(Q_pad), np.array(D_pad)] + train_lgb_input, y\n\n\nflag = 'train'\nbatch_size = 1024 * 5\n\nPAD = 0\nUNK = 1\n\nW2V_DIM = 200\n\nmax_seq_len = 25\nepochs = 20\n\n\ndef get_used_feature_names(featurecol_h5):\n    features = []\n    for k, v in featurecol_h5.items():\n        features.extend(v)\n    return features\n\n\nfeaturecol_h5 = {\n\n    'sim_feat': ['jaccard_q3_t3',\n                 'jaccard_q3_t5',\n                 'jaccard_q5_t5', 'levenshtein_q5_t5',\n                 'jaccard_q5_t10', 'levenshtein_q5_t10',\n                 'jaccard_q10_t10', 'levenshtein_q10_t10',\n                 'jaccard_q15_t25', 'levenshtein_q15_t25',\n                 'jaccard', 'levenshtein'],\n\n    'len_feat': [\"querykw_num\", \"titlekw_num\"],\n\n    \"title_nunique_query\": [\"title_nunique_query\"],\n    \"query_nunique_title\": [\"query_nunique_title\"],\n\n    'title_score_count_feat': [\"title_score_count\", \"title_score_click_num\"],\n    'title_code_score_feat': [\"title_code_score\"],\n    'title_convert_feat': [\"title_code_convert\", 'title_code_label_count'],\n\n    'query_count': [\"query_code_count\"],\n    'title_count': [\"title_count\"],\n\n    \"match_feat\": ['count_match', 'blockcount_match', 'proximity', 'maxMatchBlockLen',\n                   'q1_match_start', 'q1_match_end'],\n\n    \"BM25\": [\"BM25\"],\n}\n\nothercols = [\"titlekw_querykw_diff\", \"titlekw_querykw_rate\"]\n\n\ndef reduce_mem_usage(D, verbose=True):\n    start_mem = D.memory_usage().sum() / 1024 ** 2\n    for c, d in zip(D.columns, D.dtypes):\n        if d.kind == 'f':\n            D[c] = pd.to_numeric(D[c], downcast='float')\n        elif d.kind == 'i':\n            D[c] = pd.to_numeric(D[c], downcast='signed')\n    end_mem = D.memory_usage().sum() / 1024 ** 2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (\n            start_mem - end_mem) / start_mem))\n    return D\n\n\ndef ReadTrainData(datatype='train', nrows=100000000):\n    DataSet = pd.read_hdf('/home/kesci/work/sunrui/post10kw/query_id_post10kw.h5', stop=nrows).reset_index(drop=True)\n    label_set = pd.read_hdf('/home/kesci/work/label_train.h5', start=900000000, stop=900000000 + nrows).reset_index(\n        drop=True)\n    DataSet = pd.concat([DataSet, label_set], axis=1)\n\n    featuremap_h5 = {\n        'title_count': f'count_feature_{datatype}.h5',\n        'query_count': f'query_count_all_{datatype}.h5',\n\n        'cross_feat': f'cross_{datatype}_feat.h5',\n        'editdistance_relativepos': f'editdistance_relativepos_{datatype}_feat.h5',\n        'fuzz': f\"fuzz_{datatype}_feat.h5\",\n        'len_feat': f'len_{datatype}_feat.h5',\n        'NN_SIM': f'nn_sim_feature.h5',\n        \"title_nunique_query\": f'nunique_feature_{datatype}.h5',\n        \"query_nunique_title\": f'query_nunique_title_all_{datatype}.h5',\n        'match_feat': f'query_match_{datatype}_feat.h5',\n        'query_pos_feat': f'query_pos_{datatype}_feat.h5',\n        'sen_dis': f\"sen_dis_{datatype}_200.h5\",\n        'sen_dis2': f\"sen_dis2_{datatype}_200.h5\",\n        'sim_feat': f'sim_{datatype}_feat.h5',\n        'textpair': f\"textpair_{datatype}_feat.h5\",\n\n        'tag_score_feat': f'tag_score_10foldtime_{datatype}_feat.h5',\n        'title_score_count_feat': f'title_score_count_{datatype}_feat.h5',\n        'title_code_score_feat': f'title_code_score_10foldtime_{datatype}_feat.h5',\n        'title_convert_feat': f'title_convert_{datatype}.h5',\n        'tag': f'tag_{datatype}.h5',\n        \"tag_convert_feat\": f\"tag_convert_{datatype}.h5\",\n        \"query_convert\": f\"query_convert_{datatype}.h5\",\n\n        \"BM25\": f'BM25_{datatype}_feat.h5',\n    }\n    path_pre6kw = \"/home/kesci/work/post_4kw_data/train_6kw/\"\n    path_post4kw = \"/home/kesci/work/post_4kw_data/train/\"\n    for featurefile in featurecol_h5:\n        print(\"读取\", featuremap_h5[featurefile])\n        feature_set1 = pd.read_hdf(path_pre6kw + featuremap_h5[featurefile], key='data',\n                                   start=0, stop=60000000\n                                   # start=0,stop=nrows\n                                   )[featurecol_h5[featurefile]].reset_index(drop=True)\n        # print(feature_set1.head(1))\n        print(\"feature_set1 len:\", feature_set1.__len__())\n        feature_set2 = pd.read_hdf(path_post4kw + featuremap_h5[featurefile], key='data',\n                                   start=0, stop=40000000\n                                   # start=0,stop=nrows\n                                   )[featurecol_h5[featurefile]].reset_index(drop=True)\n        # print(feature_set2.head(1))\n        print(\"feature_set2 len:\", feature_set2.__len__())\n        feature_set = pd.concat([feature_set1, feature_set2], axis=0).reset_index(drop=True)\n        print(\"length:\", feature_set.__len__())\n        feature_set = reduce_mem_usage(feature_set, verbose=True)\n        DataSet = pd.concat([DataSet, feature_set], axis=1)\n\n    DataSet[\"titlekw_querykw_diff\"] = DataSet[\"titlekw_num\"] - DataSet[\"querykw_num\"]\n    DataSet[\"titlekw_querykw_rate\"] = DataSet[\"titlekw_num\"] / DataSet[\"querykw_num\"]\n    print(\"Data Read Finish!\")\n    return DataSet\n\n\nif __name__ == \"__main__\":\n    if flag == 'train':\n\n        train_size = 98000000\n        emb_mat = np.load('/home/kesci/work/sunrui/NN_second_2e/word2vec_fasttext_6kw_nn_sim.npy')\n\n        text_ = pd.read_csv('/home/kesci/work/word2vec/post_10kw.csv').reset_index(drop=True)\n        print(text_.shape)\n        query_id = pd.read_hdf('/home/kesci/work/sunrui/post10kw/query_id_post10kw.h5').reset_index(drop=True)\n        print(query_id.shape)\n\n        label_ = pd.read_hdf('/home/kesci/work/label_train.h5', start=900000000, end=1000000000).reset_index(drop=True)\n        print(label_.shape)\n\n        text_data = pd.concat([text_, query_id, label_], axis=1)\n        # 读取 lgb feature\n\n        lgb_data = ReadTrainData(nrows=100000000)\n\n        used_lgb_dense_feature = get_used_feature_names(featurecol_h5) + othercols\n\n        print(used_lgb_dense_feature)\n        lgb_data[used_lgb_dense_feature] = lgb_data[used_lgb_dense_feature].fillna(-1, )\n\n        train_lgb_data = lgb_data[:train_size]\n        val_lgb_data = lgb_data[train_size:]\n        # 读取 lgb feature 完毕\n\n        with open('/home/kesci/work/sunrui/NN_second_2e/second_6kw_nn_sim.pkl', 'rb') as f:\n            word2id = pickle.load(f)\n\n        val_text_data = text_data[train_size:]\n        train_text_data = text_data[:train_size]\n\n        Q_val = []\n        D_val = []\n        for query in val_text_data['query']:\n            query = query.split()\n            Q_val.append([word2id[w] for w in query])  # 没有命中就返回UNK\n\n        for title in val_text_data['title']:\n            title = title.split()\n            D_val.append([word2id[w] for w in title])\n\n        val_query_input = seq_padding(Q_val, max_seq_len)\n        val_title_input = seq_padding(D_val, max_seq_len)\n\n        Y_val = val_text_data['label'].values\n\n        train_generator = DataGenerator(word2id=word2id, text_data=train_text_data,\n                                        lgb_data=train_lgb_data, batch_size=batch_size)\n\n        val_text_data['query_id_nums'] = val_text_data.groupby(['query_id'])['label'].transform('count')\n\n        val_group_df = val_text_data[['query_id', 'query_id_nums']].drop_duplicates()\n        val_group = val_group_df.query_id_nums.get_values()\n\n        swa = SWA(checkpoint_dir='./sunrui/swa/', model_name='swa.model')\n\n        clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                       step_size=2 * train_size // batch_size, mode='triangular')\n\n        model = build_model(lstm_dim=64, emb_mat=emb_mat)\n    \n    \n        filepath = \"./sunrui/nn/gated_esim_{epoch:02d}_64_1e.hdf5\"\n\n        checkpoint = ModelCheckpoint(filepath, verbose=1)\n\n        early_stopping = EarlyStopping(monitor='val_auc', patience=5, verbose=1, mode='max')\n\n        val_lgb_input = [val_lgb_data[feat].values for feat in used_lgb_dense_feature]\n\n        # train_model_input = [train_query_input, train_title_input] + train_lgb_input\n        val_model_input = [val_query_input, val_title_input] + val_lgb_input\n\n        eval_callback = Evaluation(\n            validation_data=(\n                val_model_input, val_group, Y_val))\n\n        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[auc])\n        model.summary()\n        model.fit_generator(train_generator, epochs=epochs,\n                            validation_data=(val_model_input, Y_val),\n                            callbacks=[early_stopping, eval_callback, swa, clr, checkpoint], shuffle=True,\n                            workers=2,\n                            use_multiprocessing=True)\n    else:\n        pass\n    \n","execution_count":null},{"metadata":{"id":"31700C6FFE43416C9B9845A5BFAEA49D"},"cell_type":"code","outputs":[],"source":"","execution_count":null},{"metadata":{"id":"F58A7314AF534862970A32E98C4BB215"},"cell_type":"code","outputs":[],"source":"# 以下是带有特征的esim网络 前1e数据的 隐层个数为64 我们选择了第1轮的结果 其实训练了4轮 \n# 但是看第一轮的线下指标最好 ","execution_count":null},{"metadata":{"id":"0D234D2C454C49328611D07F83256735","hide_input":true},"cell_type":"code","outputs":[],"source":"import math\nimport os\nimport pickle\nfrom collections import OrderedDict\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom keras import Input, Model\nfrom keras.backend.tensorflow_backend import set_session\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import Embedding, Dense, Dropout, Lambda, concatenate, GlobalAveragePooling1D, subtract, multiply, \\\n    TimeDistributed, LSTM\nfrom keras.regularizers import l2\nfrom tensorflow import set_random_seed\n\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nsess = tf.Session(config=config)\nset_session(sess)\n\nseed = 2019\n\nset_random_seed(seed)  # Tensorflow\nnp.random.seed(seed)  # NumPy\n\nfrom keras.callbacks import Callback, ModelCheckpoint\nimport keras\nimport keras.backend as K\nfrom keras.engine import Layer\n\nimport numpy as np\n\nimport logging\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(level=logging.INFO)\nhandler = logging.FileHandler(\"./sunrui/gated_esim_64_pre_1e_log.txt\")  # 训练前1e的数据\nhandler.setLevel(logging.INFO)\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nhandler.setFormatter(formatter)\n\nconsole = logging.StreamHandler()\nconsole.setLevel(logging.INFO)\n\nlogger.addHandler(handler)\nlogger.addHandler(console)\n\n\nclass DotProductAttention(Layer):\n    \"\"\"\n    dot-product-attention mechanism, supporting masking\n    \"\"\"\n\n    def __init__(self, return_attend_weight=False, keep_mask=True, **kwargs):\n        self.return_attend_weight = return_attend_weight\n        self.keep_mask = keep_mask\n        self.supports_masking = True\n        super(DotProductAttention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert isinstance(input_shape, list)\n        input_shape_a, input_shape_b = input_shape\n\n        if len(input_shape_a) != 3 or len(input_shape_b) != 3:\n            raise ValueError('Inputs into DotProductAttention should be 3D tensors')\n\n        if input_shape_a[-1] != input_shape_b[-1]:\n            raise ValueError('Inputs into DotProductAttention should have the same dimensionality at the last axis')\n\n    def call(self, inputs, mask=None):\n        assert isinstance(inputs, list)\n        inputs_a, inputs_b = inputs\n\n        if mask is not None:\n            mask_a, mask_b = mask\n        else:\n            mask_a, mask_b = None, None\n\n        e = K.exp(K.batch_dot(inputs_a, inputs_b, axes=2))  # similarity between a & b\n\n        # apply mask before normalization (softmax)\n        if mask_a is not None:\n            e *= K.expand_dims(K.cast(mask_a, K.floatx()), 2)\n        if mask_b is not None:\n            e *= K.expand_dims(K.cast(mask_b, K.floatx()), 1)\n\n        e_b = e / K.cast(K.sum(e, axis=2, keepdims=True) + K.epsilon(), K.floatx())  # attention weight over b\n        e_a = e / K.cast(K.sum(e, axis=1, keepdims=True) + K.epsilon(), K.floatx())  # attention weight over a\n\n        if self.return_attend_weight:\n            return [e_b, e_a]\n\n        a_attend = K.batch_dot(e_b, inputs_b, axes=(2, 1))  # a attend to b\n        b_attend = K.batch_dot(e_a, inputs_a, axes=(1, 1))  # b attend to a\n        return [a_attend, b_attend]\n\n    def compute_mask(self, inputs, mask=None):\n        if self.keep_mask:\n            return mask\n        else:\n            return [None, None]\n\n    def compute_output_shape(self, input_shape):\n        if self.return_attend_weight:\n            input_shape_a, input_shape_b = input_shape\n            return [(input_shape_a[0], input_shape_a[1], input_shape_b[1]),\n                    (input_shape_a[0], input_shape_a[1], input_shape_b[1])]\n        return input_shape\n\n\nclass SWA(Callback):\n    def __init__(self, checkpoint_dir, model_name, swa_start=1):\n        \"\"\"\n        :param checkpoint_dir: the directory where the model will be saved in\n        :param model_name: the name of model we're training\n        :param swa_start: the epoch when averaging begins. We generally pre-train the network for a certain amount of\n                          epochs to start (swa_start > 1), as opposed to starting to track the average from the\n                          very beginning.\n        \"\"\"\n        super(SWA, self).__init__()\n        self.checkpoint_dir = checkpoint_dir\n        self.model_name = model_name\n        self.swa_start = swa_start\n        self.swa_model = None  # the model that we will use to store the average of the weights once SWA begins\n\n    def on_train_begin(self, logs=None):\n        self.epoch = 0\n        self.swa_n = 0\n        # self.swa_model = copy.deepcopy(self.model)  # make a copy of the model we're training\n        # Note: I found deep copy of a model with customized layer would give errors\n        self.swa_model = keras.models.clone_model(self.model)\n        self.swa_model.set_weights(self.model.get_weights())  # see: https://github.com/keras-team/keras/issues/1765\n\n    def on_epoch_end(self, epoch, logs=None):\n        if (self.epoch + 1) >= self.swa_start:\n            self.update_average_model()\n            self.swa_n += 1\n\n        self.epoch += 1\n\n    def update_average_model(self):\n        # update running average of parameters\n        alpha = 1. / (self.swa_n + 1)\n        for layer, swa_layer in zip(self.model.layers, self.swa_model.layers):\n            weights = []\n            for w1, w2 in zip(swa_layer.get_weights(), layer.get_weights()):\n                weights.append((1 - alpha) * w1 + alpha * w2)\n            swa_layer.set_weights(weights)\n\n    def on_train_end(self, logs=None):\n        print('Logging Info - Saving SWA model checkpoint: %s_swa.hdf5\\n' % self.model_name)\n        self.swa_model.save_weights(os.path.join(self.checkpoint_dir, '{}_swa.hdf5'.format(self.model_name)))\n        print('Logging Info - SWA model Saved')\n\n\nclass CyclicLR(Callback):\n    def __init__(\n            self,\n            base_lr=0.001,\n            max_lr=0.006,\n            step_size=2000.,\n            mode='triangular',\n            gamma=1.,\n            scale_fn=None,\n            scale_mode='cycle'):\n        super(CyclicLR, self).__init__()\n\n        if mode not in ['triangular', 'triangular2',\n                        'exp_range']:\n            raise KeyError(\"mode must be one of 'triangular', \"\n                           \"'triangular2', or 'exp_range'\")\n        self.base_lr = base_lr\n        self.max_lr = max_lr\n        self.step_size = step_size\n        self.mode = mode\n        self.gamma = gamma\n        if scale_fn is None:\n            if self.mode == 'triangular':\n                self.scale_fn = lambda x: 1.\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = lambda x: 1 / (2. ** (x - 1))\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = lambda x: gamma ** x\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n        self.clr_iterations = 0.\n        self.trn_iterations = 0.\n        self.history = {}\n\n        self._reset()\n\n    def _reset(self, new_base_lr=None, new_max_lr=None,\n               new_step_size=None):\n        \"\"\"Resets cycle iterations.\n        Optional boundary/step size adjustment.\n        \"\"\"\n        if new_base_lr is not None:\n            self.base_lr = new_base_lr\n        if new_max_lr is not None:\n            self.max_lr = new_max_lr\n        if new_step_size is not None:\n            self.step_size = new_step_size\n        self.clr_iterations = 0.\n\n    def clr(self):\n        cycle = np.floor(1 + self.clr_iterations / (2 * self.step_size))\n        x = np.abs(self.clr_iterations / self.step_size - 2 * cycle + 1)\n        if self.scale_mode == 'cycle':\n            return self.base_lr + (self.max_lr - self.base_lr) * \\\n                   np.maximum(0, (1 - x)) * self.scale_fn(cycle)\n        else:\n            return self.base_lr + (self.max_lr - self.base_lr) * \\\n                   np.maximum(0, (1 - x)) * self.scale_fn(self.clr_iterations)\n\n    def on_train_begin(self, logs={}):\n        logs = logs or {}\n\n        if self.clr_iterations == 0:\n            K.set_value(self.model.optimizer.lr, self.base_lr)\n        else:\n            K.set_value(self.model.optimizer.lr, self.clr())\n\n    def on_batch_end(self, epoch, logs=None):\n\n        logs = logs or {}\n        self.trn_iterations += 1\n        self.clr_iterations += 1\n        K.set_value(self.model.optimizer.lr, self.clr())\n\n        self.history.setdefault(\n            'lr', []).append(\n            K.get_value(\n                self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.trn_iterations)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n\n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        logs['lr'] = K.get_value(self.model.optimizer.lr)\n\n\ndef get_dense_feature_inputs(dense_features_: list):\n    dense_input = OrderedDict()\n    for feature in dense_features_:\n        dense_input[feature] = Input(shape=(1,), name=feature + '_input')\n    return dense_input\n\n\ndef get_dense_feature_fc_list(dense_input_: OrderedDict, fc_dim=8, use_bias=True, l2_reg=1e-4):\n    dense_input = list(dense_input_.values())\n    fc_out_list = list(map(Dense(fc_dim, use_bias=use_bias, kernel_regularizer=l2(l2_reg)), dense_input))\n    return fc_out_list\n\n\ndef build_model(lstm_dim=64, emb_mat=None):\n    print('Build model...')\n\n    query_input = Input(shape=(max_seq_len,))\n    title_input = Input(shape=(max_seq_len,))\n\n    embedding = Embedding(emb_mat.shape[0], W2V_DIM, weights=[emb_mat], trainable=False, mask_zero=True)\n\n    query_emb = embedding(query_input)\n    query_emb = Dropout(0.2)(query_emb)\n\n    title_emb = embedding(title_input)\n    title_emb = Dropout(0.2)(title_emb)\n\n    bilstm_1 = LSTM(units=lstm_dim, return_sequences=True)\n\n    query_hidden = bilstm_1(query_emb)\n    title_hidden = bilstm_1(title_emb)\n\n    query_attend, title_attend = DotProductAttention()([query_hidden, title_hidden])\n\n    query_enhance = concatenate([query_hidden, query_attend, subtract([query_hidden, query_attend]),\n                                 multiply([query_hidden, query_attend])])  # [?,25,256]\n\n    title_enhance = concatenate([title_hidden, title_attend,\n                                 subtract([title_hidden, title_attend]),\n                                 multiply([title_hidden, title_attend])])  # [?,25,256]\n\n    # inference composition\n    feed_forward = TimeDistributed(Dense(units=lstm_dim, activation='relu'))\n\n    bilstm_2 = LSTM(units=lstm_dim, return_sequences=True)\n\n    query_compose = bilstm_2(feed_forward(query_enhance))  # [?,25,32]\n    title_compose = bilstm_2(feed_forward(title_enhance))\n\n    global_max_pooling = Lambda(lambda x: K.max(x, axis=1))  # GlobalMaxPooling1D didn't support masking\n    query_avg = GlobalAveragePooling1D()(query_compose)\n    query_max = global_max_pooling(query_compose)\n    title_avg = GlobalAveragePooling1D()(title_compose)\n    title_max = global_max_pooling(title_compose)\n\n    lgb_dense_feature_input = get_dense_feature_inputs(used_lgb_dense_feature)\n\n    dense_fc_list = get_dense_feature_fc_list(lgb_dense_feature_input)\n\n    if len(dense_fc_list) > 1:\n        dense_feature_concat = concatenate(dense_fc_list)\n    else:\n        dense_feature_concat = dense_fc_list[0]\n\n    inference_compose = concatenate([query_avg, query_max, title_avg, title_max])\n\n    # inference_compose = BatchNormalization()(inference_compose)  # 尝试\n\n    dense_esim = Dense(units=lstm_dim)(inference_compose)\n\n    dense_feature_gate = Dense(lstm_dim, activation='sigmoid')(dense_feature_concat)\n\n    gated_esim = Lambda(lambda x: x[0] * x[1])([dense_esim, dense_feature_gate])\n\n    gated_esim = Dense(lstm_dim, activation='elu')(gated_esim)\n    model_dense_input = [lgb_dense_feature_input[feat] for feat in used_lgb_dense_feature]\n\n    # gated_esim = BatchNormalization()(gated_esim)  # 加了BN 训练不稳定 val loss会跳 但是性能尚可\n    # gated_esim = Dropout(0.1)(gated_esim)\n\n    output = Dense(1, activation='sigmoid')(gated_esim)\n    model = Model(inputs=[query_input, title_input] + model_dense_input, outputs=output)\n    return model\n\n\ndef auc(y_true, y_pred):\n    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)\n\n\nfrom joblib import Parallel, delayed\nfrom sklearn.metrics import roc_auc_score\n\n\ndef cal_AUC(labels, prob):\n    try:\n        return roc_auc_score(labels, prob)\n    except:\n        return 0.5\n\n\n# 计算各个组的qauc值\ndef sum_AUC(mycombinedata):\n    grouplist = mycombinedata[0]\n    y_true = mycombinedata[1]\n    y_pred = mycombinedata[2]\n\n    if len(y_true) != sum(grouplist):\n        print(\"评分函数中len(y_true)!=sum(group)\")\n        return\n    start = 0\n    sum_AUC = 0\n    for group in grouplist:\n        roc_auc = cal_AUC(y_true[start:start + group], y_pred[start:start + group])\n        start = start + group\n        sum_AUC = sum_AUC + roc_auc\n    return sum_AUC\n\n\ndef QAUC_parallel(y_true, y_pred, group):\n    groupnum = 4\n    import math\n    group_len = math.ceil(len(group) / groupnum)\n    groups = [group[i * group_len:(i + 1) * group_len] for i in range(groupnum)]\n    mycombines = []\n\n    start = 0\n    for agroup in groups:\n        mycombinedata = []\n        mycombinedata.append(agroup)\n        mycombinedata.append(y_true[start:start + sum(agroup)])\n        mycombinedata.append(y_pred[start:start + sum(agroup)])\n        start = start + sum(agroup)\n        mycombines.append(mycombinedata)\n\n    sum_AUC_ = Parallel(n_jobs=groupnum)(delayed(sum_AUC)(mycombinedata) for mycombinedata in mycombines)\n\n    return sum(sum_AUC_) / len(group)\n\n\nclass Evaluation(Callback):\n    def __init__(self, validation_data=(), interval=1):\n        super(Callback, self).__init__()\n        self.interval = interval\n        self.x_val, self.val_group, self.y_val = validation_data\n        self.best_score = 0.\n        self.best_epoch = 0\n        self.best_auc = 0.0\n        self.best_auc_epoch = 0\n        self.auc_list = []\n        self.q_auc_list = []\n\n    def on_epoch_end(self, epoch, log={}):\n        if epoch % self.interval == 0:\n            y_pred = self.model.predict(self.x_val, verbose=0, batch_size=batch_size)\n            auc_score = roc_auc_score(self.y_val, y_pred)\n            self.auc_list.append(auc_score)\n            if auc_score > self.best_auc:\n                self.best_auc = auc_score\n                self.best_auc_epoch = epoch + 1\n\n            score_parallel = QAUC_parallel(self.y_val, y_pred, self.val_group)\n            self.q_auc_list.append(score_parallel)\n            if score_parallel > self.best_score:\n                self.best_score = score_parallel\n                self.best_epoch = epoch + 1\n            logger.info(f'Q_AUC = {score_parallel} epoch = {epoch + 1}')\n\n            print('\\n ROC_AUC - epoch:%d - score:%.6f' % (epoch + 1, auc_score))\n            print('\\n Q_AUC - epoch:%d - score:%.6f' % (epoch + 1, score_parallel))\n\n    def get_best_score_epoch(self):\n        return self.best_score, self.best_epoch\n\n    def get_best_auc_score_epoch(self):\n        return self.best_auc, self.best_auc_epoch\n\n    def show_result_list(self):\n        print('auc', self.auc_list)\n        print('\\n')\n        print('qauc', self.q_auc_list)\n\n\ndef seq_padding(X, max_len):\n    return [x + [PAD] * (max_len - len(x)) if len(x) < max_len else x[:max_len] for x in X]\n\n\nclass DataGenerator(keras.utils.Sequence):\n\n    def __init__(self, word2id, text_data, lgb_data, batch_size=1024 * 5):\n        self.batch_size = batch_size\n        self.word2id = word2id\n\n        self.text_data = text_data\n        self.lgb_data = lgb_data\n\n    def __len__(self):\n        # 计算每一个epoch的迭代次数\n        return math.ceil(len(self.text_data) / float(self.batch_size))\n\n    def __getitem__(self, index):\n        start = index * self.batch_size\n        stop = (index + 1) * self.batch_size\n\n        batch_lgb_df = self.lgb_data.iloc[start:stop]\n        batch_text_df = self.text_data.iloc[start:stop]\n        y = batch_text_df['label'].values\n\n        train_lgb_input = [batch_lgb_df[feat].values for feat in used_lgb_dense_feature]\n\n        Q = []\n        D = []\n        for query in batch_text_df['query']:\n            query = query.split()\n            Q.append([word2id[w] for w in query])\n\n        for title in batch_text_df['title']:\n            title = title.split()\n            D.append([word2id[w] for w in title])\n\n        Q_pad = seq_padding(Q, max_seq_len)\n        D_pad = seq_padding(D, max_seq_len)\n        return [np.array(Q_pad), np.array(D_pad)] + train_lgb_input, y\n\n\nflag = 'train'\nbatch_size = 1024 * 5\n\nPAD = 0\nUNK = 1\n\nW2V_DIM = 200\n\nmax_seq_len = 25\nepochs = 20\n\n\ndef get_used_feature_names(featurecol_h5):\n    features = []\n    for k, v in featurecol_h5.items():\n        features.extend(v)\n    return features\n\n\nfeaturecol_h5 = {\n    'sim_feat': ['jaccard_q3_t3',\n                 'jaccard_q3_t5',\n                 'jaccard_q5_t5', 'levenshtein_q5_t5',\n                 'jaccard_q5_t10', 'levenshtein_q5_t10',\n                 'jaccard_q10_t10', 'levenshtein_q10_t10',\n                 'jaccard_q15_t25', 'levenshtein_q15_t25',\n                 'jaccard', 'levenshtein'],\n\n    'len_feat': [\"querykw_num\", \"titlekw_num\"],\n\n    \"title_nunique_query\": [\"title_nunique_query\"],\n    \"query_nunique_title\": [\"query_nunique_title\"],\n\n    'title_score_count_feat': [\"title_score_count\", \"title_score_click_num\"],\n    'title_code_score_feat': [\"title_code_score\"],\n    'title_convert_feat': [\"title_code_convert\", 'title_code_label_count'],\n\n    'query_count': [\"query_code_count\"],\n    'title_count': [\"title_count\"],\n\n    \"match_feat\": ['count_match', 'blockcount_match', 'proximity', 'maxMatchBlockLen',\n                   'q1_match_start', 'q1_match_end'],\n\n    \"BM25\": [\"BM25\"],\n}\n\nothercols = [\"titlekw_querykw_diff\", \"titlekw_querykw_rate\"]\n\n\ndef reduce_mem_usage(D, verbose=True):\n    start_mem = D.memory_usage().sum() / 1024 ** 2\n    for c, d in zip(D.columns, D.dtypes):\n        if d.kind == 'f':\n            D[c] = pd.to_numeric(D[c], downcast='float')\n        elif d.kind == 'i':\n            D[c] = pd.to_numeric(D[c], downcast='signed')\n    end_mem = D.memory_usage().sum() / 1024 ** 2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (\n            start_mem - end_mem) / start_mem))\n    return D\n\n\ndef ReadData(datatype='train', nrows=1000000000):\n    if datatype == 'train':\n        id_feature = '/home/kesci/input/bytedance/train_final.csv'\n        usecols = [0, 4]\n        names = ['query_id', 'label']\n\n        print(\"read \", id_feature)\n        DataSet = pd.read_csv(id_feature,\n                              header=None,\n                              nrows=nrows,\n                              usecols=usecols,\n                              names=names\n                              )\n        path_h5 = \"/home/kesci/work/pre_3billion_data/train/\"\n    elif datatype == 'test1':\n        id_feature = '/home/kesci/input/bytedance/test_final_part1.csv'\n        usecols = [0, 2]\n        names = ['query_id', 'query_title_id']\n        path_h5 = \"/home/kesci/work/post_4kw_data/test1/\"\n        print(\"read \", id_feature)\n        DataSet = pd.read_csv(id_feature,\n                              header=None,\n                              nrows=nrows,\n                              usecols=usecols,\n                              names=names\n                              )\n    elif datatype == 'test2':\n        id_feature = '/home/kesci/input/bytedance/bytedance_contest.final_2.csv'\n        usecols = [0, 2]\n        names = ['query_id', 'query_title_id']\n        path_h5 = \"/home/kesci/work/post_4kw_data/test2/\"\n        print(\"read \", id_feature)\n        DataSet = pd.read_csv(id_feature,\n                              header=None,\n                              nrows=nrows,\n                              usecols=usecols,\n                              names=names\n                              )\n\n    print(\"length:\", DataSet.__len__())\n    DataSet = reduce_mem_usage(DataSet, verbose=True)\n\n    featuremap_h5 = {\n        'cross_feat': path_h5 + f'cross_{datatype}_feat.h5',\n\n        'query_pos_feat': path_h5 + f'query_pos_{datatype}_feat.h5',\n        'title_pos_feat': path_h5 + f'title_pos_{datatype}_feat.h5',\n\n        'match_feat': path_h5 + f'query_match_{datatype}_feat.h5',\n        'editDistance_feat': path_h5 + f'editDistance_{datatype}_feat.h5',\n\n        'sim_feat': path_h5 + f'sim_{datatype}_feat.h5',\n        'tag_score_feat': path_h5 + f'tag_score_10foldtime_{datatype}_feat.h5',\n        'title_score_count_feat': path_h5 + f'title_score_count_{datatype}_feat.h5',\n        'title_code_score_feat': path_h5 + f'title_code_score_10foldtime_{datatype}_feat.h5',\n        'title_convert_feat': path_h5 + f'title_convert_{datatype}.h5',\n        'sif_feat': path_h5 + f'sif_{datatype}_post_4kw.h5',\n        'len_feat': path_h5 + f'len_{datatype}_feat.h5',\n\n        'title_count': path_h5 + f'count_feature_{datatype}.h5',\n        'query_count': path_h5 + f'query_count_all_{datatype}.h5',\n\n        \"title_nunique_query\": path_h5 + f'nunique_feature_{datatype}.h5',\n        \"query_nunique_title\": path_h5 + f'query_nunique_title_all_{datatype}.h5',\n\n        'tag': path_h5 + f'tag_{datatype}.h5',\n        \"tag_convert_feat\": path_h5 + f\"tag_convert_{datatype}.h5\",\n        \"query_convert\": path_h5 + f\"query_convert_{datatype}.h5\",\n\n        \"M_cosine\": path_h5 + f\"M_sim_{datatype}_feat.h5\",\n        \"M_tfidf_cosine\": path_h5 + f\"M_tfidf_sim_{datatype}_feat.h5\",\n        \"BM25\": path_h5 + f'BM25_{datatype}_feat.h5',\n        'NN_SIM': path_h5 + f'nn_sim_feature.h5',\n\n        'editdistance_relativepos': path_h5 + f'editdistance_relativepos_{datatype}_feat.h5',\n        'fuzz': path_h5 + f\"fuzz_{datatype}_feat.h5\",\n        'textpair': path_h5 + f\"textpair_{datatype}_feat.h5\",\n\n        'sen_dis': path_h5 + f\"sen_dis_{datatype}_200.h5\",\n        'sen_dis2': path_h5 + f\"sen_dis2_{datatype}_200.h5\",\n    }\n\n    for featurefile in featurecol_h5:\n        print(\"read \", featuremap_h5[featurefile])\n        feature_set = pd.read_hdf(featuremap_h5[featurefile],\n                                  key='data',\n                                  start=0,\n                                  stop=nrows)[featurecol_h5[featurefile]].reset_index(drop=True)\n        print(\"length:\", feature_set.__len__())\n        # print(feature_set.head(1))\n        # feature_set=reduce_mem_usage(feature_set, verbose=True)\n        DataSet = pd.concat([DataSet, feature_set], axis=1)\n\n    DataSet[\"titlekw_querykw_diff\"] = DataSet[\"titlekw_num\"] - DataSet[\"querykw_num\"]\n    DataSet[\"titlekw_querykw_rate\"] = DataSet[\"titlekw_num\"] / DataSet[\"querykw_num\"]\n\n    if \"title_code_score\" in DataSet.columns:\n        DataSet.title_code_score = DataSet.title_code_score.fillna(0)\n    if \"tag_score\" in DataSet.columns:\n        DataSet.tag_score = DataSet.tag_score.fillna(0)\n\n    DataSet = reduce_mem_usage(DataSet, verbose=True)\n    print(\"Data Read Finish!\")\n    return DataSet\n\n\nif __name__ == \"__main__\":\n    if flag == 'train':\n\n        train_size = 98000000\n        emb_mat = np.load('/home/kesci/work/sunrui/NN_second_2e/word2vec_fasttext_6kw_nn_sim.npy')\n        \n        lgb_data = ReadData(datatype='train', nrows=100000000)\n        used_lgb_dense_feature = get_used_feature_names(featurecol_h5) + othercols\n\n        print(used_lgb_dense_feature)\n\n        text_data = pd.read_csv('/home/kesci/input/bytedance/train_final.csv', usecols=[0, 1, 3, 4], header=None,\n            names=['query_id', 'query', 'title', 'label'], nrows=100000000)\n        # 读取 lgb feature\n        print(text_data.shape)\n        lgb_data[used_lgb_dense_feature] = lgb_data[used_lgb_dense_feature].fillna(-1, )\n\n        train_lgb_data = lgb_data[:train_size]\n        val_lgb_data = lgb_data[train_size:]\n        # 读取 lgb feature 完毕\n\n        with open('/home/kesci/work/sunrui/NN_second_2e/second_6kw_nn_sim.pkl', 'rb') as f:\n            word2id = pickle.load(f)\n\n        val_text_data = text_data[train_size:]\n        train_text_data = text_data[:train_size]\n\n        Q_val = []\n        D_val = []\n        for query in val_text_data['query']:\n            query = query.split()\n            Q_val.append([word2id[w] for w in query])  # 没有命中就返回UNK\n\n        for title in val_text_data['title']:\n            title = title.split()\n            D_val.append([word2id[w] for w in title])\n\n        val_query_input = seq_padding(Q_val, max_seq_len)\n        val_title_input = seq_padding(D_val, max_seq_len)\n\n        Y_val = val_text_data['label'].values\n\n        train_generator = DataGenerator(word2id=word2id, text_data=train_text_data,\n                                        lgb_data=train_lgb_data, batch_size=batch_size)\n\n        val_text_data['query_id_nums'] = val_text_data.groupby(['query_id'])['label'].transform('count')\n\n        val_group_df = val_text_data[['query_id', 'query_id_nums']].drop_duplicates()\n        val_group = val_group_df.query_id_nums.get_values()\n\n        swa = SWA(checkpoint_dir='./sunrui/swa/', model_name='swa.model')\n\n        clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                       step_size=2 * train_size // batch_size, mode='triangular')\n\n        model = build_model(lstm_dim=64, emb_mat=emb_mat)\n\n        filepath = \"/home/kesci/work/sunrui/nn/gated_pre_1e/gated-{epoch:02d}_esim_64_pre1e.hdf5\"\n        checkpoint = ModelCheckpoint(filepath, verbose=1)\n\n        early_stopping = EarlyStopping(monitor='val_auc', patience=5, verbose=1, mode='max')\n\n        val_lgb_input = [val_lgb_data[feat].values for feat in used_lgb_dense_feature]\n\n        # train_model_input = [train_query_input, train_title_input] + train_lgb_input\n        val_model_input = [val_query_input, val_title_input] + val_lgb_input\n\n        eval_callback = Evaluation(\n            validation_data=(\n                val_model_input, val_group, Y_val))\n\n        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[auc])\n        model.summary()\n        model.fit_generator(train_generator, epochs=epochs,\n                            validation_data=(val_model_input, Y_val),\n                            callbacks=[early_stopping, eval_callback, swa, clr, checkpoint], shuffle=True,\n                            workers=2,\n                            use_multiprocessing=True)\n    else:\n        pass","execution_count":null},{"metadata":{"id":"1FAB9471D87B4047A587D0E8046DC152"},"cell_type":"code","outputs":[],"source":"","execution_count":null},{"metadata":{"id":"EBC0E2F3BAEC4E8CAFC52EA91EA1EB06"},"cell_type":"code","outputs":[],"source":"","execution_count":null},{"metadata":{"id":"B9654F9D93D3460C8C61012B718CBB18"},"cell_type":"code","outputs":[],"source":"","execution_count":null}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":0}